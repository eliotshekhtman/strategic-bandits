{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# I'm assuming samples want to maximize 0-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799f5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v, temp=0.02):\n",
    "    sm = nn.Sigmoid()\n",
    "    v = (v + 1) / 2\n",
    "    return sm(v / temp) * 2 - 1 # bc -1 1 instead of 0 1\n",
    "\n",
    "def f(X, theta):\n",
    "    return X @ theta.T\n",
    "\n",
    "def R(X, y, theta):\n",
    "    return torch.maximum(1 - f(X, theta) * y.reshape(-1, 1), torch.zeros(X.shape[0], theta.shape[0]))\n",
    "\n",
    "def alpha_reg(alpha):\n",
    "    one_m = torch.ones(alpha.shape[1])\n",
    "    # return 0.5 * torch.trace(alpha @ alpha.T) # - torch.sum(torch.minimum(torch.zeros(4, 2), alpha) * 1000)\n",
    "    return 0.5 * (alpha @ one_m).T @ (alpha @ one_m)\n",
    "\n",
    "def alpha_loss(X, theta, alpha):\n",
    "    return - torch.trace(alpha @ f(X, theta).T) + alpha_reg(alpha)\n",
    "\n",
    "def theta_reg(theta):\n",
    "    return torch.trace(theta @ theta.T)\n",
    "\n",
    "def memory(alphas, p=0.5):\n",
    "    v = []\n",
    "    m = []\n",
    "    for i, a in enumerate(alphas):\n",
    "        m.append(p ** (len(alphas) - i - 1))\n",
    "        v.append(m[-1] * a)\n",
    "    v = torch.sum(torch.stack(v), dim=0) / sum(m)\n",
    "    return v\n",
    "\n",
    "def cache_memory(alpha, mem, p=0.5):\n",
    "    return (alpha + p * mem) / (1 + p)\n",
    "\n",
    "# def memory(alphas, ip=2):\n",
    "#     v = []\n",
    "#     m = []\n",
    "#     for i, a in enumerate(alphas):\n",
    "#         m.append(ip ** i)\n",
    "#         v.append(m[-1] * a)\n",
    "#     v = torch.sum(torch.stack(v), dim=0) / sum(m)\n",
    "#     return v\n",
    "\n",
    "def theta_loss(X, y, alphas, theta, p=0.5, c=1):\n",
    "    return torch.trace(memory(alphas, p=p) @ R(X, y, theta).T) + c * theta_reg(theta)\n",
    "\n",
    "def theta_loss_mem(X, y, mem, theta, c=1):\n",
    "    return torch.trace(mem @ R(X, y, theta).T) + c * theta_reg(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bf0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    theta = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float)\n",
    "    X = torch.tensor([[1, 1, 1], [1, -1, 1], [-1, 1, 1], [-1, -1, 1]], dtype=torch.float)\n",
    "    y = torch.tensor([1, -1, -1, -1], dtype=torch.float)\n",
    "    p = 0\n",
    "    c = 0\n",
    "    return { 'theta' : theta,  'X' : X, 'y' : y, 'p' : p, 'c' : c }\n",
    "\n",
    "def setup_2p():\n",
    "    theta = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float)\n",
    "    X = torch.tensor([[1, 1, 1], [1, 1, 1], [1, -1, 1], [-1, 1, 1], [-1, -1, 1]], dtype=torch.float)\n",
    "    y = torch.tensor([1, 1, -1, -1, -1], dtype=torch.float)\n",
    "    p = 0\n",
    "    c = 1\n",
    "    return { 'theta' : theta,  'X' : X, 'y' : y, 'p' : p, 'c' : c }\n",
    "\n",
    "\n",
    "def setup_3():\n",
    "    theta = torch.tensor([[-1, 1, 0]], dtype=torch.float)\n",
    "    X = torch.tensor([[0, 0.5, 1], [1, 0, 1], [-1, 0, 1]], dtype=torch.float)\n",
    "    y = torch.tensor([1, -1, -1], dtype=torch.float)\n",
    "    p = 1\n",
    "    c = 0\n",
    "    return { 'theta' : theta,  'X' : X, 'y' : y, 'p' : p, 'c' : c }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9a33e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diff: 0.0:   0%|                                                                                | 0/20 [00:00<?, ?it/s]C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_11532\\650747303.py:15: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2318.)\n",
      "  return 0.5 * (alpha @ one_m).T @ (alpha @ one_m)\n",
      "Diff: 1.9242945909500122: 100%|████████████████████████████████████████████████████████| 20/20 [12:20<00:00, 37.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Iteratively run non-closed-form optimization\n",
    "alphas = []\n",
    "thetas = []\n",
    "diffs = []\n",
    "mems = []\n",
    "alpha_losses = []\n",
    "theta_losses = []\n",
    "vals = setup_2p()\n",
    "theta = vals['theta']\n",
    "X = vals['X']\n",
    "y = vals['y']\n",
    "p = vals['p']\n",
    "c = vals['c']\n",
    "old_theta = theta.detach()\n",
    "\n",
    "for e in (pbar := tqdm(range(20), position=0, leave=True)):\n",
    "    diffs.append(torch.linalg.norm(old_theta - theta))\n",
    "    pbar.set_description(f\"Diff: {diffs[-1]}\")\n",
    "    alpha = torch.zeros(X.shape[0], theta.shape[0])\n",
    "    for _ in range(50000):\n",
    "        alpha = torch.maximum(torch.zeros(X.shape[0], theta.shape[0]), alpha).detach()\n",
    "        alpha.requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([alpha], lr=0.0001)\n",
    "        optimizer.zero_grad()\n",
    "        loss = alpha_loss(X, theta, alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    alphas.append(torch.maximum(torch.zeros(X.shape[0], theta.shape[0]), alpha).detach())\n",
    "    alpha_losses.append(alpha_loss(X, theta, alpha).detach())\n",
    "    if e == 0:\n",
    "        mem = alpha\n",
    "    mem = cache_memory(alpha, mem, p=p).detach()\n",
    "    mems.append(mem)\n",
    "\n",
    "#     print('ON:', old_theta, theta, torch.linalg.norm(old_theta - theta))\n",
    "    old_theta = torch.clone(theta.detach())\n",
    "    theta = torch.ones_like(theta)\n",
    "    theta.requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([theta], lr=0.005)\n",
    "    for i in range(10000):\n",
    "#         theta = torch.maximum(torch.ones(2, 3), theta).detach()\n",
    "#         theta.requires_grad_(True)\n",
    "#         optimizer = torch.optim.Adam([theta], lr=0.005)\n",
    "        optimizer.zero_grad()\n",
    "        loss = theta_loss_mem(X, y, mem, theta, c=c)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    thetas.append(theta.detach())\n",
    "    theta_losses.append(theta_loss(X, y, alphas, theta, p=p, c=c).detach())\n",
    "\n",
    "ncf_alphas = alphas\n",
    "ncf_thetas = thetas\n",
    "ncf_diffs = diffs\n",
    "ncf_alpha_losses = alpha_losses\n",
    "ncf_theta_losses = theta_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa35bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_grad(X, y, mem, theta, k, c):\n",
    "    grad = 2 * c * theta[k]\n",
    "    grad -= sum([mem[i,k] * y[i] * X[i].T for i in range(X.shape[0]) if X[i] @ theta[k].T * y[i] < 1])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively run explicit gradient step optimization\n",
    "# Iteratively run closed-form optimization (NO HINGE LOSS)\n",
    "vals = setup_3()\n",
    "theta = vals['theta']\n",
    "X = vals['X']\n",
    "y = vals['y']\n",
    "p = vals['p']\n",
    "c = vals['c']\n",
    "old_theta = theta.detach()\n",
    "alphas = []\n",
    "thetas = []\n",
    "diffs = []\n",
    "mems = []\n",
    "f_x = []\n",
    "alpha_losses = []\n",
    "theta_losses = []\n",
    "        \n",
    "\n",
    "for e in (pbar := tqdm(range(20), position=0, leave=True)):\n",
    "    diffs.append(torch.linalg.norm(old_theta - theta))\n",
    "    pbar.set_description(f\"Diff: {diffs[-1]}\")\n",
    "    alpha = torch.maximum(torch.zeros(X.shape[0], theta.shape[0]), f(X, theta))\n",
    "    for i in range(X.shape[0]):\n",
    "        if torch.sum(alpha[i]) > 1.5:\n",
    "            alpha[i] *= 1.5 / torch.sum(alpha[i])\n",
    "    alpha_losses.append(alpha_loss(X, theta, alpha).detach())\n",
    "    alphas.append(alpha)\n",
    "    if e == 0:\n",
    "        mem = alpha\n",
    "    print(alpha)\n",
    "\n",
    "    old_theta = theta.detach().clone()\n",
    "    theta = theta.detach().clone()\n",
    "    mem = cache_memory(alpha, mem, p=p)\n",
    "    mems.append(mem)\n",
    "    for i in range(10_000):\n",
    "        lr = 1 / (1 * (i + 1))\n",
    "        theta_list = [theta_grad(X, y, mem, theta, k) for k in range(theta.shape[0])]\n",
    "        theta -= lr * torch.stack(theta_list)\n",
    "    thetas.append(theta.detach())\n",
    "    theta_losses.append(theta_loss(X, y, alphas, theta, p=p, c=c).detach())\n",
    "    f_x.append(f(X, theta))\n",
    "    print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc927c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time  0\n",
      "0.0\n",
      "[[0.49997058510780334, 0.49997058510780334], [0.49997058510780334, 0.49997058510780334], [1.0000535249710083, 0.0], [0.0, 1.0000535249710083], [0.0, 0.0]]\n",
      "[[-0.0004241214192006737, 0.968101978302002, -0.00022413194528780878], [0.968101978302002, -0.0004241214192006737, -0.00022413194528780878]]\n",
      "-2.0006003379821777\n",
      "2.002035140991211\n",
      "\n",
      "Time  1\n",
      "1.9687914848327637\n",
      "[[0.48377275466918945, 0.48377275466918945], [0.48377275466918945, 0.48377275466918945], [0.0, 0.9682483077049255], [0.9682483077049255, 0.0], [0.0, 0.0]]\n",
      "[[0.9678969383239746, -0.0004513925814535469, -0.000251405785093084], [-0.0004513925814535469, 0.9678969383239746, -0.000251405785093084]]\n",
      "-1.8741563558578491\n",
      "1.9979382753372192\n",
      "\n",
      "Time  2\n",
      "1.936874508857727\n",
      "[[0.483572781085968, 0.483572781085968], [0.483572781085968, 0.483572781085968], [0.9680482745170593, 0.0], [0.0, 0.9680482745170593], [0.0, 0.0]]\n",
      "[[-0.0005513495998457074, 0.9675968885421753, -0.00035136277438141406], [0.9675968885421753, -0.0005513495998457074, -0.00035136277438141406]]\n",
      "-1.8732569217681885\n",
      "1.9978992938995361\n",
      "\n",
      "Time  3\n",
      "1.936496615409851\n",
      "[[0.4833728075027466, 0.4833728075027466], [0.4833728075027466, 0.4833728075027466], [0.0, 0.9678481817245483], [0.9678481817245483, 0.0], [0.0, 0.0]]\n",
      "[[0.9672968983650208, -0.0006512767868116498, -0.00045128996134735644], [-0.0006512767868116498, 0.9672968983650208, -0.00045128996134735644]]\n",
      "-1.8717093467712402\n",
      "1.9978597164154053\n",
      "\n",
      "Time  4\n",
      "1.936096429824829\n",
      "[[0.48317283391952515, 0.48317283391952515], [0.48317283391952515, 0.48317283391952515], [0.9674481749534607, 0.0], [0.0, 0.9674481749534607], [0.0, 0.0]]\n",
      "[[-0.0006512469262816012, 0.9668968915939331, -0.00045126015902496874], [0.9668968915939331, -0.0006512469262816012, -0.00045126015902496874]]\n",
      "-1.8701624870300293\n",
      "1.9978071451187134\n",
      "\n",
      "Time  5\n",
      "1.9354963302612305\n",
      "[[0.4829728603363037, 0.4829728603363037], [0.4829728603363037, 0.4829728603363037], [0.0, 0.9670481085777283], [0.9670481085777283, 0.0], [0.0, 0.0]]\n",
      "[[0.9664969444274902, -0.0006511873798444867, -0.00045120055438019335], [-0.0006511873798444867, 0.9664969444274902, -0.00045120055438019335]]\n",
      "-1.8686155080795288\n",
      "1.9977538585662842\n",
      "\n",
      "Time  6\n",
      "1.9346963167190552\n",
      "[[0.4827728867530823, 0.4827728867530823], [0.4827728867530823, 0.4827728867530823], [0.9666480422019958, 0.0], [0.0, 0.9666480422019958], [0.0, 0.0]]\n",
      "[[-0.0006511277751997113, 0.9660968780517578, -0.00045114094973541796], [0.9660968780517578, -0.0006511277751997113, -0.00045114094973541796]]\n",
      "-1.8670697212219238\n",
      "1.9976999759674072\n",
      "\n",
      "Time  7\n",
      "1.9338961839675903\n",
      "[[0.48257291316986084, 0.48257291316986084], [0.48257291316986084, 0.48257291316986084], [0.0, 0.9662479758262634], [0.9662479758262634, 0.0], [0.0, 0.0]]\n",
      "[[0.9656968712806702, -0.0006510681705549359, -0.00045108134509064257], [-0.0006510681705549359, 0.9656968712806702, -0.00045108134509064257]]\n",
      "-1.8655242919921875\n",
      "1.997645378112793\n",
      "\n",
      "Time  8\n",
      "1.933095932006836\n",
      "[[0.4823729395866394, 0.4823729395866394], [0.4823729395866394, 0.4823729395866394], [0.965847909450531, 0.0], [0.0, 0.965847909450531], [0.0, 0.0]]\n",
      "[[-0.0006510085077024996, 0.9652968645095825, -0.0004510217404458672], [0.9652968645095825, -0.0006510085077024996, -0.0004510217404458672]]\n",
      "-1.8639793395996094\n",
      "1.997590184211731\n",
      "\n",
      "Time  9\n",
      "1.9322959184646606\n",
      "[[0.48217296600341797, 0.48217296600341797], [0.48217296600341797, 0.48217296600341797], [0.0, 0.9654478430747986], [0.9654478430747986, 0.0], [0.0, 0.0]]\n",
      "[[0.9648969173431396, -0.0006509489030577242, -0.0004509621358010918], [-0.0006509489030577242, 0.9648969173431396, -0.0004509621358010918]]\n",
      "-1.8624351024627686\n",
      "1.9975342750549316\n",
      "\n",
      "Time  10\n",
      "1.9314957857131958\n",
      "[[0.48197299242019653, 0.48197299242019653], [0.48197299242019653, 0.48197299242019653], [0.9650477766990662, 0.0], [0.0, 0.9650477766990662], [0.0, 0.0]]\n",
      "[[-0.0006508892984129488, 0.9644968509674072, -0.0004509025311563164], [0.9644968509674072, -0.0006508892984129488, -0.0004509025311563164]]\n",
      "-1.8608920574188232\n",
      "1.9974777698516846\n",
      "\n",
      "Time  11\n",
      "1.930695652961731\n",
      "[[0.4817730188369751, 0.4817730188369751], [0.4817730188369751, 0.4817730188369751], [0.0, 0.9646477103233337], [0.9646477103233337, 0.0], [0.0, 0.0]]\n",
      "[[0.9640968441963196, -0.0006508296937681735, -0.000450842926511541], [-0.0006508296937681735, 0.9640968441963196, -0.000450842926511541]]\n",
      "-1.8593487739562988\n",
      "1.9974206686019897\n",
      "\n",
      "Time  12\n",
      "1.9298954010009766\n",
      "[[0.48157304525375366, 0.48157304525375366], [0.48157304525375366, 0.48157304525375366], [0.9642476439476013, 0.0], [0.0, 0.9642476439476013], [0.0, 0.0]]\n",
      "[[-0.000650770147331059, 0.9636968374252319, -0.0004507833218667656], [0.9636968374252319, -0.000650770147331059, -0.0004507833218667656]]\n",
      "-1.8578068017959595\n",
      "1.9973629713058472\n",
      "\n",
      "Time  13\n",
      "1.9290953874588013\n",
      "[[0.4813730716705322, 0.4813730716705322], [0.4813730716705322, 0.4813730716705322], [0.0, 0.9638475775718689], [0.9638475775718689, 0.0], [0.0, 0.0]]\n",
      "[[0.9632968902587891, -0.0006507104844786227, -0.00045072371722199023], [-0.0006507104844786227, 0.9632968902587891, -0.00045072371722199023]]\n",
      "-1.8562653064727783\n",
      "1.9973045587539673\n",
      "\n",
      "Time  14\n",
      "1.9282952547073364\n",
      "[[0.4811730980873108, 0.4811730980873108], [0.4811730980873108, 0.4811730980873108], [0.9634475111961365, 0.0], [0.0, 0.9634475111961365], [0.0, 0.0]]\n",
      "[[-0.0006506508798338473, 0.9628968238830566, -0.00045066411257721484], [0.9628968238830566, -0.0006506508798338473, -0.00045066411257721484]]\n",
      "-1.854724407196045\n",
      "1.99724543094635\n",
      "\n",
      "Time  15\n",
      "1.9274951219558716\n",
      "[[0.48097312450408936, 0.48097312450408936], [0.48097312450408936, 0.48097312450408936], [0.0, 0.963047444820404], [0.963047444820404, 0.0], [0.0, 0.0]]\n",
      "[[0.962496817111969, -0.0006505912751890719, -0.00045060450793243945], [-0.0006505912751890719, 0.962496817111969, -0.00045060450793243945]]\n",
      "-1.8531841039657593\n",
      "1.9971858263015747\n",
      "\n",
      "Time  16\n",
      "1.9266948699951172\n",
      "[[0.4807731509208679, 0.4807731509208679], [0.4807731509208679, 0.4807731509208679], [0.9626473784446716, 0.0], [0.0, 0.9626473784446716], [0.0, 0.0]]\n",
      "[[-0.0006505316705442965, 0.9620968103408813, -0.00045054490328766406], [0.9620968103408813, -0.0006505316705442965, -0.00045054490328766406]]\n",
      "-1.8516442775726318\n",
      "1.997125506401062\n",
      "\n",
      "Time  17\n",
      "1.9258947372436523\n",
      "[[0.4805731773376465, 0.4805731773376465], [0.4805731773376465, 0.4805731773376465], [0.0, 0.9622473120689392], [0.9622473120689392, 0.0], [0.0, 0.0]]\n",
      "[[0.9616968631744385, -0.0006504720658995211, -0.00045048529864288867], [-0.0006504720658995211, 0.9616968631744385, -0.00045048529864288867]]\n",
      "-1.8501052856445312\n",
      "1.997064471244812\n",
      "\n",
      "Time  18\n",
      "1.925094723701477\n",
      "[[0.48037320375442505, 0.48037320375442505], [0.48037320375442505, 0.48037320375442505], [0.9618472456932068, 0.0], [0.0, 0.9618472456932068], [0.0, 0.0]]\n",
      "[[-0.0006504124612547457, 0.961296796798706, -0.0004504256939981133], [0.961296796798706, -0.0006504124612547457, -0.0004504256939981133]]\n",
      "-1.848567247390747\n",
      "1.9970028400421143\n",
      "\n",
      "Time  19\n",
      "1.9242945909500122\n",
      "[[0.4801732301712036, 0.4801732301712036], [0.4801732301712036, 0.4801732301712036], [0.0, 0.9614471793174744], [0.9614471793174744, 0.0], [0.0, 0.0]]\n",
      "[[0.9608967900276184, -0.0006503528566099703, -0.0004503660893533379], [-0.0006503528566099703, 0.9608967900276184, -0.0004503660893533379]]\n",
      "-1.847029447555542\n",
      "1.9969406127929688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_disp = [ncf_diffs, ncf_alphas, ncf_thetas, ncf_alpha_losses, ncf_theta_losses]\n",
    "for i in range(20):\n",
    "    # print(f'Time {i}: {[a[i].tolist() for a in to_disp]}')\n",
    "    print('Time ', i)\n",
    "    for a in to_disp:\n",
    "        print(a[i].tolist())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70972b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_disp = [diffs, alphas, thetas, alpha_losses, theta_losses]\n",
    "for i in range(20):\n",
    "    # print(f'Time {i}: {[a[i].tolist() for a in to_disp]}')\n",
    "    print('Time ', i)\n",
    "    for a in to_disp:\n",
    "        print(a[i].tolist())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864aea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
