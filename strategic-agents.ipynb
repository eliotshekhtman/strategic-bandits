{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f39341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3352067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.79109255e-19  2.85112420e-02  2.79973443e-19  3.37658729e-20\n",
      " -2.72802663e-19  1.49285011e-01 -9.94082533e-20  8.35373900e-20\n",
      "  2.46718649e-01  5.78224144e-01 -4.03739463e-19  1.01242860e-03\n",
      " -9.28486180e-20  2.26767464e-01 -1.58813678e-19 -8.97232272e-20\n",
      " -1.22145729e-19 -1.51509428e-19  1.12060672e-19 -3.48318635e-19]\n",
      "[ 2.50938945  0.          2.78354615  1.79425782 13.08579183  0.\n",
      "  0.73716363  3.35344995  0.          0.          8.93825054  0.\n",
      "  7.02955161  0.          4.71068649  3.18873635  2.06090107 10.08166738\n",
      "  3.0481157   8.53268239]\n"
     ]
    }
   ],
   "source": [
    "# Problem data.\n",
    "m = 30\n",
    "n = 20\n",
    "np.random.seed(1)\n",
    "A = np.random.randn(m, n)\n",
    "b = np.random.randn(m)\n",
    "\n",
    "# Construct the problem.\n",
    "x = cp.Variable(n)\n",
    "objective = cp.Minimize(cp.sum_squares(A @ x - b))\n",
    "constraints = [0 <= x, x <= 1]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in `x.value`.\n",
    "print(x.value)\n",
    "# The optimal Lagrange multiplier for a constraint is stored in\n",
    "# `constraint.dual_value`.\n",
    "print(constraints[0].dual_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f435e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(M, axis=-1):\n",
    "    return cp.exp(M - cp.log(cp.sum(cp.exp(M), axis=axis, keepdims=True)))\n",
    "\n",
    "def l1norm_agent(X, phi, b, u):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    X : (d,)\n",
    "    phi : (m,d)\n",
    "    b : (m,)\n",
    "    u : (m,)\n",
    "    \"\"\"\n",
    "    d, = X.shape\n",
    "    m, = b.shape\n",
    "    one_m = np.ones(m)\n",
    "    X_hat = cp.Variable(d)\n",
    "    \n",
    "    objective = cp.Maximize(u @ (phi @ X_hat + b) / (one_m @ (phi @ X_hat + b)) - 0.5 * cp.sum_squares(X - X_hat))\n",
    "    constraints = [-X_hat <= 0]\n",
    "    prob = cp.Problem(objective, constraints)#, constraints)\n",
    "    result = prob.solve()#qcp=True)\n",
    "    \n",
    "    return X_hat.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0386c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfp(A, b, c, d, alpha, beta):\n",
    "    m, n = A.shape\n",
    "    \n",
    "    x = cp.Variable(n)\n",
    "    objective = cp.Maximize((c @ x + alpha) / (d @ x + beta))\n",
    "    constraints = [x >= 0]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    result = prob.solve(qcp=True)\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be499a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "n = 2\n",
    "A = np.random.rand(m, n)\n",
    "b = np.random.rand(m)\n",
    "c = np.random.rand(n)\n",
    "d = np.random.rand(n)\n",
    "alpha = np.random.rand()\n",
    "beta = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f1db27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.68245371, 0.22820573],\n",
       "        [0.01376751, 0.41672396],\n",
       "        [0.93848189, 0.34302811]]),\n",
       " array([0.7797443 , 0.17473631, 0.34195284]),\n",
       " array([0.14459772, 0.71677081]),\n",
       " array([0.69930762, 0.68849732]),\n",
       " 0.25339603448629966,\n",
       " 0.6923601216234089)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, b, c, d, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d90607dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfp(A, b, c, d, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13124cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.50000165)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cp.Variable()\n",
    "concave_fractional_fn = cp.sqrt(x) / cp.exp(x)\n",
    "problem = cp.Problem(cp.Maximize(concave_fractional_fn))\n",
    "assert problem.is_dqcp()\n",
    "problem.solve(qcp=True)\n",
    "x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "89b33d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "m = 2\n",
    "X = np.ones(d)\n",
    "phi = np.ones((m, d))\n",
    "b = np.ones(m)\n",
    "u = np.ones(m)\n",
    "one_m = np.ones(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "b44d6f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u @ (phi @ X) / (one_m @ (phi @ X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "82d93662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1norm_agent(X, phi, b, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc184de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd04ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(X_hat, X, phi, b, u):\n",
    "    m, = b.shape\n",
    "    one_m = torch.ones(m)\n",
    "    action = phi @ X_hat + b\n",
    "    reward = u @ action / (one_m @ action)\n",
    "    cost = torch.sum((X - X_hat) ** 2)\n",
    "    return - (reward - 0.5 * cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "eb7790f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = torch.tensor([1.] * d)\n",
    "X = torch.tensor([2.] * d)\n",
    "phi = torch.tensor(phi, dtype=torch.float32)\n",
    "b = torch.tensor(b, dtype=torch.float32)\n",
    "u = torch.tensor(u, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "5a5964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat.requires_grad_(True)\n",
    "optimizer = torch.optim.SGD([X_hat], lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "76b8d243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0100, 1.0100], requires_grad=True),\n",
       " tensor(-0., grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss = criterion(X_hat, X, phi, b, u)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "X_hat, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e752f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1norm_gragent(X, phi, b, u):\n",
    "    m, d = phi.shape\n",
    "    one_m = torch.ones(m)\n",
    "    X_hat = torch.rand(d)\n",
    "    X_hat.requires_grad_(True)\n",
    "    optimizer = torch.optim.SGD([X_hat], lr=0.01, momentum=0.9)\n",
    "    for _ in range(300):\n",
    "        optimizer.zero_grad()\n",
    "        action = phi @ X_hat + b\n",
    "        reward = u @ action / (one_m @ action)\n",
    "        cost = torch.sum((X - X_hat) ** 2)\n",
    "        loss = - (reward - 0.5 * cost)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(loss)\n",
    "    return X_hat\n",
    "\n",
    "def softmax_gragent(X, phi, b, u):\n",
    "    m, d = phi.shape\n",
    "    one_m = torch.ones(m)\n",
    "    X_hat = torch.rand(d)\n",
    "    X_hat.requires_grad_(True)\n",
    "    softmax = nn.Softmax()\n",
    "    optimizer = torch.optim.SGD([X_hat], lr=0.01, momentum=0.9)\n",
    "    for _ in range(300):\n",
    "        optimizer.zero_grad()\n",
    "        reward = u @ softmax(phi @ X_hat + b)\n",
    "        cost = torch.sum((X - X_hat) ** 2)\n",
    "        loss = - (reward - 0.5 * cost)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(loss)\n",
    "    return X_hat\n",
    "\n",
    "def l1norm_decision_maker_criterion(Xs_hat, Xs, phi, b, W, u):\n",
    "    Xs_hat = Xs_hat.T\n",
    "    Xs = Xs.T\n",
    "    d, m = W.shape\n",
    "    one_m = torch.ones(m)\n",
    "    action = phi @ Xs_hat + b.reshape(-1, 1) # md @ dn + m --> mn\n",
    "    penalty = 0\n",
    "    reward = Xs.T @ (W @ action / (one_m @ action)) # nd @ (dm @ mn / (m @ mn)) --> n\n",
    "    for i in range(d):\n",
    "        pen_i = Xs[i] * Xs_hat[i] # n\n",
    "        for j in range(d):\n",
    "            pen_i -= ((u @ phi[:, i]) * (one_m @ phi[:, j]) + (u @ phi[:, j]) * (one_m @ phi[:, i])) * Xs_hat[i] * Xs_hat[j]\n",
    "        pen_i += 2 * Xs_hat[i] ** 2\n",
    "        penalty -= pen_i ** 2\n",
    "    # print(reward, penalty)\n",
    "    return torch.sum(reward)\n",
    "    \n",
    "\n",
    "def l1norm_phi(Xs, W, u):\n",
    "    d, m = W.shape\n",
    "    # one_m = torch.ones(m)\n",
    "    Xs_hat = Xs.clone()\n",
    "    Xs_hat.requires_grad_(True)\n",
    "    phi = torch.rand((m, d))\n",
    "    phi.requires_grad_(True)\n",
    "    b = torch.zeros(m)\n",
    "    b.requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([Xs_hat, phi, b], lr=0.0001)\n",
    "    for _ in range(300):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -l1norm_decision_maker_criterion(Xs_hat, Xs, phi, b, W, u)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return phi, b\n",
    "\n",
    "def l1norm_phi(Xs, W, u):\n",
    "    d, m = W.shape\n",
    "    # one_m = torch.ones(m)\n",
    "    Xs_hat = Xs.clone()\n",
    "    Xs_hat.requires_grad_(True)\n",
    "    phi = torch.rand((m, d))\n",
    "    phi.requires_grad_(True)\n",
    "    b = torch.zeros(m)\n",
    "    b.requires_grad_(True)\n",
    "    W.requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([W, Xs_hat, phi, b], lr=0.0001)\n",
    "    for _ in range(300):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -l1norm_decision_maker_criterion(Xs_hat, Xs, phi, b, W, u)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return W, phi, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "dd9d94f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2437, 0.9902]), tensor([0.2437, 0.9902], requires_grad=True))"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new, l1norm_gragent(x_new, phi, b, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "c03b7e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_39092\\89251819.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  reward = u @ softmax(phi @ X_hat + b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.9627, 0.8434]), tensor([0.9653, 0.8426], requires_grad=True))"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(d)\n",
    "phi = torch.rand((m, d))\n",
    "b = torch.zeros(m)\n",
    "X, softmax_gragent(X, phi, b, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c462ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, phi, b):\n",
    "    m, d = phi.shape\n",
    "    one_m = torch.ones(m, dtype=torch.float64)\n",
    "    action = phi @ X + b\n",
    "    return action / (one_m @ action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "b6871d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify(X, phi, b), classify(X_hat, phi, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "26abb1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5595, 0.0952],\n",
       "         [0.1264, 0.7859]]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_phi, true_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_phi = torch.rand((m, d))\n",
    "true_b = torch.zeros(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "b7045d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x247f2b82520>"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_x = [np.random.uniform(low=0, high=1) for _ in range(10_000)]\n",
    "X_y = [np.random.uniform(low=0, high=1) for _ in range(10_000)]\n",
    "X = np.array([X_x, X_y]).T\n",
    "Y = [torch.argmax(classify(x, true_phi, true_b)).item() for x in X]\n",
    "plt.scatter(X_x, X_y, c=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ba597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ...  True  True False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x15e7c9bf730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_x = [np.random.uniform(low=0, high=1) for _ in range(10_000)]\n",
    "X_y = [np.random.uniform(low=0, high=1) for _ in range(10_000)]\n",
    "X = np.array([X_x, X_y]).T\n",
    "W = np.random.rand(2, 2)\n",
    "print((X @ W)[:,0] > (X @ W)[:,1])\n",
    "Y = (X @ W)[:,0] > (X @ W)[:,1]\n",
    "# Y = [torch.argmax(classify(x, true_phi, true_b)).item() for x in X]\n",
    "plt.scatter(X_x, X_y, c=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "94a92323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_39092\\828025484.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  reward = u @ softmax(phi @ X_hat + b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x247f58e1280>]"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_new = torch.tensor([0.5, 0.2])\n",
    "u = torch.tensor([0.3, 0])\n",
    "x_prime = softmax_gragent(x_new, true_phi, true_b, u).detach()\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.scatter(X_x, X_y, c=Y)\n",
    "plt.plot(x_new[0], x_new[1], marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"red\")\n",
    "plt.plot(x_prime[0], x_prime[1], marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "56572dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5216, 0.1656])"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "9318778b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3704, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_phi = torch.rand(2,2)\n",
    "fake_phi.requires_grad_(True)\n",
    "res = torch.sum(fake_phi + fake_phi[:,1] * 2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "a72c1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "d4f556bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "Xs = torch.rand((n, d))\n",
    "W = torch.rand((d, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "410bfa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4646, 0.8714, 0.9069, 0.7962, 0.3983],\n",
       "        [0.3173, 0.5402, 0.5802, 0.4952, 0.2943]])"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi @ Xs.T + b.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "e1fefbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3718, 0.3890, 0.3838, 0.3885, 0.3538], grad_fn=<SqueezeBackward3>) tensor([-1.2584, -7.6001, -9.6864, -5.2877, -1.6877], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8869, grad_fn=<NegBackward0>)\n",
      "tensor([0.3720, 0.3892, 0.3840, 0.3886, 0.3541], grad_fn=<SqueezeBackward3>) tensor([-1.2577, -7.5999, -9.6846, -5.2874, -1.6868], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8879, grad_fn=<NegBackward0>)\n",
      "tensor([0.3722, 0.3893, 0.3842, 0.3888, 0.3544], grad_fn=<SqueezeBackward3>) tensor([-1.2571, -7.5996, -9.6828, -5.2871, -1.6860], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8889, grad_fn=<NegBackward0>)\n",
      "tensor([0.3725, 0.3895, 0.3843, 0.3890, 0.3546], grad_fn=<SqueezeBackward3>) tensor([-1.2564, -7.5993, -9.6811, -5.2868, -1.6851], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8899, grad_fn=<NegBackward0>)\n",
      "tensor([0.3727, 0.3897, 0.3845, 0.3891, 0.3549], grad_fn=<SqueezeBackward3>) tensor([-1.2557, -7.5990, -9.6793, -5.2865, -1.6842], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8909, grad_fn=<NegBackward0>)\n",
      "tensor([0.3730, 0.3898, 0.3847, 0.3893, 0.3552], grad_fn=<SqueezeBackward3>) tensor([-1.2551, -7.5988, -9.6775, -5.2863, -1.6833], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8920, grad_fn=<NegBackward0>)\n",
      "tensor([0.3732, 0.3900, 0.3848, 0.3895, 0.3555], grad_fn=<SqueezeBackward3>) tensor([-1.2544, -7.5985, -9.6758, -5.2860, -1.6824], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8930, grad_fn=<NegBackward0>)\n",
      "tensor([0.3734, 0.3902, 0.3850, 0.3897, 0.3558], grad_fn=<SqueezeBackward3>) tensor([-1.2537, -7.5982, -9.6740, -5.2857, -1.6815], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8940, grad_fn=<NegBackward0>)\n",
      "tensor([0.3737, 0.3903, 0.3852, 0.3898, 0.3560], grad_fn=<SqueezeBackward3>) tensor([-1.2531, -7.5980, -9.6723, -5.2854, -1.6806], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8950, grad_fn=<NegBackward0>)\n",
      "tensor([0.3739, 0.3905, 0.3853, 0.3900, 0.3563], grad_fn=<SqueezeBackward3>) tensor([-1.2524, -7.5977, -9.6705, -5.2851, -1.6797], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8961, grad_fn=<NegBackward0>)\n",
      "tensor([0.3741, 0.3906, 0.3855, 0.3902, 0.3566], grad_fn=<SqueezeBackward3>) tensor([-1.2517, -7.5974, -9.6687, -5.2848, -1.6788], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8971, grad_fn=<NegBackward0>)\n",
      "tensor([0.3744, 0.3908, 0.3857, 0.3903, 0.3569], grad_fn=<SqueezeBackward3>) tensor([-1.2511, -7.5972, -9.6670, -5.2845, -1.6779], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8981, grad_fn=<NegBackward0>)\n",
      "tensor([0.3746, 0.3910, 0.3858, 0.3905, 0.3572], grad_fn=<SqueezeBackward3>) tensor([-1.2504, -7.5969, -9.6652, -5.2842, -1.6770], grad_fn=<SubBackward0>)\n",
      "tensor(-1.8991, grad_fn=<NegBackward0>)\n",
      "tensor([0.3749, 0.3911, 0.3860, 0.3907, 0.3574], grad_fn=<SqueezeBackward3>) tensor([-1.2497, -7.5966, -9.6634, -5.2839, -1.6761], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9001, grad_fn=<NegBackward0>)\n",
      "tensor([0.3751, 0.3913, 0.3862, 0.3909, 0.3577], grad_fn=<SqueezeBackward3>) tensor([-1.2491, -7.5964, -9.6617, -5.2837, -1.6752], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9012, grad_fn=<NegBackward0>)\n",
      "tensor([0.3753, 0.3915, 0.3863, 0.3910, 0.3580], grad_fn=<SqueezeBackward3>) tensor([-1.2484, -7.5961, -9.6599, -5.2834, -1.6743], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9022, grad_fn=<NegBackward0>)\n",
      "tensor([0.3756, 0.3916, 0.3865, 0.3912, 0.3583], grad_fn=<SqueezeBackward3>) tensor([-1.2477, -7.5958, -9.6581, -5.2831, -1.6734], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9032, grad_fn=<NegBackward0>)\n",
      "tensor([0.3758, 0.3918, 0.3867, 0.3914, 0.3586], grad_fn=<SqueezeBackward3>) tensor([-1.2471, -7.5956, -9.6564, -5.2828, -1.6725], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9042, grad_fn=<NegBackward0>)\n",
      "tensor([0.3761, 0.3920, 0.3868, 0.3915, 0.3588], grad_fn=<SqueezeBackward3>) tensor([-1.2464, -7.5953, -9.6546, -5.2825, -1.6716], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9052, grad_fn=<NegBackward0>)\n",
      "tensor([0.3763, 0.3921, 0.3870, 0.3917, 0.3591], grad_fn=<SqueezeBackward3>) tensor([-1.2457, -7.5950, -9.6528, -5.2822, -1.6707], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9063, grad_fn=<NegBackward0>)\n",
      "tensor([0.3765, 0.3923, 0.3872, 0.3919, 0.3594], grad_fn=<SqueezeBackward3>) tensor([-1.2450, -7.5948, -9.6511, -5.2819, -1.6698], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9073, grad_fn=<NegBackward0>)\n",
      "tensor([0.3768, 0.3925, 0.3873, 0.3921, 0.3597], grad_fn=<SqueezeBackward3>) tensor([-1.2444, -7.5945, -9.6493, -5.2816, -1.6688], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9083, grad_fn=<NegBackward0>)\n",
      "tensor([0.3770, 0.3926, 0.3875, 0.3922, 0.3600], grad_fn=<SqueezeBackward3>) tensor([-1.2437, -7.5942, -9.6475, -5.2813, -1.6679], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9093, grad_fn=<NegBackward0>)\n",
      "tensor([0.3772, 0.3928, 0.3876, 0.3924, 0.3602], grad_fn=<SqueezeBackward3>) tensor([-1.2430, -7.5939, -9.6458, -5.2810, -1.6670], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9103, grad_fn=<NegBackward0>)\n",
      "tensor([0.3775, 0.3930, 0.3878, 0.3926, 0.3605], grad_fn=<SqueezeBackward3>) tensor([-1.2424, -7.5937, -9.6440, -5.2807, -1.6660], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9114, grad_fn=<NegBackward0>)\n",
      "tensor([0.3777, 0.3931, 0.3880, 0.3927, 0.3608], grad_fn=<SqueezeBackward3>) tensor([-1.2417, -7.5934, -9.6422, -5.2804, -1.6651], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9124, grad_fn=<NegBackward0>)\n",
      "tensor([0.3780, 0.3933, 0.3881, 0.3929, 0.3611], grad_fn=<SqueezeBackward3>) tensor([-1.2410, -7.5931, -9.6405, -5.2801, -1.6641], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9134, grad_fn=<NegBackward0>)\n",
      "tensor([0.3782, 0.3934, 0.3883, 0.3931, 0.3614], grad_fn=<SqueezeBackward3>) tensor([-1.2403, -7.5928, -9.6387, -5.2798, -1.6632], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9144, grad_fn=<NegBackward0>)\n",
      "tensor([0.3784, 0.3936, 0.3885, 0.3933, 0.3616], grad_fn=<SqueezeBackward3>) tensor([-1.2397, -7.5926, -9.6369, -5.2795, -1.6622], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9154, grad_fn=<NegBackward0>)\n",
      "tensor([0.3787, 0.3938, 0.3886, 0.3934, 0.3619], grad_fn=<SqueezeBackward3>) tensor([-1.2390, -7.5923, -9.6352, -5.2792, -1.6613], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9165, grad_fn=<NegBackward0>)\n",
      "tensor([0.3789, 0.3939, 0.3888, 0.3936, 0.3622], grad_fn=<SqueezeBackward3>) tensor([-1.2383, -7.5920, -9.6334, -5.2789, -1.6603], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9175, grad_fn=<NegBackward0>)\n",
      "tensor([0.3792, 0.3941, 0.3890, 0.3938, 0.3625], grad_fn=<SqueezeBackward3>) tensor([-1.2376, -7.5917, -9.6316, -5.2786, -1.6594], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9185, grad_fn=<NegBackward0>)\n",
      "tensor([0.3794, 0.3943, 0.3891, 0.3939, 0.3628], grad_fn=<SqueezeBackward3>) tensor([-1.2369, -7.5914, -9.6298, -5.2783, -1.6584], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9195, grad_fn=<NegBackward0>)\n",
      "tensor([0.3796, 0.3944, 0.3893, 0.3941, 0.3630], grad_fn=<SqueezeBackward3>) tensor([-1.2363, -7.5912, -9.6281, -5.2780, -1.6574], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9205, grad_fn=<NegBackward0>)\n",
      "tensor([0.3799, 0.3946, 0.3895, 0.3943, 0.3633], grad_fn=<SqueezeBackward3>) tensor([-1.2356, -7.5909, -9.6263, -5.2777, -1.6564], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9216, grad_fn=<NegBackward0>)\n",
      "tensor([0.3801, 0.3948, 0.3896, 0.3945, 0.3636], grad_fn=<SqueezeBackward3>) tensor([-1.2349, -7.5906, -9.6245, -5.2774, -1.6555], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9226, grad_fn=<NegBackward0>)\n",
      "tensor([0.3803, 0.3949, 0.3898, 0.3946, 0.3639], grad_fn=<SqueezeBackward3>) tensor([-1.2342, -7.5903, -9.6227, -5.2771, -1.6545], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9236, grad_fn=<NegBackward0>)\n",
      "tensor([0.3806, 0.3951, 0.3900, 0.3948, 0.3642], grad_fn=<SqueezeBackward3>) tensor([-1.2335, -7.5900, -9.6209, -5.2768, -1.6535], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9246, grad_fn=<NegBackward0>)\n",
      "tensor([0.3808, 0.3953, 0.3901, 0.3950, 0.3644], grad_fn=<SqueezeBackward3>) tensor([-1.2329, -7.5897, -9.6192, -5.2765, -1.6525], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9256, grad_fn=<NegBackward0>)\n",
      "tensor([0.3811, 0.3954, 0.3903, 0.3951, 0.3647], grad_fn=<SqueezeBackward3>) tensor([-1.2322, -7.5894, -9.6174, -5.2762, -1.6515], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9266, grad_fn=<NegBackward0>)\n",
      "tensor([0.3813, 0.3956, 0.3905, 0.3953, 0.3650], grad_fn=<SqueezeBackward3>) tensor([-1.2315, -7.5891, -9.6156, -5.2759, -1.6505], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9277, grad_fn=<NegBackward0>)\n",
      "tensor([0.3815, 0.3958, 0.3906, 0.3955, 0.3653], grad_fn=<SqueezeBackward3>) tensor([-1.2308, -7.5888, -9.6138, -5.2756, -1.6495], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9287, grad_fn=<NegBackward0>)\n",
      "tensor([0.3818, 0.3959, 0.3908, 0.3957, 0.3655], grad_fn=<SqueezeBackward3>) tensor([-1.2301, -7.5885, -9.6120, -5.2753, -1.6485], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9297, grad_fn=<NegBackward0>)\n",
      "tensor([0.3820, 0.3961, 0.3910, 0.3958, 0.3658], grad_fn=<SqueezeBackward3>) tensor([-1.2294, -7.5882, -9.6102, -5.2750, -1.6475], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9307, grad_fn=<NegBackward0>)\n",
      "tensor([0.3823, 0.3963, 0.3911, 0.3960, 0.3661], grad_fn=<SqueezeBackward3>) tensor([-1.2287, -7.5879, -9.6084, -5.2746, -1.6465], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9317, grad_fn=<NegBackward0>)\n",
      "tensor([0.3825, 0.3964, 0.3913, 0.3962, 0.3664], grad_fn=<SqueezeBackward3>) tensor([-1.2280, -7.5876, -9.6066, -5.2743, -1.6455], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9328, grad_fn=<NegBackward0>)\n",
      "tensor([0.3827, 0.3966, 0.3915, 0.3963, 0.3667], grad_fn=<SqueezeBackward3>) tensor([-1.2273, -7.5873, -9.6048, -5.2740, -1.6444], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9338, grad_fn=<NegBackward0>)\n",
      "tensor([0.3830, 0.3967, 0.3916, 0.3965, 0.3669], grad_fn=<SqueezeBackward3>) tensor([-1.2267, -7.5870, -9.6031, -5.2737, -1.6434], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9348, grad_fn=<NegBackward0>)\n",
      "tensor([0.3832, 0.3969, 0.3918, 0.3967, 0.3672], grad_fn=<SqueezeBackward3>) tensor([-1.2260, -7.5867, -9.6013, -5.2734, -1.6424], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9358, grad_fn=<NegBackward0>)\n",
      "tensor([0.3834, 0.3971, 0.3920, 0.3969, 0.3675], grad_fn=<SqueezeBackward3>) tensor([-1.2253, -7.5864, -9.5995, -5.2730, -1.6414], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9368, grad_fn=<NegBackward0>)\n",
      "tensor([0.3837, 0.3972, 0.3921, 0.3970, 0.3678], grad_fn=<SqueezeBackward3>) tensor([-1.2246, -7.5861, -9.5977, -5.2727, -1.6403], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9379, grad_fn=<NegBackward0>)\n",
      "tensor([0.3839, 0.3974, 0.3923, 0.3972, 0.3681], grad_fn=<SqueezeBackward3>) tensor([-1.2239, -7.5858, -9.5959, -5.2724, -1.6393], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9389, grad_fn=<NegBackward0>)\n",
      "tensor([0.3842, 0.3976, 0.3925, 0.3974, 0.3683], grad_fn=<SqueezeBackward3>) tensor([-1.2232, -7.5855, -9.5941, -5.2721, -1.6382], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9399, grad_fn=<NegBackward0>)\n",
      "tensor([0.3844, 0.3977, 0.3926, 0.3975, 0.3686], grad_fn=<SqueezeBackward3>) tensor([-1.2225, -7.5852, -9.5923, -5.2717, -1.6372], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9409, grad_fn=<NegBackward0>)\n",
      "tensor([0.3846, 0.3979, 0.3928, 0.3977, 0.3689], grad_fn=<SqueezeBackward3>) tensor([-1.2218, -7.5849, -9.5905, -5.2714, -1.6362], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9419, grad_fn=<NegBackward0>)\n",
      "tensor([0.3849, 0.3981, 0.3929, 0.3979, 0.3692], grad_fn=<SqueezeBackward3>) tensor([-1.2211, -7.5845, -9.5886, -5.2711, -1.6351], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9430, grad_fn=<NegBackward0>)\n",
      "tensor([0.3851, 0.3982, 0.3931, 0.3981, 0.3695], grad_fn=<SqueezeBackward3>) tensor([-1.2204, -7.5842, -9.5868, -5.2707, -1.6341], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9440, grad_fn=<NegBackward0>)\n",
      "tensor([0.3853, 0.3984, 0.3933, 0.3982, 0.3697], grad_fn=<SqueezeBackward3>) tensor([-1.2197, -7.5839, -9.5850, -5.2704, -1.6330], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9450, grad_fn=<NegBackward0>)\n",
      "tensor([0.3856, 0.3986, 0.3934, 0.3984, 0.3700], grad_fn=<SqueezeBackward3>) tensor([-1.2190, -7.5836, -9.5832, -5.2701, -1.6319], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9460, grad_fn=<NegBackward0>)\n",
      "tensor([0.3858, 0.3987, 0.3936, 0.3986, 0.3703], grad_fn=<SqueezeBackward3>) tensor([-1.2183, -7.5833, -9.5814, -5.2697, -1.6309], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9470, grad_fn=<NegBackward0>)\n",
      "tensor([0.3861, 0.3989, 0.3938, 0.3987, 0.3706], grad_fn=<SqueezeBackward3>) tensor([-1.2176, -7.5829, -9.5796, -5.2694, -1.6298], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9480, grad_fn=<NegBackward0>)\n",
      "tensor([0.3863, 0.3991, 0.3939, 0.3989, 0.3708], grad_fn=<SqueezeBackward3>) tensor([-1.2169, -7.5826, -9.5778, -5.2691, -1.6288], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9491, grad_fn=<NegBackward0>)\n",
      "tensor([0.3865, 0.3992, 0.3941, 0.3991, 0.3711], grad_fn=<SqueezeBackward3>) tensor([-1.2162, -7.5823, -9.5760, -5.2687, -1.6277], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9501, grad_fn=<NegBackward0>)\n",
      "tensor([0.3868, 0.3994, 0.3943, 0.3993, 0.3714], grad_fn=<SqueezeBackward3>) tensor([-1.2155, -7.5819, -9.5741, -5.2684, -1.6266], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9511, grad_fn=<NegBackward0>)\n",
      "tensor([0.3870, 0.3996, 0.3944, 0.3994, 0.3717], grad_fn=<SqueezeBackward3>) tensor([-1.2148, -7.5816, -9.5723, -5.2680, -1.6256], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9521, grad_fn=<NegBackward0>)\n",
      "tensor([0.3873, 0.3997, 0.3946, 0.3996, 0.3720], grad_fn=<SqueezeBackward3>) tensor([-1.2141, -7.5813, -9.5705, -5.2677, -1.6245], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9531, grad_fn=<NegBackward0>)\n",
      "tensor([0.3875, 0.3999, 0.3948, 0.3998, 0.3722], grad_fn=<SqueezeBackward3>) tensor([-1.2133, -7.5809, -9.5687, -5.2673, -1.6234], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9542, grad_fn=<NegBackward0>)\n",
      "tensor([0.3877, 0.4000, 0.3949, 0.3999, 0.3725], grad_fn=<SqueezeBackward3>) tensor([-1.2126, -7.5806, -9.5669, -5.2670, -1.6223], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9552, grad_fn=<NegBackward0>)\n",
      "tensor([0.3880, 0.4002, 0.3951, 0.4001, 0.3728], grad_fn=<SqueezeBackward3>) tensor([-1.2119, -7.5803, -9.5650, -5.2667, -1.6212], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9562, grad_fn=<NegBackward0>)\n",
      "tensor([0.3882, 0.4004, 0.3953, 0.4003, 0.3731], grad_fn=<SqueezeBackward3>) tensor([-1.2112, -7.5799, -9.5632, -5.2663, -1.6202], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9572, grad_fn=<NegBackward0>)\n",
      "tensor([0.3884, 0.4005, 0.3954, 0.4005, 0.3734], grad_fn=<SqueezeBackward3>) tensor([-1.2105, -7.5796, -9.5614, -5.2659, -1.6191], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9582, grad_fn=<NegBackward0>)\n",
      "tensor([0.3887, 0.4007, 0.3956, 0.4006, 0.3736], grad_fn=<SqueezeBackward3>) tensor([-1.2098, -7.5792, -9.5595, -5.2656, -1.6180], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9593, grad_fn=<NegBackward0>)\n",
      "tensor([0.3889, 0.4009, 0.3958, 0.4008, 0.3739], grad_fn=<SqueezeBackward3>) tensor([-1.2091, -7.5789, -9.5577, -5.2652, -1.6169], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9603, grad_fn=<NegBackward0>)\n",
      "tensor([0.3892, 0.4010, 0.3959, 0.4010, 0.3742], grad_fn=<SqueezeBackward3>) tensor([-1.2084, -7.5785, -9.5559, -5.2649, -1.6158], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9613, grad_fn=<NegBackward0>)\n",
      "tensor([0.3894, 0.4012, 0.3961, 0.4012, 0.3745], grad_fn=<SqueezeBackward3>) tensor([-1.2076, -7.5782, -9.5540, -5.2645, -1.6147], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9623, grad_fn=<NegBackward0>)\n",
      "tensor([0.3896, 0.4014, 0.3963, 0.4013, 0.3747], grad_fn=<SqueezeBackward3>) tensor([-1.2069, -7.5778, -9.5522, -5.2642, -1.6136], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9633, grad_fn=<NegBackward0>)\n",
      "tensor([0.3899, 0.4015, 0.3964, 0.4015, 0.3750], grad_fn=<SqueezeBackward3>) tensor([-1.2062, -7.5775, -9.5504, -5.2638, -1.6125], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9644, grad_fn=<NegBackward0>)\n",
      "tensor([0.3901, 0.4017, 0.3966, 0.4017, 0.3753], grad_fn=<SqueezeBackward3>) tensor([-1.2055, -7.5771, -9.5485, -5.2634, -1.6114], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9654, grad_fn=<NegBackward0>)\n",
      "tensor([0.3903, 0.4019, 0.3968, 0.4018, 0.3756], grad_fn=<SqueezeBackward3>) tensor([-1.2048, -7.5767, -9.5467, -5.2631, -1.6103], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9664, grad_fn=<NegBackward0>)\n",
      "tensor([0.3906, 0.4020, 0.3969, 0.4020, 0.3759], grad_fn=<SqueezeBackward3>) tensor([-1.2041, -7.5764, -9.5448, -5.2627, -1.6092], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9674, grad_fn=<NegBackward0>)\n",
      "tensor([0.3908, 0.4022, 0.3971, 0.4022, 0.3761], grad_fn=<SqueezeBackward3>) tensor([-1.2033, -7.5760, -9.5430, -5.2623, -1.6081], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9684, grad_fn=<NegBackward0>)\n",
      "tensor([0.3911, 0.4024, 0.3973, 0.4024, 0.3764], grad_fn=<SqueezeBackward3>) tensor([-1.2026, -7.5756, -9.5412, -5.2620, -1.6070], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9695, grad_fn=<NegBackward0>)\n",
      "tensor([0.3913, 0.4025, 0.3974, 0.4025, 0.3767], grad_fn=<SqueezeBackward3>) tensor([-1.2019, -7.5753, -9.5393, -5.2616, -1.6059], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9705, grad_fn=<NegBackward0>)\n",
      "tensor([0.3915, 0.4027, 0.3976, 0.4027, 0.3770], grad_fn=<SqueezeBackward3>) tensor([-1.2012, -7.5749, -9.5375, -5.2612, -1.6048], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9715, grad_fn=<NegBackward0>)\n",
      "tensor([0.3918, 0.4029, 0.3978, 0.4029, 0.3773], grad_fn=<SqueezeBackward3>) tensor([-1.2005, -7.5745, -9.5356, -5.2608, -1.6037], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9725, grad_fn=<NegBackward0>)\n",
      "tensor([0.3920, 0.4030, 0.3979, 0.4030, 0.3775], grad_fn=<SqueezeBackward3>) tensor([-1.1997, -7.5742, -9.5338, -5.2605, -1.6025], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9735, grad_fn=<NegBackward0>)\n",
      "tensor([0.3923, 0.4032, 0.3981, 0.4032, 0.3778], grad_fn=<SqueezeBackward3>) tensor([-1.1990, -7.5738, -9.5319, -5.2601, -1.6014], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9746, grad_fn=<NegBackward0>)\n",
      "tensor([0.3925, 0.4033, 0.3983, 0.4034, 0.3781], grad_fn=<SqueezeBackward3>) tensor([-1.1983, -7.5734, -9.5300, -5.2597, -1.6003], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9756, grad_fn=<NegBackward0>)\n",
      "tensor([0.3927, 0.4035, 0.3984, 0.4036, 0.3784], grad_fn=<SqueezeBackward3>) tensor([-1.1976, -7.5730, -9.5282, -5.2593, -1.5992], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9766, grad_fn=<NegBackward0>)\n",
      "tensor([0.3930, 0.4037, 0.3986, 0.4037, 0.3786], grad_fn=<SqueezeBackward3>) tensor([-1.1968, -7.5727, -9.5263, -5.2589, -1.5981], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9776, grad_fn=<NegBackward0>)\n",
      "tensor([0.3932, 0.4038, 0.3988, 0.4039, 0.3789], grad_fn=<SqueezeBackward3>) tensor([-1.1961, -7.5723, -9.5245, -5.2586, -1.5969], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9786, grad_fn=<NegBackward0>)\n",
      "tensor([0.3934, 0.4040, 0.3989, 0.4041, 0.3792], grad_fn=<SqueezeBackward3>) tensor([-1.1954, -7.5719, -9.5226, -5.2582, -1.5958], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9797, grad_fn=<NegBackward0>)\n",
      "tensor([0.3937, 0.4042, 0.3991, 0.4042, 0.3795], grad_fn=<SqueezeBackward3>) tensor([-1.1947, -7.5715, -9.5207, -5.2578, -1.5947], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9807, grad_fn=<NegBackward0>)\n",
      "tensor([0.3939, 0.4043, 0.3992, 0.4044, 0.3798], grad_fn=<SqueezeBackward3>) tensor([-1.1939, -7.5711, -9.5189, -5.2574, -1.5936], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9817, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3942, 0.4045, 0.3994, 0.4046, 0.3800], grad_fn=<SqueezeBackward3>) tensor([-1.1932, -7.5707, -9.5170, -5.2570, -1.5924], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9827, grad_fn=<NegBackward0>)\n",
      "tensor([0.3944, 0.4047, 0.3996, 0.4048, 0.3803], grad_fn=<SqueezeBackward3>) tensor([-1.1925, -7.5703, -9.5151, -5.2566, -1.5913], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9837, grad_fn=<NegBackward0>)\n",
      "tensor([0.3946, 0.4048, 0.3997, 0.4049, 0.3806], grad_fn=<SqueezeBackward3>) tensor([-1.1917, -7.5699, -9.5133, -5.2562, -1.5902], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9848, grad_fn=<NegBackward0>)\n",
      "tensor([0.3949, 0.4050, 0.3999, 0.4051, 0.3809], grad_fn=<SqueezeBackward3>) tensor([-1.1910, -7.5695, -9.5114, -5.2558, -1.5890], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9858, grad_fn=<NegBackward0>)\n",
      "tensor([0.3951, 0.4052, 0.4001, 0.4053, 0.3812], grad_fn=<SqueezeBackward3>) tensor([-1.1903, -7.5691, -9.5095, -5.2554, -1.5879], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9868, grad_fn=<NegBackward0>)\n",
      "tensor([0.3954, 0.4053, 0.4002, 0.4054, 0.3814], grad_fn=<SqueezeBackward3>) tensor([-1.1895, -7.5687, -9.5077, -5.2550, -1.5868], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9878, grad_fn=<NegBackward0>)\n",
      "tensor([0.3956, 0.4055, 0.4004, 0.4056, 0.3817], grad_fn=<SqueezeBackward3>) tensor([-1.1888, -7.5683, -9.5058, -5.2546, -1.5856], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9888, grad_fn=<NegBackward0>)\n",
      "tensor([0.3958, 0.4057, 0.4006, 0.4058, 0.3820], grad_fn=<SqueezeBackward3>) tensor([-1.1881, -7.5679, -9.5039, -5.2542, -1.5845], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9899, grad_fn=<NegBackward0>)\n",
      "tensor([0.3961, 0.4058, 0.4007, 0.4060, 0.3823], grad_fn=<SqueezeBackward3>) tensor([-1.1873, -7.5675, -9.5020, -5.2538, -1.5834], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9909, grad_fn=<NegBackward0>)\n",
      "tensor([0.3963, 0.4060, 0.4009, 0.4061, 0.3826], grad_fn=<SqueezeBackward3>) tensor([-1.1866, -7.5671, -9.5002, -5.2534, -1.5822], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9919, grad_fn=<NegBackward0>)\n",
      "tensor([0.3965, 0.4062, 0.4011, 0.4063, 0.3828], grad_fn=<SqueezeBackward3>) tensor([-1.1859, -7.5667, -9.4983, -5.2530, -1.5811], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9929, grad_fn=<NegBackward0>)\n",
      "tensor([0.3968, 0.4063, 0.4012, 0.4065, 0.3831], grad_fn=<SqueezeBackward3>) tensor([-1.1851, -7.5663, -9.4964, -5.2526, -1.5799], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9939, grad_fn=<NegBackward0>)\n",
      "tensor([0.3970, 0.4065, 0.4014, 0.4067, 0.3834], grad_fn=<SqueezeBackward3>) tensor([-1.1844, -7.5659, -9.4945, -5.2522, -1.5788], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9950, grad_fn=<NegBackward0>)\n",
      "tensor([0.3973, 0.4067, 0.4016, 0.4068, 0.3837], grad_fn=<SqueezeBackward3>) tensor([-1.1837, -7.5654, -9.4926, -5.2518, -1.5777], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9960, grad_fn=<NegBackward0>)\n",
      "tensor([0.3975, 0.4068, 0.4017, 0.4070, 0.3839], grad_fn=<SqueezeBackward3>) tensor([-1.1829, -7.5650, -9.4907, -5.2514, -1.5765], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9970, grad_fn=<NegBackward0>)\n",
      "tensor([0.3977, 0.4070, 0.4019, 0.4072, 0.3842], grad_fn=<SqueezeBackward3>) tensor([-1.1822, -7.5646, -9.4888, -5.2509, -1.5754], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9980, grad_fn=<NegBackward0>)\n",
      "tensor([0.3980, 0.4072, 0.4021, 0.4073, 0.3845], grad_fn=<SqueezeBackward3>) tensor([-1.1815, -7.5642, -9.4869, -5.2505, -1.5742], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9991, grad_fn=<NegBackward0>)\n",
      "tensor([0.3982, 0.4073, 0.4022, 0.4075, 0.3848], grad_fn=<SqueezeBackward3>) tensor([-1.1807, -7.5638, -9.4851, -5.2501, -1.5731], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0001, grad_fn=<NegBackward0>)\n",
      "tensor([0.3985, 0.4075, 0.4024, 0.4077, 0.3851], grad_fn=<SqueezeBackward3>) tensor([-1.1800, -7.5633, -9.4832, -5.2497, -1.5719], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0011, grad_fn=<NegBackward0>)\n",
      "tensor([0.3987, 0.4077, 0.4026, 0.4079, 0.3853], grad_fn=<SqueezeBackward3>) tensor([-1.1792, -7.5629, -9.4813, -5.2493, -1.5708], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0021, grad_fn=<NegBackward0>)\n",
      "tensor([0.3989, 0.4078, 0.4027, 0.4080, 0.3856], grad_fn=<SqueezeBackward3>) tensor([-1.1785, -7.5625, -9.4794, -5.2488, -1.5696], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0031, grad_fn=<NegBackward0>)\n",
      "tensor([0.3992, 0.4080, 0.4029, 0.4082, 0.3859], grad_fn=<SqueezeBackward3>) tensor([-1.1778, -7.5620, -9.4775, -5.2484, -1.5685], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0042, grad_fn=<NegBackward0>)\n",
      "tensor([0.3994, 0.4082, 0.4031, 0.4084, 0.3862], grad_fn=<SqueezeBackward3>) tensor([-1.1770, -7.5616, -9.4756, -5.2480, -1.5673], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0052, grad_fn=<NegBackward0>)\n",
      "tensor([0.3997, 0.4083, 0.4032, 0.4085, 0.3865], grad_fn=<SqueezeBackward3>) tensor([-1.1763, -7.5611, -9.4737, -5.2476, -1.5662], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0062, grad_fn=<NegBackward0>)\n",
      "tensor([0.3999, 0.4085, 0.4034, 0.4087, 0.3867], grad_fn=<SqueezeBackward3>) tensor([-1.1755, -7.5607, -9.4718, -5.2471, -1.5650], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0072, grad_fn=<NegBackward0>)\n",
      "tensor([0.4001, 0.4086, 0.4036, 0.4089, 0.3870], grad_fn=<SqueezeBackward3>) tensor([-1.1748, -7.5603, -9.4699, -5.2467, -1.5639], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0083, grad_fn=<NegBackward0>)\n",
      "tensor([0.4004, 0.4088, 0.4037, 0.4091, 0.3873], grad_fn=<SqueezeBackward3>) tensor([-1.1740, -7.5598, -9.4680, -5.2463, -1.5627], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0093, grad_fn=<NegBackward0>)\n",
      "tensor([0.4006, 0.4090, 0.4039, 0.4092, 0.3876], grad_fn=<SqueezeBackward3>) tensor([-1.1733, -7.5594, -9.4660, -5.2458, -1.5616], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0103, grad_fn=<NegBackward0>)\n",
      "tensor([0.4008, 0.4091, 0.4041, 0.4094, 0.3879], grad_fn=<SqueezeBackward3>) tensor([-1.1725, -7.5589, -9.4641, -5.2454, -1.5604], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0113, grad_fn=<NegBackward0>)\n",
      "tensor([0.4011, 0.4093, 0.4042, 0.4096, 0.3881], grad_fn=<SqueezeBackward3>) tensor([-1.1718, -7.5585, -9.4622, -5.2449, -1.5592], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0124, grad_fn=<NegBackward0>)\n",
      "tensor([0.4013, 0.4095, 0.4044, 0.4098, 0.3884], grad_fn=<SqueezeBackward3>) tensor([-1.1711, -7.5580, -9.4603, -5.2445, -1.5581], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0134, grad_fn=<NegBackward0>)\n",
      "tensor([0.4016, 0.4096, 0.4046, 0.4099, 0.3887], grad_fn=<SqueezeBackward3>) tensor([-1.1703, -7.5576, -9.4584, -5.2441, -1.5569], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0144, grad_fn=<NegBackward0>)\n",
      "tensor([0.4018, 0.4098, 0.4047, 0.4101, 0.3890], grad_fn=<SqueezeBackward3>) tensor([-1.1696, -7.5571, -9.4565, -5.2436, -1.5558], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0154, grad_fn=<NegBackward0>)\n",
      "tensor([0.4020, 0.4100, 0.4049, 0.4103, 0.3893], grad_fn=<SqueezeBackward3>) tensor([-1.1688, -7.5566, -9.4546, -5.2432, -1.5546], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0165, grad_fn=<NegBackward0>)\n",
      "tensor([0.4023, 0.4101, 0.4051, 0.4104, 0.3895], grad_fn=<SqueezeBackward3>) tensor([-1.1681, -7.5562, -9.4526, -5.2427, -1.5535], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0175, grad_fn=<NegBackward0>)\n",
      "tensor([0.4025, 0.4103, 0.4052, 0.4106, 0.3898], grad_fn=<SqueezeBackward3>) tensor([-1.1673, -7.5557, -9.4507, -5.2423, -1.5523], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0185, grad_fn=<NegBackward0>)\n",
      "tensor([0.4028, 0.4105, 0.4054, 0.4108, 0.3901], grad_fn=<SqueezeBackward3>) tensor([-1.1666, -7.5552, -9.4488, -5.2418, -1.5511], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0195, grad_fn=<NegBackward0>)\n",
      "tensor([0.4030, 0.4106, 0.4056, 0.4110, 0.3904], grad_fn=<SqueezeBackward3>) tensor([-1.1658, -7.5548, -9.4469, -5.2413, -1.5500], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0206, grad_fn=<NegBackward0>)\n",
      "tensor([0.4032, 0.4108, 0.4057, 0.4111, 0.3907], grad_fn=<SqueezeBackward3>) tensor([-1.1651, -7.5543, -9.4449, -5.2409, -1.5488], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0216, grad_fn=<NegBackward0>)\n",
      "tensor([0.4035, 0.4110, 0.4059, 0.4113, 0.3909], grad_fn=<SqueezeBackward3>) tensor([-1.1643, -7.5538, -9.4430, -5.2404, -1.5476], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0226, grad_fn=<NegBackward0>)\n",
      "tensor([0.4037, 0.4111, 0.4061, 0.4115, 0.3912], grad_fn=<SqueezeBackward3>) tensor([-1.1636, -7.5533, -9.4411, -5.2400, -1.5465], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0236, grad_fn=<NegBackward0>)\n",
      "tensor([0.4040, 0.4113, 0.4062, 0.4117, 0.3915], grad_fn=<SqueezeBackward3>) tensor([-1.1628, -7.5529, -9.4392, -5.2395, -1.5453], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0247, grad_fn=<NegBackward0>)\n",
      "tensor([0.4042, 0.4115, 0.4064, 0.4118, 0.3918], grad_fn=<SqueezeBackward3>) tensor([-1.1621, -7.5524, -9.4372, -5.2390, -1.5442], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0257, grad_fn=<NegBackward0>)\n",
      "tensor([0.4044, 0.4116, 0.4066, 0.4120, 0.3921], grad_fn=<SqueezeBackward3>) tensor([-1.1613, -7.5519, -9.4353, -5.2386, -1.5430], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0267, grad_fn=<NegBackward0>)\n",
      "tensor([0.4047, 0.4118, 0.4067, 0.4122, 0.3923], grad_fn=<SqueezeBackward3>) tensor([-1.1606, -7.5514, -9.4333, -5.2381, -1.5418], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0277, grad_fn=<NegBackward0>)\n",
      "tensor([0.4049, 0.4120, 0.4069, 0.4123, 0.3926], grad_fn=<SqueezeBackward3>) tensor([-1.1598, -7.5509, -9.4314, -5.2376, -1.5407], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0288, grad_fn=<NegBackward0>)\n",
      "tensor([0.4052, 0.4121, 0.4071, 0.4125, 0.3929], grad_fn=<SqueezeBackward3>) tensor([-1.1590, -7.5504, -9.4295, -5.2372, -1.5395], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0298, grad_fn=<NegBackward0>)\n",
      "tensor([0.4054, 0.4123, 0.4072, 0.4127, 0.3932], grad_fn=<SqueezeBackward3>) tensor([-1.1583, -7.5499, -9.4275, -5.2367, -1.5383], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0308, grad_fn=<NegBackward0>)\n",
      "tensor([0.4056, 0.4125, 0.4074, 0.4129, 0.3935], grad_fn=<SqueezeBackward3>) tensor([-1.1575, -7.5494, -9.4256, -5.2362, -1.5372], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0318, grad_fn=<NegBackward0>)\n",
      "tensor([0.4059, 0.4126, 0.4076, 0.4130, 0.3937], grad_fn=<SqueezeBackward3>) tensor([-1.1568, -7.5489, -9.4236, -5.2357, -1.5360], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0329, grad_fn=<NegBackward0>)\n",
      "tensor([0.4061, 0.4128, 0.4077, 0.4132, 0.3940], grad_fn=<SqueezeBackward3>) tensor([-1.1560, -7.5484, -9.4217, -5.2352, -1.5348], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0339, grad_fn=<NegBackward0>)\n",
      "tensor([0.4064, 0.4130, 0.4079, 0.4134, 0.3943], grad_fn=<SqueezeBackward3>) tensor([-1.1553, -7.5479, -9.4197, -5.2348, -1.5337], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0349, grad_fn=<NegBackward0>)\n",
      "tensor([0.4066, 0.4131, 0.4081, 0.4136, 0.3946], grad_fn=<SqueezeBackward3>) tensor([-1.1545, -7.5474, -9.4178, -5.2343, -1.5325], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0359, grad_fn=<NegBackward0>)\n",
      "tensor([0.4068, 0.4133, 0.4082, 0.4137, 0.3949], grad_fn=<SqueezeBackward3>) tensor([-1.1538, -7.5469, -9.4158, -5.2338, -1.5313], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0370, grad_fn=<NegBackward0>)\n",
      "tensor([0.4071, 0.4135, 0.4084, 0.4139, 0.3951], grad_fn=<SqueezeBackward3>) tensor([-1.1530, -7.5464, -9.4139, -5.2333, -1.5301], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0380, grad_fn=<NegBackward0>)\n",
      "tensor([0.4073, 0.4136, 0.4086, 0.4141, 0.3954], grad_fn=<SqueezeBackward3>) tensor([-1.1522, -7.5459, -9.4119, -5.2328, -1.5290], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0390, grad_fn=<NegBackward0>)\n",
      "tensor([0.4076, 0.4138, 0.4087, 0.4143, 0.3957], grad_fn=<SqueezeBackward3>) tensor([-1.1515, -7.5454, -9.4100, -5.2323, -1.5278], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0401, grad_fn=<NegBackward0>)\n",
      "tensor([0.4078, 0.4140, 0.4089, 0.4144, 0.3960], grad_fn=<SqueezeBackward3>) tensor([-1.1507, -7.5449, -9.4080, -5.2318, -1.5266], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0411, grad_fn=<NegBackward0>)\n",
      "tensor([0.4080, 0.4141, 0.4091, 0.4146, 0.3963], grad_fn=<SqueezeBackward3>) tensor([-1.1500, -7.5444, -9.4061, -5.2313, -1.5255], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0421, grad_fn=<NegBackward0>)\n",
      "tensor([0.4083, 0.4143, 0.4092, 0.4148, 0.3965], grad_fn=<SqueezeBackward3>) tensor([-1.1492, -7.5438, -9.4041, -5.2308, -1.5243], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0431, grad_fn=<NegBackward0>)\n",
      "tensor([0.4085, 0.4145, 0.4094, 0.4149, 0.3968], grad_fn=<SqueezeBackward3>) tensor([-1.1485, -7.5433, -9.4021, -5.2303, -1.5231], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0442, grad_fn=<NegBackward0>)\n",
      "tensor([0.4088, 0.4146, 0.4096, 0.4151, 0.3971], grad_fn=<SqueezeBackward3>) tensor([-1.1477, -7.5428, -9.4002, -5.2298, -1.5219], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0452, grad_fn=<NegBackward0>)\n",
      "tensor([0.4090, 0.4148, 0.4097, 0.4153, 0.3974], grad_fn=<SqueezeBackward3>) tensor([-1.1469, -7.5422, -9.3982, -5.2293, -1.5208], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0462, grad_fn=<NegBackward0>)\n",
      "tensor([0.4092, 0.4150, 0.4099, 0.4155, 0.3977], grad_fn=<SqueezeBackward3>) tensor([-1.1462, -7.5417, -9.3963, -5.2288, -1.5196], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0473, grad_fn=<NegBackward0>)\n",
      "tensor([0.4095, 0.4151, 0.4101, 0.4156, 0.3980], grad_fn=<SqueezeBackward3>) tensor([-1.1454, -7.5412, -9.3943, -5.2283, -1.5184], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0483, grad_fn=<NegBackward0>)\n",
      "tensor([0.4097, 0.4153, 0.4102, 0.4158, 0.3982], grad_fn=<SqueezeBackward3>) tensor([-1.1446, -7.5406, -9.3923, -5.2278, -1.5173], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0493, grad_fn=<NegBackward0>)\n",
      "tensor([0.4100, 0.4155, 0.4104, 0.4160, 0.3985], grad_fn=<SqueezeBackward3>) tensor([-1.1439, -7.5401, -9.3903, -5.2273, -1.5161], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0503, grad_fn=<NegBackward0>)\n",
      "tensor([0.4102, 0.4156, 0.4106, 0.4162, 0.3988], grad_fn=<SqueezeBackward3>) tensor([-1.1431, -7.5396, -9.3884, -5.2268, -1.5149], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0514, grad_fn=<NegBackward0>)\n",
      "tensor([0.4104, 0.4158, 0.4107, 0.4163, 0.3991], grad_fn=<SqueezeBackward3>) tensor([-1.1424, -7.5390, -9.3864, -5.2263, -1.5137], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0524, grad_fn=<NegBackward0>)\n",
      "tensor([0.4107, 0.4160, 0.4109, 0.4165, 0.3994], grad_fn=<SqueezeBackward3>) tensor([-1.1416, -7.5385, -9.3844, -5.2257, -1.5126], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0534, grad_fn=<NegBackward0>)\n",
      "tensor([0.4109, 0.4161, 0.4111, 0.4167, 0.3996], grad_fn=<SqueezeBackward3>) tensor([-1.1408, -7.5379, -9.3824, -5.2252, -1.5114], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0545, grad_fn=<NegBackward0>)\n",
      "tensor([0.4112, 0.4163, 0.4112, 0.4169, 0.3999], grad_fn=<SqueezeBackward3>) tensor([-1.1401, -7.5374, -9.3805, -5.2247, -1.5102], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0555, grad_fn=<NegBackward0>)\n",
      "tensor([0.4114, 0.4165, 0.4114, 0.4170, 0.4002], grad_fn=<SqueezeBackward3>) tensor([-1.1393, -7.5368, -9.3785, -5.2242, -1.5090], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0565, grad_fn=<NegBackward0>)\n",
      "tensor([0.4117, 0.4166, 0.4116, 0.4172, 0.4005], grad_fn=<SqueezeBackward3>) tensor([-1.1385, -7.5363, -9.3765, -5.2236, -1.5079], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0576, grad_fn=<NegBackward0>)\n",
      "tensor([0.4119, 0.4168, 0.4117, 0.4174, 0.4008], grad_fn=<SqueezeBackward3>) tensor([-1.1378, -7.5357, -9.3745, -5.2231, -1.5067], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0586, grad_fn=<NegBackward0>)\n",
      "tensor([0.4121, 0.4170, 0.4119, 0.4175, 0.4011], grad_fn=<SqueezeBackward3>) tensor([-1.1370, -7.5351, -9.3725, -5.2226, -1.5055], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0596, grad_fn=<NegBackward0>)\n",
      "tensor([0.4124, 0.4171, 0.4121, 0.4177, 0.4013], grad_fn=<SqueezeBackward3>) tensor([-1.1362, -7.5346, -9.3705, -5.2220, -1.5043], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0607, grad_fn=<NegBackward0>)\n",
      "tensor([0.4126, 0.4173, 0.4123, 0.4179, 0.4016], grad_fn=<SqueezeBackward3>) tensor([-1.1355, -7.5340, -9.3685, -5.2215, -1.5032], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0617, grad_fn=<NegBackward0>)\n",
      "tensor([0.4129, 0.4175, 0.4124, 0.4181, 0.4019], grad_fn=<SqueezeBackward3>) tensor([-1.1347, -7.5334, -9.3665, -5.2210, -1.5020], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0627, grad_fn=<NegBackward0>)\n",
      "tensor([0.4131, 0.4176, 0.4126, 0.4182, 0.4022], grad_fn=<SqueezeBackward3>) tensor([-1.1339, -7.5329, -9.3645, -5.2204, -1.5008], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0638, grad_fn=<NegBackward0>)\n",
      "tensor([0.4133, 0.4178, 0.4128, 0.4184, 0.4025], grad_fn=<SqueezeBackward3>) tensor([-1.1332, -7.5323, -9.3625, -5.2199, -1.4996], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0648, grad_fn=<NegBackward0>)\n",
      "tensor([0.4136, 0.4180, 0.4129, 0.4186, 0.4028], grad_fn=<SqueezeBackward3>) tensor([-1.1324, -7.5317, -9.3605, -5.2193, -1.4985], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0658, grad_fn=<NegBackward0>)\n",
      "tensor([0.4138, 0.4181, 0.4131, 0.4188, 0.4030], grad_fn=<SqueezeBackward3>) tensor([-1.1316, -7.5311, -9.3585, -5.2188, -1.4973], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0669, grad_fn=<NegBackward0>)\n",
      "tensor([0.4141, 0.4183, 0.4133, 0.4189, 0.4033], grad_fn=<SqueezeBackward3>) tensor([-1.1309, -7.5305, -9.3565, -5.2182, -1.4961], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0679, grad_fn=<NegBackward0>)\n",
      "tensor([0.4143, 0.4185, 0.4134, 0.4191, 0.4036], grad_fn=<SqueezeBackward3>) tensor([-1.1301, -7.5300, -9.3545, -5.2177, -1.4949], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0689, grad_fn=<NegBackward0>)\n",
      "tensor([0.4145, 0.4186, 0.4136, 0.4193, 0.4039], grad_fn=<SqueezeBackward3>) tensor([-1.1293, -7.5294, -9.3525, -5.2171, -1.4938], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0700, grad_fn=<NegBackward0>)\n",
      "tensor([0.4148, 0.4188, 0.4138, 0.4195, 0.4042], grad_fn=<SqueezeBackward3>) tensor([-1.1286, -7.5288, -9.3505, -5.2166, -1.4926], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0710, grad_fn=<NegBackward0>)\n",
      "tensor([0.4150, 0.4190, 0.4139, 0.4196, 0.4044], grad_fn=<SqueezeBackward3>) tensor([-1.1278, -7.5282, -9.3485, -5.2160, -1.4914], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0720, grad_fn=<NegBackward0>)\n",
      "tensor([0.4153, 0.4191, 0.4141, 0.4198, 0.4047], grad_fn=<SqueezeBackward3>) tensor([-1.1270, -7.5276, -9.3465, -5.2155, -1.4902], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0731, grad_fn=<NegBackward0>)\n",
      "tensor([0.4155, 0.4193, 0.4143, 0.4200, 0.4050], grad_fn=<SqueezeBackward3>) tensor([-1.1263, -7.5270, -9.3445, -5.2149, -1.4891], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0741, grad_fn=<NegBackward0>)\n",
      "tensor([0.4158, 0.4195, 0.4144, 0.4202, 0.4053], grad_fn=<SqueezeBackward3>) tensor([-1.1255, -7.5264, -9.3425, -5.2143, -1.4879], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0751, grad_fn=<NegBackward0>)\n",
      "tensor([0.4160, 0.4196, 0.4146, 0.4203, 0.4056], grad_fn=<SqueezeBackward3>) tensor([-1.1247, -7.5258, -9.3405, -5.2138, -1.4867], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0762, grad_fn=<NegBackward0>)\n",
      "tensor([0.4162, 0.4198, 0.4148, 0.4205, 0.4059], grad_fn=<SqueezeBackward3>) tensor([-1.1240, -7.5252, -9.3385, -5.2132, -1.4855], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0772, grad_fn=<NegBackward0>)\n",
      "tensor([0.4165, 0.4200, 0.4149, 0.4207, 0.4062], grad_fn=<SqueezeBackward3>) tensor([-1.1232, -7.5246, -9.3364, -5.2126, -1.4844], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0782, grad_fn=<NegBackward0>)\n",
      "tensor([0.4167, 0.4202, 0.4151, 0.4209, 0.4064], grad_fn=<SqueezeBackward3>) tensor([-1.1224, -7.5239, -9.3344, -5.2121, -1.4832], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0793, grad_fn=<NegBackward0>)\n",
      "tensor([0.4170, 0.4203, 0.4153, 0.4210, 0.4067], grad_fn=<SqueezeBackward3>) tensor([-1.1216, -7.5233, -9.3324, -5.2115, -1.4820], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0803, grad_fn=<NegBackward0>)\n",
      "tensor([0.4172, 0.4205, 0.4154, 0.4212, 0.4070], grad_fn=<SqueezeBackward3>) tensor([-1.1209, -7.5227, -9.3304, -5.2109, -1.4808], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0813, grad_fn=<NegBackward0>)\n",
      "tensor([0.4175, 0.4207, 0.4156, 0.4214, 0.4073], grad_fn=<SqueezeBackward3>) tensor([-1.1201, -7.5221, -9.3283, -5.2103, -1.4796], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0824, grad_fn=<NegBackward0>)\n",
      "tensor([0.4177, 0.4208, 0.4158, 0.4216, 0.4076], grad_fn=<SqueezeBackward3>) tensor([-1.1193, -7.5215, -9.3263, -5.2097, -1.4785], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0834, grad_fn=<NegBackward0>)\n",
      "tensor([0.4179, 0.4210, 0.4159, 0.4217, 0.4079], grad_fn=<SqueezeBackward3>) tensor([-1.1186, -7.5208, -9.3243, -5.2091, -1.4773], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0845, grad_fn=<NegBackward0>)\n",
      "tensor([0.4182, 0.4212, 0.4161, 0.4219, 0.4081], grad_fn=<SqueezeBackward3>) tensor([-1.1178, -7.5202, -9.3223, -5.2086, -1.4761], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0855, grad_fn=<NegBackward0>)\n",
      "tensor([0.4184, 0.4213, 0.4163, 0.4221, 0.4084], grad_fn=<SqueezeBackward3>) tensor([-1.1170, -7.5196, -9.3202, -5.2080, -1.4749], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0865, grad_fn=<NegBackward0>)\n",
      "tensor([0.4187, 0.4215, 0.4164, 0.4223, 0.4087], grad_fn=<SqueezeBackward3>) tensor([-1.1162, -7.5189, -9.3182, -5.2074, -1.4738], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0876, grad_fn=<NegBackward0>)\n",
      "tensor([0.4189, 0.4217, 0.4166, 0.4224, 0.4090], grad_fn=<SqueezeBackward3>) tensor([-1.1155, -7.5183, -9.3161, -5.2068, -1.4726], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0886, grad_fn=<NegBackward0>)\n",
      "tensor([0.4192, 0.4218, 0.4168, 0.4226, 0.4093], grad_fn=<SqueezeBackward3>) tensor([-1.1147, -7.5177, -9.3141, -5.2062, -1.4714], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0897, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4194, 0.4220, 0.4170, 0.4228, 0.4096], grad_fn=<SqueezeBackward3>) tensor([-1.1139, -7.5170, -9.3121, -5.2056, -1.4702], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0907, grad_fn=<NegBackward0>)\n",
      "tensor([0.4196, 0.4222, 0.4171, 0.4230, 0.4098], grad_fn=<SqueezeBackward3>) tensor([-1.1131, -7.5164, -9.3100, -5.2050, -1.4691], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0917, grad_fn=<NegBackward0>)\n",
      "tensor([0.4199, 0.4223, 0.4173, 0.4231, 0.4101], grad_fn=<SqueezeBackward3>) tensor([-1.1124, -7.5157, -9.3080, -5.2044, -1.4679], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0928, grad_fn=<NegBackward0>)\n",
      "tensor([0.4201, 0.4225, 0.4175, 0.4233, 0.4104], grad_fn=<SqueezeBackward3>) tensor([-1.1116, -7.5151, -9.3059, -5.2038, -1.4667], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0938, grad_fn=<NegBackward0>)\n",
      "tensor([0.4204, 0.4227, 0.4176, 0.4235, 0.4107], grad_fn=<SqueezeBackward3>) tensor([-1.1108, -7.5144, -9.3039, -5.2032, -1.4655], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0949, grad_fn=<NegBackward0>)\n",
      "tensor([0.4206, 0.4228, 0.4178, 0.4237, 0.4110], grad_fn=<SqueezeBackward3>) tensor([-1.1100, -7.5137, -9.3018, -5.2025, -1.4644], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0959, grad_fn=<NegBackward0>)\n",
      "tensor([0.4209, 0.4230, 0.4180, 0.4238, 0.4113], grad_fn=<SqueezeBackward3>) tensor([-1.1093, -7.5131, -9.2998, -5.2019, -1.4632], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0969, grad_fn=<NegBackward0>)\n",
      "tensor([0.4211, 0.4232, 0.4181, 0.4240, 0.4116], grad_fn=<SqueezeBackward3>) tensor([-1.1085, -7.5124, -9.2977, -5.2013, -1.4620], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0980, grad_fn=<NegBackward0>)\n",
      "tensor([0.4214, 0.4233, 0.4183, 0.4242, 0.4118], grad_fn=<SqueezeBackward3>) tensor([-1.1077, -7.5117, -9.2957, -5.2007, -1.4608], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0990, grad_fn=<NegBackward0>)\n",
      "tensor([0.4216, 0.4235, 0.4185, 0.4244, 0.4121], grad_fn=<SqueezeBackward3>) tensor([-1.1069, -7.5111, -9.2936, -5.2001, -1.4597], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1001, grad_fn=<NegBackward0>)\n",
      "tensor([0.4218, 0.4237, 0.4186, 0.4245, 0.4124], grad_fn=<SqueezeBackward3>) tensor([-1.1062, -7.5104, -9.2916, -5.1994, -1.4585], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1011, grad_fn=<NegBackward0>)\n",
      "tensor([0.4221, 0.4239, 0.4188, 0.4247, 0.4127], grad_fn=<SqueezeBackward3>) tensor([-1.1054, -7.5097, -9.2895, -5.1988, -1.4573], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1021, grad_fn=<NegBackward0>)\n",
      "tensor([0.4223, 0.4240, 0.4190, 0.4249, 0.4130], grad_fn=<SqueezeBackward3>) tensor([-1.1046, -7.5090, -9.2874, -5.1982, -1.4561], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1032, grad_fn=<NegBackward0>)\n",
      "tensor([0.4226, 0.4242, 0.4191, 0.4251, 0.4133], grad_fn=<SqueezeBackward3>) tensor([-1.1038, -7.5084, -9.2854, -5.1976, -1.4550], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1042, grad_fn=<NegBackward0>)\n",
      "tensor([0.4228, 0.4244, 0.4193, 0.4252, 0.4136], grad_fn=<SqueezeBackward3>) tensor([-1.1030, -7.5077, -9.2833, -5.1969, -1.4538], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1053, grad_fn=<NegBackward0>)\n",
      "tensor([0.4231, 0.4245, 0.4195, 0.4254, 0.4138], grad_fn=<SqueezeBackward3>) tensor([-1.1023, -7.5070, -9.2812, -5.1963, -1.4526], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1063, grad_fn=<NegBackward0>)\n",
      "tensor([0.4233, 0.4247, 0.4196, 0.4256, 0.4141], grad_fn=<SqueezeBackward3>) tensor([-1.1015, -7.5063, -9.2792, -5.1956, -1.4514], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1074, grad_fn=<NegBackward0>)\n",
      "tensor([0.4235, 0.4249, 0.4198, 0.4258, 0.4144], grad_fn=<SqueezeBackward3>) tensor([-1.1007, -7.5056, -9.2771, -5.1950, -1.4503], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1084, grad_fn=<NegBackward0>)\n",
      "tensor([0.4238, 0.4250, 0.4200, 0.4259, 0.4147], grad_fn=<SqueezeBackward3>) tensor([-1.0999, -7.5049, -9.2750, -5.1944, -1.4491], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1095, grad_fn=<NegBackward0>)\n",
      "tensor([0.4240, 0.4252, 0.4202, 0.4261, 0.4150], grad_fn=<SqueezeBackward3>) tensor([-1.0991, -7.5042, -9.2729, -5.1937, -1.4479], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1105, grad_fn=<NegBackward0>)\n",
      "tensor([0.4243, 0.4254, 0.4203, 0.4263, 0.4153], grad_fn=<SqueezeBackward3>) tensor([-1.0984, -7.5035, -9.2709, -5.1931, -1.4467], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1115, grad_fn=<NegBackward0>)\n",
      "tensor([0.4245, 0.4255, 0.4205, 0.4265, 0.4156], grad_fn=<SqueezeBackward3>) tensor([-1.0976, -7.5028, -9.2688, -5.1924, -1.4456], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1126, grad_fn=<NegBackward0>)\n",
      "tensor([0.4248, 0.4257, 0.4207, 0.4266, 0.4159], grad_fn=<SqueezeBackward3>) tensor([-1.0968, -7.5021, -9.2667, -5.1917, -1.4444], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1136, grad_fn=<NegBackward0>)\n",
      "tensor([0.4250, 0.4259, 0.4208, 0.4268, 0.4161], grad_fn=<SqueezeBackward3>) tensor([-1.0960, -7.5014, -9.2646, -5.1911, -1.4432], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1147, grad_fn=<NegBackward0>)\n",
      "tensor([0.4253, 0.4260, 0.4210, 0.4270, 0.4164], grad_fn=<SqueezeBackward3>) tensor([-1.0952, -7.5006, -9.2625, -5.1904, -1.4420], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1157, grad_fn=<NegBackward0>)\n",
      "tensor([0.4255, 0.4262, 0.4212, 0.4272, 0.4167], grad_fn=<SqueezeBackward3>) tensor([-1.0945, -7.4999, -9.2605, -5.1898, -1.4409], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1168, grad_fn=<NegBackward0>)\n",
      "tensor([0.4258, 0.4264, 0.4213, 0.4273, 0.4170], grad_fn=<SqueezeBackward3>) tensor([-1.0937, -7.4992, -9.2584, -5.1891, -1.4397], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1178, grad_fn=<NegBackward0>)\n",
      "tensor([0.4260, 0.4265, 0.4215, 0.4275, 0.4173], grad_fn=<SqueezeBackward3>) tensor([-1.0929, -7.4985, -9.2563, -5.1884, -1.4385], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1189, grad_fn=<NegBackward0>)\n",
      "tensor([0.4262, 0.4267, 0.4217, 0.4277, 0.4176], grad_fn=<SqueezeBackward3>) tensor([-1.0921, -7.4977, -9.2542, -5.1877, -1.4373], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1199, grad_fn=<NegBackward0>)\n",
      "tensor([0.4265, 0.4269, 0.4218, 0.4279, 0.4179], grad_fn=<SqueezeBackward3>) tensor([-1.0913, -7.4970, -9.2521, -5.1871, -1.4362], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1210, grad_fn=<NegBackward0>)\n",
      "tensor([0.4267, 0.4271, 0.4220, 0.4280, 0.4182], grad_fn=<SqueezeBackward3>) tensor([-1.0906, -7.4963, -9.2500, -5.1864, -1.4350], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1220, grad_fn=<NegBackward0>)\n",
      "tensor([0.4270, 0.4272, 0.4222, 0.4282, 0.4184], grad_fn=<SqueezeBackward3>) tensor([-1.0898, -7.4955, -9.2479, -5.1857, -1.4338], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1231, grad_fn=<NegBackward0>)\n",
      "tensor([0.4272, 0.4274, 0.4224, 0.4284, 0.4187], grad_fn=<SqueezeBackward3>) tensor([-1.0890, -7.4948, -9.2458, -5.1850, -1.4327], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1241, grad_fn=<NegBackward0>)\n",
      "tensor([0.4275, 0.4276, 0.4225, 0.4286, 0.4190], grad_fn=<SqueezeBackward3>) tensor([-1.0882, -7.4940, -9.2437, -5.1843, -1.4315], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1252, grad_fn=<NegBackward0>)\n",
      "tensor([0.4277, 0.4277, 0.4227, 0.4287, 0.4193], grad_fn=<SqueezeBackward3>) tensor([-1.0874, -7.4933, -9.2416, -5.1837, -1.4303], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1262, grad_fn=<NegBackward0>)\n",
      "tensor([0.4280, 0.4279, 0.4229, 0.4289, 0.4196], grad_fn=<SqueezeBackward3>) tensor([-1.0866, -7.4925, -9.2395, -5.1830, -1.4291], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1273, grad_fn=<NegBackward0>)\n",
      "tensor([0.4282, 0.4281, 0.4230, 0.4291, 0.4199], grad_fn=<SqueezeBackward3>) tensor([-1.0859, -7.4918, -9.2374, -5.1823, -1.4280], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1283, grad_fn=<NegBackward0>)\n",
      "tensor([0.4285, 0.4282, 0.4232, 0.4293, 0.4202], grad_fn=<SqueezeBackward3>) tensor([-1.0851, -7.4910, -9.2353, -5.1816, -1.4268], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1294, grad_fn=<NegBackward0>)\n",
      "tensor([0.4287, 0.4284, 0.4234, 0.4294, 0.4205], grad_fn=<SqueezeBackward3>) tensor([-1.0843, -7.4902, -9.2331, -5.1809, -1.4256], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1304, grad_fn=<NegBackward0>)\n",
      "tensor([0.4290, 0.4286, 0.4235, 0.4296, 0.4208], grad_fn=<SqueezeBackward3>) tensor([-1.0835, -7.4895, -9.2310, -5.1802, -1.4245], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1315, grad_fn=<NegBackward0>)\n",
      "tensor([0.4292, 0.4288, 0.4237, 0.4298, 0.4211], grad_fn=<SqueezeBackward3>) tensor([-1.0827, -7.4887, -9.2289, -5.1795, -1.4233], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1325, grad_fn=<NegBackward0>)\n",
      "tensor([0.4294, 0.4289, 0.4239, 0.4300, 0.4213], grad_fn=<SqueezeBackward3>) tensor([-1.0819, -7.4879, -9.2268, -5.1788, -1.4221], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1336, grad_fn=<NegBackward0>)\n",
      "tensor([0.4297, 0.4291, 0.4241, 0.4302, 0.4216], grad_fn=<SqueezeBackward3>) tensor([-1.0812, -7.4872, -9.2247, -5.1780, -1.4209], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1346, grad_fn=<NegBackward0>)\n",
      "tensor([0.4299, 0.4293, 0.4242, 0.4303, 0.4219], grad_fn=<SqueezeBackward3>) tensor([-1.0804, -7.4864, -9.2225, -5.1773, -1.4198], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1357, grad_fn=<NegBackward0>)\n",
      "tensor([0.4302, 0.4294, 0.4244, 0.4305, 0.4222], grad_fn=<SqueezeBackward3>) tensor([-1.0796, -7.4856, -9.2204, -5.1766, -1.4186], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1367, grad_fn=<NegBackward0>)\n",
      "tensor([0.4304, 0.4296, 0.4246, 0.4307, 0.4225], grad_fn=<SqueezeBackward3>) tensor([-1.0788, -7.4848, -9.2183, -5.1759, -1.4174], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1378, grad_fn=<NegBackward0>)\n",
      "tensor([0.4307, 0.4298, 0.4247, 0.4309, 0.4228], grad_fn=<SqueezeBackward3>) tensor([-1.0780, -7.4840, -9.2162, -5.1752, -1.4163], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1388, grad_fn=<NegBackward0>)\n",
      "tensor([0.4309, 0.4299, 0.4249, 0.4310, 0.4231], grad_fn=<SqueezeBackward3>) tensor([-1.0772, -7.4832, -9.2140, -5.1744, -1.4151], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1399, grad_fn=<NegBackward0>)\n",
      "tensor([0.4312, 0.4301, 0.4251, 0.4312, 0.4234], grad_fn=<SqueezeBackward3>) tensor([-1.0764, -7.4824, -9.2119, -5.1737, -1.4139], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1409, grad_fn=<NegBackward0>)\n",
      "tensor([0.4314, 0.4303, 0.4252, 0.4314, 0.4237], grad_fn=<SqueezeBackward3>) tensor([-1.0757, -7.4816, -9.2098, -5.1730, -1.4128], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1420, grad_fn=<NegBackward0>)\n",
      "tensor([0.4317, 0.4304, 0.4254, 0.4316, 0.4240], grad_fn=<SqueezeBackward3>) tensor([-1.0749, -7.4808, -9.2076, -5.1722, -1.4116], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1431, grad_fn=<NegBackward0>)\n",
      "tensor([0.4319, 0.4306, 0.4256, 0.4317, 0.4242], grad_fn=<SqueezeBackward3>) tensor([-1.0741, -7.4800, -9.2055, -5.1715, -1.4104], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1441, grad_fn=<NegBackward0>)\n",
      "tensor([0.4322, 0.4308, 0.4258, 0.4319, 0.4245], grad_fn=<SqueezeBackward3>) tensor([-1.0733, -7.4792, -9.2033, -5.1708, -1.4092], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1452, grad_fn=<NegBackward0>)\n",
      "tensor([0.4324, 0.4310, 0.4259, 0.4321, 0.4248], grad_fn=<SqueezeBackward3>) tensor([-1.0725, -7.4784, -9.2012, -5.1700, -1.4081], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1462, grad_fn=<NegBackward0>)\n",
      "tensor([0.4327, 0.4311, 0.4261, 0.4323, 0.4251], grad_fn=<SqueezeBackward3>) tensor([-1.0717, -7.4775, -9.1991, -5.1693, -1.4069], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1473, grad_fn=<NegBackward0>)\n",
      "tensor([0.4329, 0.4313, 0.4263, 0.4325, 0.4254], grad_fn=<SqueezeBackward3>) tensor([-1.0709, -7.4767, -9.1969, -5.1685, -1.4057], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1483, grad_fn=<NegBackward0>)\n",
      "tensor([0.4332, 0.4315, 0.4264, 0.4326, 0.4257], grad_fn=<SqueezeBackward3>) tensor([-1.0702, -7.4759, -9.1948, -5.1678, -1.4046], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1494, grad_fn=<NegBackward0>)\n",
      "tensor([0.4334, 0.4316, 0.4266, 0.4328, 0.4260], grad_fn=<SqueezeBackward3>) tensor([-1.0694, -7.4751, -9.1926, -5.1670, -1.4034], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1505, grad_fn=<NegBackward0>)\n",
      "tensor([0.4337, 0.4318, 0.4268, 0.4330, 0.4263], grad_fn=<SqueezeBackward3>) tensor([-1.0686, -7.4742, -9.1905, -5.1663, -1.4022], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1515, grad_fn=<NegBackward0>)\n",
      "tensor([0.4339, 0.4320, 0.4269, 0.4332, 0.4266], grad_fn=<SqueezeBackward3>) tensor([-1.0678, -7.4734, -9.1883, -5.1655, -1.4011], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1526, grad_fn=<NegBackward0>)\n",
      "tensor([0.4342, 0.4322, 0.4271, 0.4333, 0.4269], grad_fn=<SqueezeBackward3>) tensor([-1.0670, -7.4726, -9.1861, -5.1647, -1.3999], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1536, grad_fn=<NegBackward0>)\n",
      "tensor([0.4344, 0.4323, 0.4273, 0.4335, 0.4272], grad_fn=<SqueezeBackward3>) tensor([-1.0662, -7.4717, -9.1840, -5.1640, -1.3987], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1547, grad_fn=<NegBackward0>)\n",
      "tensor([0.4347, 0.4325, 0.4275, 0.4337, 0.4275], grad_fn=<SqueezeBackward3>) tensor([-1.0654, -7.4709, -9.1818, -5.1632, -1.3976], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1558, grad_fn=<NegBackward0>)\n",
      "tensor([0.4349, 0.4327, 0.4276, 0.4339, 0.4277], grad_fn=<SqueezeBackward3>) tensor([-1.0646, -7.4700, -9.1797, -5.1624, -1.3964], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1568, grad_fn=<NegBackward0>)\n",
      "tensor([0.4352, 0.4328, 0.4278, 0.4341, 0.4280], grad_fn=<SqueezeBackward3>) tensor([-1.0638, -7.4691, -9.1775, -5.1616, -1.3953], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1579, grad_fn=<NegBackward0>)\n",
      "tensor([0.4354, 0.4330, 0.4280, 0.4342, 0.4283], grad_fn=<SqueezeBackward3>) tensor([-1.0631, -7.4683, -9.1753, -5.1609, -1.3941], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1589, grad_fn=<NegBackward0>)\n",
      "tensor([0.4356, 0.4332, 0.4281, 0.4344, 0.4286], grad_fn=<SqueezeBackward3>) tensor([-1.0623, -7.4674, -9.1732, -5.1601, -1.3929], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1600, grad_fn=<NegBackward0>)\n",
      "tensor([0.4359, 0.4333, 0.4283, 0.4346, 0.4289], grad_fn=<SqueezeBackward3>) tensor([-1.0615, -7.4665, -9.1710, -5.1593, -1.3918], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1611, grad_fn=<NegBackward0>)\n",
      "tensor([0.4361, 0.4335, 0.4285, 0.4348, 0.4292], grad_fn=<SqueezeBackward3>) tensor([-1.0607, -7.4657, -9.1688, -5.1585, -1.3906], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1621, grad_fn=<NegBackward0>)\n",
      "tensor([0.4364, 0.4337, 0.4286, 0.4349, 0.4295], grad_fn=<SqueezeBackward3>) tensor([-1.0599, -7.4648, -9.1666, -5.1577, -1.3894], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1632, grad_fn=<NegBackward0>)\n",
      "tensor([0.4366, 0.4339, 0.4288, 0.4351, 0.4298], grad_fn=<SqueezeBackward3>) tensor([-1.0591, -7.4639, -9.1644, -5.1569, -1.3883], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1642, grad_fn=<NegBackward0>)\n",
      "tensor([0.4369, 0.4340, 0.4290, 0.4353, 0.4301], grad_fn=<SqueezeBackward3>) tensor([-1.0583, -7.4630, -9.1623, -5.1561, -1.3871], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1653, grad_fn=<NegBackward0>)\n",
      "tensor([0.4371, 0.4342, 0.4292, 0.4355, 0.4304], grad_fn=<SqueezeBackward3>) tensor([-1.0575, -7.4622, -9.1601, -5.1553, -1.3859], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1664, grad_fn=<NegBackward0>)\n",
      "tensor([0.4374, 0.4344, 0.4293, 0.4357, 0.4307], grad_fn=<SqueezeBackward3>) tensor([-1.0567, -7.4613, -9.1579, -5.1545, -1.3848], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1674, grad_fn=<NegBackward0>)\n",
      "tensor([0.4376, 0.4345, 0.4295, 0.4358, 0.4310], grad_fn=<SqueezeBackward3>) tensor([-1.0559, -7.4604, -9.1557, -5.1537, -1.3836], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1685, grad_fn=<NegBackward0>)\n",
      "tensor([0.4379, 0.4347, 0.4297, 0.4360, 0.4313], grad_fn=<SqueezeBackward3>) tensor([-1.0551, -7.4595, -9.1535, -5.1529, -1.3825], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1696, grad_fn=<NegBackward0>)\n",
      "tensor([0.4382, 0.4349, 0.4298, 0.4362, 0.4316], grad_fn=<SqueezeBackward3>) tensor([-1.0544, -7.4586, -9.1513, -5.1520, -1.3813], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1706, grad_fn=<NegBackward0>)\n",
      "tensor([0.4384, 0.4351, 0.4300, 0.4364, 0.4319], grad_fn=<SqueezeBackward3>) tensor([-1.0536, -7.4577, -9.1491, -5.1512, -1.3801], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1717, grad_fn=<NegBackward0>)\n",
      "tensor([0.4387, 0.4352, 0.4302, 0.4365, 0.4322], grad_fn=<SqueezeBackward3>) tensor([-1.0528, -7.4567, -9.1469, -5.1504, -1.3790], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1728, grad_fn=<NegBackward0>)\n",
      "tensor([0.4389, 0.4354, 0.4304, 0.4367, 0.4325], grad_fn=<SqueezeBackward3>) tensor([-1.0520, -7.4558, -9.1447, -5.1496, -1.3778], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1738, grad_fn=<NegBackward0>)\n",
      "tensor([0.4392, 0.4356, 0.4305, 0.4369, 0.4328], grad_fn=<SqueezeBackward3>) tensor([-1.0512, -7.4549, -9.1425, -5.1487, -1.3766], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1749, grad_fn=<NegBackward0>)\n",
      "tensor([0.4394, 0.4357, 0.4307, 0.4371, 0.4330], grad_fn=<SqueezeBackward3>) tensor([-1.0504, -7.4540, -9.1403, -5.1479, -1.3755], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1760, grad_fn=<NegBackward0>)\n",
      "tensor([0.4397, 0.4359, 0.4309, 0.4373, 0.4333], grad_fn=<SqueezeBackward3>) tensor([-1.0496, -7.4531, -9.1381, -5.1471, -1.3743], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1770, grad_fn=<NegBackward0>)\n",
      "tensor([0.4399, 0.4361, 0.4310, 0.4374, 0.4336], grad_fn=<SqueezeBackward3>) tensor([-1.0488, -7.4521, -9.1359, -5.1462, -1.3732], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1781, grad_fn=<NegBackward0>)\n",
      "tensor([0.4402, 0.4363, 0.4312, 0.4376, 0.4339], grad_fn=<SqueezeBackward3>) tensor([-1.0480, -7.4512, -9.1337, -5.1454, -1.3720], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1792, grad_fn=<NegBackward0>)\n",
      "tensor([0.4404, 0.4364, 0.4314, 0.4378, 0.4342], grad_fn=<SqueezeBackward3>) tensor([-1.0472, -7.4502, -9.1315, -5.1445, -1.3708], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1803, grad_fn=<NegBackward0>)\n",
      "tensor([0.4407, 0.4366, 0.4316, 0.4380, 0.4345], grad_fn=<SqueezeBackward3>) tensor([-1.0464, -7.4493, -9.1293, -5.1437, -1.3697], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1813, grad_fn=<NegBackward0>)\n",
      "tensor([0.4409, 0.4368, 0.4317, 0.4381, 0.4348], grad_fn=<SqueezeBackward3>) tensor([-1.0456, -7.4484, -9.1271, -5.1428, -1.3685], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1824, grad_fn=<NegBackward0>)\n",
      "tensor([0.4412, 0.4369, 0.4319, 0.4383, 0.4351], grad_fn=<SqueezeBackward3>) tensor([-1.0448, -7.4474, -9.1248, -5.1419, -1.3674], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1835, grad_fn=<NegBackward0>)\n",
      "tensor([0.4414, 0.4371, 0.4321, 0.4385, 0.4354], grad_fn=<SqueezeBackward3>) tensor([-1.0441, -7.4464, -9.1226, -5.1411, -1.3662], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1845, grad_fn=<NegBackward0>)\n",
      "tensor([0.4417, 0.4373, 0.4322, 0.4387, 0.4357], grad_fn=<SqueezeBackward3>) tensor([-1.0433, -7.4455, -9.1204, -5.1402, -1.3651], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1856, grad_fn=<NegBackward0>)\n",
      "tensor([0.4419, 0.4375, 0.4324, 0.4389, 0.4360], grad_fn=<SqueezeBackward3>) tensor([-1.0425, -7.4445, -9.1182, -5.1393, -1.3639], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1867, grad_fn=<NegBackward0>)\n",
      "tensor([0.4422, 0.4376, 0.4326, 0.4390, 0.4363], grad_fn=<SqueezeBackward3>) tensor([-1.0417, -7.4435, -9.1159, -5.1385, -1.3627], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1878, grad_fn=<NegBackward0>)\n",
      "tensor([0.4424, 0.4378, 0.4328, 0.4392, 0.4366], grad_fn=<SqueezeBackward3>) tensor([-1.0409, -7.4426, -9.1137, -5.1376, -1.3616], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1888, grad_fn=<NegBackward0>)\n",
      "tensor([0.4427, 0.4380, 0.4329, 0.4394, 0.4369], grad_fn=<SqueezeBackward3>) tensor([-1.0401, -7.4416, -9.1115, -5.1367, -1.3604], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1899, grad_fn=<NegBackward0>)\n",
      "tensor([0.4429, 0.4381, 0.4331, 0.4396, 0.4372], grad_fn=<SqueezeBackward3>) tensor([-1.0393, -7.4406, -9.1092, -5.1358, -1.3593], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1910, grad_fn=<NegBackward0>)\n",
      "tensor([0.4432, 0.4383, 0.4333, 0.4398, 0.4375], grad_fn=<SqueezeBackward3>) tensor([-1.0385, -7.4396, -9.1070, -5.1349, -1.3581], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1921, grad_fn=<NegBackward0>)\n",
      "tensor([0.4434, 0.4385, 0.4335, 0.4399, 0.4378], grad_fn=<SqueezeBackward3>) tensor([-1.0377, -7.4386, -9.1047, -5.1340, -1.3570], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1931, grad_fn=<NegBackward0>)\n",
      "tensor([0.4437, 0.4387, 0.4336, 0.4401, 0.4381], grad_fn=<SqueezeBackward3>) tensor([-1.0369, -7.4376, -9.1025, -5.1331, -1.3558], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1942, grad_fn=<NegBackward0>)\n",
      "tensor([0.4440, 0.4388, 0.4338, 0.4403, 0.4384], grad_fn=<SqueezeBackward3>) tensor([-1.0361, -7.4366, -9.1003, -5.1322, -1.3547], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1953, grad_fn=<NegBackward0>)\n",
      "tensor([0.4442, 0.4390, 0.4340, 0.4405, 0.4387], grad_fn=<SqueezeBackward3>) tensor([-1.0353, -7.4356, -9.0980, -5.1313, -1.3535], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1964, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "phi, b = l1norm_phi(Xs, W, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "69761a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7010, 0.8703]), tensor([0.7077, 0.8481], requires_grad=True))"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, l1norm_gragent(X, phi, b, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "ec5e0907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3850, 0.3733, 0.3764, 0.3736, 0.4035], grad_fn=<SqueezeBackward3>) tensor([-1.2422, -7.3037, -9.4131, -5.0872, -1.6766], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9118, grad_fn=<NegBackward0>)\n",
      "tensor([0.3851, 0.3734, 0.3765, 0.3737, 0.4038], grad_fn=<SqueezeBackward3>) tensor([-1.2428, -7.3034, -9.4141, -5.0870, -1.6775], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9126, grad_fn=<NegBackward0>)\n",
      "tensor([0.3853, 0.3735, 0.3766, 0.3738, 0.4041], grad_fn=<SqueezeBackward3>) tensor([-1.2435, -7.3030, -9.4152, -5.0869, -1.6783], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9134, grad_fn=<NegBackward0>)\n",
      "tensor([0.3855, 0.3736, 0.3767, 0.3740, 0.4044], grad_fn=<SqueezeBackward3>) tensor([-1.2441, -7.3027, -9.4163, -5.0868, -1.6791], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9142, grad_fn=<NegBackward0>)\n",
      "tensor([0.3857, 0.3738, 0.3768, 0.3741, 0.4047], grad_fn=<SqueezeBackward3>) tensor([-1.2447, -7.3024, -9.4174, -5.0867, -1.6800], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9150, grad_fn=<NegBackward0>)\n",
      "tensor([0.3859, 0.3739, 0.3769, 0.3742, 0.4050], grad_fn=<SqueezeBackward3>) tensor([-1.2453, -7.3020, -9.4184, -5.0865, -1.6808], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9158, grad_fn=<NegBackward0>)\n",
      "tensor([0.3861, 0.3740, 0.3770, 0.3743, 0.4052], grad_fn=<SqueezeBackward3>) tensor([-1.2459, -7.3017, -9.4195, -5.0864, -1.6816], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9166, grad_fn=<NegBackward0>)\n",
      "tensor([0.3863, 0.3741, 0.3772, 0.3744, 0.4055], grad_fn=<SqueezeBackward3>) tensor([-1.2465, -7.3014, -9.4206, -5.0863, -1.6824], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9174, grad_fn=<NegBackward0>)\n",
      "tensor([0.3865, 0.3742, 0.3773, 0.3745, 0.4058], grad_fn=<SqueezeBackward3>) tensor([-1.2471, -7.3010, -9.4217, -5.0862, -1.6833], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9183, grad_fn=<NegBackward0>)\n",
      "tensor([0.3867, 0.3743, 0.3774, 0.3746, 0.4061], grad_fn=<SqueezeBackward3>) tensor([-1.2477, -7.3007, -9.4228, -5.0861, -1.6841], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9191, grad_fn=<NegBackward0>)\n",
      "tensor([0.3868, 0.3744, 0.3775, 0.3747, 0.4064], grad_fn=<SqueezeBackward3>) tensor([-1.2483, -7.3004, -9.4238, -5.0860, -1.6849], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9199, grad_fn=<NegBackward0>)\n",
      "tensor([0.3870, 0.3745, 0.3776, 0.3749, 0.4067], grad_fn=<SqueezeBackward3>) tensor([-1.2489, -7.3000, -9.4249, -5.0858, -1.6857], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9207, grad_fn=<NegBackward0>)\n",
      "tensor([0.3872, 0.3746, 0.3777, 0.3750, 0.4070], grad_fn=<SqueezeBackward3>) tensor([-1.2495, -7.2997, -9.4260, -5.0857, -1.6865], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9215, grad_fn=<NegBackward0>)\n",
      "tensor([0.3874, 0.3747, 0.3778, 0.3751, 0.4072], grad_fn=<SqueezeBackward3>) tensor([-1.2501, -7.2994, -9.4271, -5.0856, -1.6873], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9223, grad_fn=<NegBackward0>)\n",
      "tensor([0.3876, 0.3748, 0.3780, 0.3752, 0.4075], grad_fn=<SqueezeBackward3>) tensor([-1.2507, -7.2991, -9.4282, -5.0855, -1.6881], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9231, grad_fn=<NegBackward0>)\n",
      "tensor([0.3878, 0.3749, 0.3781, 0.3753, 0.4078], grad_fn=<SqueezeBackward3>) tensor([-1.2513, -7.2987, -9.4292, -5.0854, -1.6889], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9239, grad_fn=<NegBackward0>)\n",
      "tensor([0.3880, 0.3750, 0.3782, 0.3754, 0.4081], grad_fn=<SqueezeBackward3>) tensor([-1.2519, -7.2984, -9.4303, -5.0852, -1.6898], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9247, grad_fn=<NegBackward0>)\n",
      "tensor([0.3882, 0.3751, 0.3783, 0.3755, 0.4084], grad_fn=<SqueezeBackward3>) tensor([-1.2525, -7.2981, -9.4314, -5.0851, -1.6905], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9255, grad_fn=<NegBackward0>)\n",
      "tensor([0.3884, 0.3753, 0.3784, 0.3756, 0.4087], grad_fn=<SqueezeBackward3>) tensor([-1.2531, -7.2977, -9.4325, -5.0850, -1.6913], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9263, grad_fn=<NegBackward0>)\n",
      "tensor([0.3885, 0.3754, 0.3785, 0.3758, 0.4090], grad_fn=<SqueezeBackward3>) tensor([-1.2538, -7.2974, -9.4336, -5.0849, -1.6921], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9271, grad_fn=<NegBackward0>)\n",
      "tensor([0.3887, 0.3755, 0.3786, 0.3759, 0.4092], grad_fn=<SqueezeBackward3>) tensor([-1.2544, -7.2971, -9.4347, -5.0848, -1.6929], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9280, grad_fn=<NegBackward0>)\n",
      "tensor([0.3889, 0.3756, 0.3788, 0.3760, 0.4095], grad_fn=<SqueezeBackward3>) tensor([-1.2550, -7.2967, -9.4357, -5.0847, -1.6937], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9288, grad_fn=<NegBackward0>)\n",
      "tensor([0.3891, 0.3757, 0.3789, 0.3761, 0.4098], grad_fn=<SqueezeBackward3>) tensor([-1.2556, -7.2964, -9.4368, -5.0845, -1.6945], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9296, grad_fn=<NegBackward0>)\n",
      "tensor([0.3893, 0.3758, 0.3790, 0.3762, 0.4101], grad_fn=<SqueezeBackward3>) tensor([-1.2562, -7.2961, -9.4379, -5.0844, -1.6952], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9304, grad_fn=<NegBackward0>)\n",
      "tensor([0.3895, 0.3759, 0.3791, 0.3763, 0.4104], grad_fn=<SqueezeBackward3>) tensor([-1.2568, -7.2958, -9.4390, -5.0843, -1.6960], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9312, grad_fn=<NegBackward0>)\n",
      "tensor([0.3897, 0.3760, 0.3792, 0.3764, 0.4107], grad_fn=<SqueezeBackward3>) tensor([-1.2574, -7.2954, -9.4401, -5.0842, -1.6967], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9320, grad_fn=<NegBackward0>)\n",
      "tensor([0.3899, 0.3761, 0.3793, 0.3765, 0.4110], grad_fn=<SqueezeBackward3>) tensor([-1.2580, -7.2951, -9.4412, -5.0841, -1.6975], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9328, grad_fn=<NegBackward0>)\n",
      "tensor([0.3901, 0.3762, 0.3794, 0.3767, 0.4113], grad_fn=<SqueezeBackward3>) tensor([-1.2586, -7.2947, -9.4423, -5.0839, -1.6982], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9336, grad_fn=<NegBackward0>)\n",
      "tensor([0.3902, 0.3763, 0.3796, 0.3768, 0.4116], grad_fn=<SqueezeBackward3>) tensor([-1.2592, -7.2944, -9.4433, -5.0838, -1.6990], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9344, grad_fn=<NegBackward0>)\n",
      "tensor([0.3904, 0.3764, 0.3797, 0.3769, 0.4118], grad_fn=<SqueezeBackward3>) tensor([-1.2598, -7.2941, -9.4444, -5.0837, -1.6997], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9353, grad_fn=<NegBackward0>)\n",
      "tensor([0.3906, 0.3765, 0.3798, 0.3770, 0.4121], grad_fn=<SqueezeBackward3>) tensor([-1.2604, -7.2937, -9.4455, -5.0836, -1.7004], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9361, grad_fn=<NegBackward0>)\n",
      "tensor([0.3908, 0.3766, 0.3799, 0.3771, 0.4124], grad_fn=<SqueezeBackward3>) tensor([-1.2610, -7.2934, -9.4466, -5.0834, -1.7011], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9369, grad_fn=<NegBackward0>)\n",
      "tensor([0.3910, 0.3768, 0.3800, 0.3772, 0.4127], grad_fn=<SqueezeBackward3>) tensor([-1.2616, -7.2931, -9.4477, -5.0833, -1.7018], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9377, grad_fn=<NegBackward0>)\n",
      "tensor([0.3912, 0.3769, 0.3801, 0.3773, 0.4130], grad_fn=<SqueezeBackward3>) tensor([-1.2622, -7.2927, -9.4488, -5.0832, -1.7025], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9385, grad_fn=<NegBackward0>)\n",
      "tensor([0.3914, 0.3770, 0.3802, 0.3774, 0.4133], grad_fn=<SqueezeBackward3>) tensor([-1.2628, -7.2924, -9.4499, -5.0831, -1.7032], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9393, grad_fn=<NegBackward0>)\n",
      "tensor([0.3916, 0.3771, 0.3804, 0.3776, 0.4136], grad_fn=<SqueezeBackward3>) tensor([-1.2634, -7.2920, -9.4509, -5.0829, -1.7039], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9402, grad_fn=<NegBackward0>)\n",
      "tensor([0.3918, 0.3772, 0.3805, 0.3777, 0.4139], grad_fn=<SqueezeBackward3>) tensor([-1.2640, -7.2917, -9.4520, -5.0828, -1.7046], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9410, grad_fn=<NegBackward0>)\n",
      "tensor([0.3920, 0.3773, 0.3806, 0.3778, 0.4142], grad_fn=<SqueezeBackward3>) tensor([-1.2646, -7.2914, -9.4531, -5.0827, -1.7052], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9418, grad_fn=<NegBackward0>)\n",
      "tensor([0.3922, 0.3774, 0.3807, 0.3779, 0.4145], grad_fn=<SqueezeBackward3>) tensor([-1.2652, -7.2910, -9.4542, -5.0825, -1.7059], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9426, grad_fn=<NegBackward0>)\n",
      "tensor([0.3923, 0.3775, 0.3808, 0.3780, 0.4147], grad_fn=<SqueezeBackward3>) tensor([-1.2658, -7.2907, -9.4553, -5.0824, -1.7065], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9434, grad_fn=<NegBackward0>)\n",
      "tensor([0.3925, 0.3776, 0.3809, 0.3781, 0.4150], grad_fn=<SqueezeBackward3>) tensor([-1.2664, -7.2903, -9.4564, -5.0823, -1.7071], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9442, grad_fn=<NegBackward0>)\n",
      "tensor([0.3927, 0.3777, 0.3810, 0.3782, 0.4153], grad_fn=<SqueezeBackward3>) tensor([-1.2670, -7.2900, -9.4575, -5.0822, -1.7077], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9451, grad_fn=<NegBackward0>)\n",
      "tensor([0.3929, 0.3778, 0.3812, 0.3783, 0.4156], grad_fn=<SqueezeBackward3>) tensor([-1.2676, -7.2896, -9.4586, -5.0820, -1.7083], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9459, grad_fn=<NegBackward0>)\n",
      "tensor([0.3931, 0.3779, 0.3813, 0.3785, 0.4159], grad_fn=<SqueezeBackward3>) tensor([-1.2682, -7.2893, -9.4597, -5.0819, -1.7089], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9467, grad_fn=<NegBackward0>)\n",
      "tensor([0.3933, 0.3781, 0.3814, 0.3786, 0.4162], grad_fn=<SqueezeBackward3>) tensor([-1.2688, -7.2889, -9.4607, -5.0818, -1.7095], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9475, grad_fn=<NegBackward0>)\n",
      "tensor([0.3935, 0.3782, 0.3815, 0.3787, 0.4165], grad_fn=<SqueezeBackward3>) tensor([-1.2694, -7.2886, -9.4618, -5.0816, -1.7101], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9483, grad_fn=<NegBackward0>)\n",
      "tensor([0.3937, 0.3783, 0.3816, 0.3788, 0.4168], grad_fn=<SqueezeBackward3>) tensor([-1.2700, -7.2882, -9.4629, -5.0815, -1.7106], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9491, grad_fn=<NegBackward0>)\n",
      "tensor([0.3939, 0.3784, 0.3817, 0.3789, 0.4171], grad_fn=<SqueezeBackward3>) tensor([-1.2706, -7.2879, -9.4640, -5.0814, -1.7112], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9500, grad_fn=<NegBackward0>)\n",
      "tensor([0.3941, 0.3785, 0.3818, 0.3790, 0.4174], grad_fn=<SqueezeBackward3>) tensor([-1.2712, -7.2875, -9.4651, -5.0812, -1.7117], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9508, grad_fn=<NegBackward0>)\n",
      "tensor([0.3943, 0.3786, 0.3820, 0.3791, 0.4177], grad_fn=<SqueezeBackward3>) tensor([-1.2718, -7.2872, -9.4662, -5.0811, -1.7122], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9516, grad_fn=<NegBackward0>)\n",
      "tensor([0.3944, 0.3787, 0.3821, 0.3792, 0.4180], grad_fn=<SqueezeBackward3>) tensor([-1.2724, -7.2868, -9.4673, -5.0810, -1.7127], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9524, grad_fn=<NegBackward0>)\n",
      "tensor([0.3946, 0.3788, 0.3822, 0.3794, 0.4183], grad_fn=<SqueezeBackward3>) tensor([-1.2730, -7.2865, -9.4684, -5.0808, -1.7132], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9533, grad_fn=<NegBackward0>)\n",
      "tensor([0.3948, 0.3789, 0.3823, 0.3795, 0.4186], grad_fn=<SqueezeBackward3>) tensor([-1.2736, -7.2861, -9.4695, -5.0807, -1.7137], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9541, grad_fn=<NegBackward0>)\n",
      "tensor([0.3950, 0.3790, 0.3824, 0.3796, 0.4189], grad_fn=<SqueezeBackward3>) tensor([-1.2742, -7.2857, -9.4705, -5.0805, -1.7141], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9549, grad_fn=<NegBackward0>)\n",
      "tensor([0.3952, 0.3791, 0.3825, 0.3797, 0.4192], grad_fn=<SqueezeBackward3>) tensor([-1.2748, -7.2854, -9.4716, -5.0804, -1.7145], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9557, grad_fn=<NegBackward0>)\n",
      "tensor([0.3954, 0.3792, 0.3826, 0.3798, 0.4194], grad_fn=<SqueezeBackward3>) tensor([-1.2754, -7.2850, -9.4727, -5.0803, -1.7150], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9565, grad_fn=<NegBackward0>)\n",
      "tensor([0.3956, 0.3793, 0.3828, 0.3799, 0.4197], grad_fn=<SqueezeBackward3>) tensor([-1.2760, -7.2847, -9.4738, -5.0801, -1.7154], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9574, grad_fn=<NegBackward0>)\n",
      "tensor([0.3958, 0.3795, 0.3829, 0.3800, 0.4200], grad_fn=<SqueezeBackward3>) tensor([-1.2765, -7.2843, -9.4749, -5.0800, -1.7157], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9582, grad_fn=<NegBackward0>)\n",
      "tensor([0.3960, 0.3796, 0.3830, 0.3801, 0.4203], grad_fn=<SqueezeBackward3>) tensor([-1.2771, -7.2839, -9.4760, -5.0798, -1.7161], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9590, grad_fn=<NegBackward0>)\n",
      "tensor([0.3962, 0.3797, 0.3831, 0.3803, 0.4206], grad_fn=<SqueezeBackward3>) tensor([-1.2777, -7.2836, -9.4771, -5.0797, -1.7165], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9598, grad_fn=<NegBackward0>)\n",
      "tensor([0.3964, 0.3798, 0.3832, 0.3804, 0.4209], grad_fn=<SqueezeBackward3>) tensor([-1.2783, -7.2832, -9.4782, -5.0796, -1.7168], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9607, grad_fn=<NegBackward0>)\n",
      "tensor([0.3966, 0.3799, 0.3833, 0.3805, 0.4212], grad_fn=<SqueezeBackward3>) tensor([-1.2789, -7.2828, -9.4793, -5.0794, -1.7171], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9615, grad_fn=<NegBackward0>)\n",
      "tensor([0.3968, 0.3800, 0.3834, 0.3806, 0.4215], grad_fn=<SqueezeBackward3>) tensor([-1.2795, -7.2825, -9.4804, -5.0793, -1.7174], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9623, grad_fn=<NegBackward0>)\n",
      "tensor([0.3970, 0.3801, 0.3836, 0.3807, 0.4218], grad_fn=<SqueezeBackward3>) tensor([-1.2801, -7.2821, -9.4815, -5.0791, -1.7177], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9632, grad_fn=<NegBackward0>)\n",
      "tensor([0.3971, 0.3802, 0.3837, 0.3808, 0.4221], grad_fn=<SqueezeBackward3>) tensor([-1.2807, -7.2818, -9.4826, -5.0790, -1.7179], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9640, grad_fn=<NegBackward0>)\n",
      "tensor([0.3973, 0.3803, 0.3838, 0.3809, 0.4224], grad_fn=<SqueezeBackward3>) tensor([-1.2813, -7.2814, -9.4837, -5.0788, -1.7182], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9648, grad_fn=<NegBackward0>)\n",
      "tensor([0.3975, 0.3804, 0.3839, 0.3811, 0.4227], grad_fn=<SqueezeBackward3>) tensor([-1.2819, -7.2810, -9.4847, -5.0787, -1.7184], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9656, grad_fn=<NegBackward0>)\n",
      "tensor([0.3977, 0.3805, 0.3840, 0.3812, 0.4230], grad_fn=<SqueezeBackward3>) tensor([-1.2824, -7.2806, -9.4858, -5.0785, -1.7186], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9665, grad_fn=<NegBackward0>)\n",
      "tensor([0.3979, 0.3806, 0.3841, 0.3813, 0.4233], grad_fn=<SqueezeBackward3>) tensor([-1.2830, -7.2803, -9.4869, -5.0784, -1.7187], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9673, grad_fn=<NegBackward0>)\n",
      "tensor([0.3981, 0.3808, 0.3843, 0.3814, 0.4236], grad_fn=<SqueezeBackward3>) tensor([-1.2836, -7.2799, -9.4880, -5.0782, -1.7189], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9681, grad_fn=<NegBackward0>)\n",
      "tensor([0.3983, 0.3809, 0.3844, 0.3815, 0.4239], grad_fn=<SqueezeBackward3>) tensor([-1.2842, -7.2795, -9.4891, -5.0781, -1.7190], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9690, grad_fn=<NegBackward0>)\n",
      "tensor([0.3985, 0.3810, 0.3845, 0.3816, 0.4242], grad_fn=<SqueezeBackward3>) tensor([-1.2848, -7.2792, -9.4902, -5.0779, -1.7191], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9698, grad_fn=<NegBackward0>)\n",
      "tensor([0.3987, 0.3811, 0.3846, 0.3817, 0.4245], grad_fn=<SqueezeBackward3>) tensor([-1.2854, -7.2788, -9.4913, -5.0778, -1.7192], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9706, grad_fn=<NegBackward0>)\n",
      "tensor([0.3989, 0.3812, 0.3847, 0.3818, 0.4248], grad_fn=<SqueezeBackward3>) tensor([-1.2860, -7.2784, -9.4924, -5.0777, -1.7192], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9715, grad_fn=<NegBackward0>)\n",
      "tensor([0.3991, 0.3813, 0.3848, 0.3820, 0.4251], grad_fn=<SqueezeBackward3>) tensor([-1.2865, -7.2781, -9.4935, -5.0775, -1.7192], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9723, grad_fn=<NegBackward0>)\n",
      "tensor([0.3993, 0.3814, 0.3849, 0.3821, 0.4254], grad_fn=<SqueezeBackward3>) tensor([-1.2871, -7.2777, -9.4946, -5.0774, -1.7192], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9731, grad_fn=<NegBackward0>)\n",
      "tensor([0.3995, 0.3815, 0.3851, 0.3822, 0.4257], grad_fn=<SqueezeBackward3>) tensor([-1.2877, -7.2773, -9.4957, -5.0772, -1.7192], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9740, grad_fn=<NegBackward0>)\n",
      "tensor([0.3997, 0.3816, 0.3852, 0.3823, 0.4260], grad_fn=<SqueezeBackward3>) tensor([-1.2883, -7.2769, -9.4968, -5.0771, -1.7192], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9748, grad_fn=<NegBackward0>)\n",
      "tensor([0.3999, 0.3817, 0.3853, 0.3824, 0.4263], grad_fn=<SqueezeBackward3>) tensor([-1.2889, -7.2766, -9.4979, -5.0769, -1.7191], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9756, grad_fn=<NegBackward0>)\n",
      "tensor([0.4001, 0.3818, 0.3854, 0.3825, 0.4266], grad_fn=<SqueezeBackward3>) tensor([-1.2894, -7.2762, -9.4990, -5.0767, -1.7190], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9765, grad_fn=<NegBackward0>)\n",
      "tensor([0.4003, 0.3819, 0.3855, 0.3826, 0.4269], grad_fn=<SqueezeBackward3>) tensor([-1.2900, -7.2758, -9.5001, -5.0766, -1.7189], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9773, grad_fn=<NegBackward0>)\n",
      "tensor([0.4004, 0.3821, 0.3856, 0.3828, 0.4272], grad_fn=<SqueezeBackward3>) tensor([-1.2906, -7.2754, -9.5012, -5.0764, -1.7187], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9781, grad_fn=<NegBackward0>)\n",
      "tensor([0.4006, 0.3822, 0.3857, 0.3829, 0.4275], grad_fn=<SqueezeBackward3>) tensor([-1.2912, -7.2751, -9.5023, -5.0763, -1.7185], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9790, grad_fn=<NegBackward0>)\n",
      "tensor([0.4008, 0.3823, 0.3859, 0.3830, 0.4279], grad_fn=<SqueezeBackward3>) tensor([-1.2918, -7.2747, -9.5034, -5.0761, -1.7183], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9798, grad_fn=<NegBackward0>)\n",
      "tensor([0.4010, 0.3824, 0.3860, 0.3831, 0.4282], grad_fn=<SqueezeBackward3>) tensor([-1.2923, -7.2743, -9.5045, -5.0760, -1.7181], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9807, grad_fn=<NegBackward0>)\n",
      "tensor([0.4012, 0.3825, 0.3861, 0.3832, 0.4285], grad_fn=<SqueezeBackward3>) tensor([-1.2929, -7.2739, -9.5056, -5.0758, -1.7178], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9815, grad_fn=<NegBackward0>)\n",
      "tensor([0.4014, 0.3826, 0.3862, 0.3833, 0.4288], grad_fn=<SqueezeBackward3>) tensor([-1.2935, -7.2735, -9.5067, -5.0757, -1.7175], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9823, grad_fn=<NegBackward0>)\n",
      "tensor([0.4016, 0.3827, 0.3863, 0.3834, 0.4291], grad_fn=<SqueezeBackward3>) tensor([-1.2941, -7.2732, -9.5078, -5.0755, -1.7172], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9832, grad_fn=<NegBackward0>)\n",
      "tensor([0.4018, 0.3828, 0.3864, 0.3836, 0.4294], grad_fn=<SqueezeBackward3>) tensor([-1.2946, -7.2728, -9.5089, -5.0754, -1.7168], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9840, grad_fn=<NegBackward0>)\n",
      "tensor([0.4020, 0.3829, 0.3866, 0.3837, 0.4297], grad_fn=<SqueezeBackward3>) tensor([-1.2952, -7.2724, -9.5100, -5.0752, -1.7164], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9849, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4022, 0.3830, 0.3867, 0.3838, 0.4300], grad_fn=<SqueezeBackward3>) tensor([-1.2958, -7.2720, -9.5111, -5.0751, -1.7160], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9857, grad_fn=<NegBackward0>)\n",
      "tensor([0.4024, 0.3831, 0.3868, 0.3839, 0.4303], grad_fn=<SqueezeBackward3>) tensor([-1.2964, -7.2716, -9.5122, -5.0749, -1.7156], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9865, grad_fn=<NegBackward0>)\n",
      "tensor([0.4026, 0.3833, 0.3869, 0.3840, 0.4306], grad_fn=<SqueezeBackward3>) tensor([-1.2969, -7.2713, -9.5133, -5.0747, -1.7151], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9874, grad_fn=<NegBackward0>)\n",
      "tensor([0.4028, 0.3834, 0.3870, 0.3841, 0.4309], grad_fn=<SqueezeBackward3>) tensor([-1.2975, -7.2709, -9.5144, -5.0746, -1.7146], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9882, grad_fn=<NegBackward0>)\n",
      "tensor([0.4030, 0.3835, 0.3871, 0.3842, 0.4312], grad_fn=<SqueezeBackward3>) tensor([-1.2981, -7.2705, -9.5155, -5.0744, -1.7141], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9891, grad_fn=<NegBackward0>)\n",
      "tensor([0.4032, 0.3836, 0.3873, 0.3844, 0.4315], grad_fn=<SqueezeBackward3>) tensor([-1.2986, -7.2701, -9.5166, -5.0743, -1.7135], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9899, grad_fn=<NegBackward0>)\n",
      "tensor([0.4034, 0.3837, 0.3874, 0.3845, 0.4319], grad_fn=<SqueezeBackward3>) tensor([-1.2992, -7.2697, -9.5177, -5.0741, -1.7129], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9908, grad_fn=<NegBackward0>)\n",
      "tensor([0.4036, 0.3838, 0.3875, 0.3846, 0.4322], grad_fn=<SqueezeBackward3>) tensor([-1.2998, -7.2693, -9.5189, -5.0740, -1.7123], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9916, grad_fn=<NegBackward0>)\n",
      "tensor([0.4038, 0.3839, 0.3876, 0.3847, 0.4325], grad_fn=<SqueezeBackward3>) tensor([-1.3004, -7.2690, -9.5200, -5.0738, -1.7117], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9925, grad_fn=<NegBackward0>)\n",
      "tensor([0.4040, 0.3840, 0.3877, 0.3848, 0.4328], grad_fn=<SqueezeBackward3>) tensor([-1.3009, -7.2686, -9.5211, -5.0736, -1.7110], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9933, grad_fn=<NegBackward0>)\n",
      "tensor([0.4042, 0.3841, 0.3878, 0.3849, 0.4331], grad_fn=<SqueezeBackward3>) tensor([-1.3015, -7.2682, -9.5222, -5.0735, -1.7103], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9942, grad_fn=<NegBackward0>)\n",
      "tensor([0.4044, 0.3842, 0.3879, 0.3850, 0.4334], grad_fn=<SqueezeBackward3>) tensor([-1.3021, -7.2678, -9.5233, -5.0733, -1.7096], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9950, grad_fn=<NegBackward0>)\n",
      "tensor([0.4046, 0.3843, 0.3881, 0.3852, 0.4337], grad_fn=<SqueezeBackward3>) tensor([-1.3026, -7.2674, -9.5244, -5.0732, -1.7088], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9959, grad_fn=<NegBackward0>)\n",
      "tensor([0.4048, 0.3845, 0.3882, 0.3853, 0.4340], grad_fn=<SqueezeBackward3>) tensor([-1.3032, -7.2670, -9.5255, -5.0730, -1.7080], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9967, grad_fn=<NegBackward0>)\n",
      "tensor([0.4050, 0.3846, 0.3883, 0.3854, 0.4344], grad_fn=<SqueezeBackward3>) tensor([-1.3038, -7.2666, -9.5266, -5.0729, -1.7072], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9976, grad_fn=<NegBackward0>)\n",
      "tensor([0.4052, 0.3847, 0.3884, 0.3855, 0.4347], grad_fn=<SqueezeBackward3>) tensor([-1.3043, -7.2663, -9.5277, -5.0727, -1.7064], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9984, grad_fn=<NegBackward0>)\n",
      "tensor([0.4054, 0.3848, 0.3885, 0.3856, 0.4350], grad_fn=<SqueezeBackward3>) tensor([-1.3049, -7.2659, -9.5288, -5.0725, -1.7055], grad_fn=<SubBackward0>)\n",
      "tensor(-1.9993, grad_fn=<NegBackward0>)\n",
      "tensor([0.4056, 0.3849, 0.3886, 0.3857, 0.4353], grad_fn=<SqueezeBackward3>) tensor([-1.3054, -7.2655, -9.5300, -5.0724, -1.7046], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0001, grad_fn=<NegBackward0>)\n",
      "tensor([0.4058, 0.3850, 0.3888, 0.3858, 0.4356], grad_fn=<SqueezeBackward3>) tensor([-1.3060, -7.2651, -9.5311, -5.0722, -1.7037], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0010, grad_fn=<NegBackward0>)\n",
      "tensor([0.4060, 0.3851, 0.3889, 0.3860, 0.4359], grad_fn=<SqueezeBackward3>) tensor([-1.3066, -7.2647, -9.5322, -5.0721, -1.7028], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0018, grad_fn=<NegBackward0>)\n",
      "tensor([0.4062, 0.3852, 0.3890, 0.3861, 0.4362], grad_fn=<SqueezeBackward3>) tensor([-1.3071, -7.2643, -9.5333, -5.0719, -1.7018], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0027, grad_fn=<NegBackward0>)\n",
      "tensor([0.4064, 0.3853, 0.3891, 0.3862, 0.4366], grad_fn=<SqueezeBackward3>) tensor([-1.3077, -7.2639, -9.5344, -5.0717, -1.7008], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0035, grad_fn=<NegBackward0>)\n",
      "tensor([0.4066, 0.3854, 0.3892, 0.3863, 0.4369], grad_fn=<SqueezeBackward3>) tensor([-1.3082, -7.2636, -9.5355, -5.0716, -1.6998], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0044, grad_fn=<NegBackward0>)\n",
      "tensor([0.4068, 0.3855, 0.3893, 0.3864, 0.4372], grad_fn=<SqueezeBackward3>) tensor([-1.3088, -7.2632, -9.5366, -5.0714, -1.6988], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0053, grad_fn=<NegBackward0>)\n",
      "tensor([0.4070, 0.3857, 0.3895, 0.3865, 0.4375], grad_fn=<SqueezeBackward3>) tensor([-1.3094, -7.2628, -9.5378, -5.0713, -1.6977], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0061, grad_fn=<NegBackward0>)\n",
      "tensor([0.4072, 0.3858, 0.3896, 0.3866, 0.4378], grad_fn=<SqueezeBackward3>) tensor([-1.3099, -7.2624, -9.5389, -5.0711, -1.6967], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0070, grad_fn=<NegBackward0>)\n",
      "tensor([0.4074, 0.3859, 0.3897, 0.3868, 0.4382], grad_fn=<SqueezeBackward3>) tensor([-1.3105, -7.2620, -9.5400, -5.0710, -1.6956], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0078, grad_fn=<NegBackward0>)\n",
      "tensor([0.4076, 0.3860, 0.3898, 0.3869, 0.4385], grad_fn=<SqueezeBackward3>) tensor([-1.3110, -7.2616, -9.5411, -5.0708, -1.6944], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0087, grad_fn=<NegBackward0>)\n",
      "tensor([0.4078, 0.3861, 0.3899, 0.3870, 0.4388], grad_fn=<SqueezeBackward3>) tensor([-1.3116, -7.2612, -9.5422, -5.0706, -1.6933], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0096, grad_fn=<NegBackward0>)\n",
      "tensor([0.4080, 0.3862, 0.3900, 0.3871, 0.4391], grad_fn=<SqueezeBackward3>) tensor([-1.3121, -7.2609, -9.5434, -5.0705, -1.6922], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0104, grad_fn=<NegBackward0>)\n",
      "tensor([0.4082, 0.3863, 0.3902, 0.3872, 0.4394], grad_fn=<SqueezeBackward3>) tensor([-1.3127, -7.2605, -9.5445, -5.0703, -1.6910], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0113, grad_fn=<NegBackward0>)\n",
      "tensor([0.4084, 0.3864, 0.3903, 0.3873, 0.4398], grad_fn=<SqueezeBackward3>) tensor([-1.3132, -7.2601, -9.5456, -5.0702, -1.6898], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0121, grad_fn=<NegBackward0>)\n",
      "tensor([0.4086, 0.3865, 0.3904, 0.3874, 0.4401], grad_fn=<SqueezeBackward3>) tensor([-1.3138, -7.2597, -9.5467, -5.0700, -1.6886], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0130, grad_fn=<NegBackward0>)\n",
      "tensor([0.4088, 0.3866, 0.3905, 0.3876, 0.4404], grad_fn=<SqueezeBackward3>) tensor([-1.3143, -7.2593, -9.5479, -5.0698, -1.6873], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0139, grad_fn=<NegBackward0>)\n",
      "tensor([0.4090, 0.3868, 0.3906, 0.3877, 0.4407], grad_fn=<SqueezeBackward3>) tensor([-1.3149, -7.2589, -9.5490, -5.0697, -1.6861], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0147, grad_fn=<NegBackward0>)\n",
      "tensor([0.4092, 0.3869, 0.3907, 0.3878, 0.4411], grad_fn=<SqueezeBackward3>) tensor([-1.3154, -7.2585, -9.5501, -5.0695, -1.6848], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0156, grad_fn=<NegBackward0>)\n",
      "tensor([0.4094, 0.3870, 0.3909, 0.3879, 0.4414], grad_fn=<SqueezeBackward3>) tensor([-1.3160, -7.2581, -9.5512, -5.0694, -1.6836], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0165, grad_fn=<NegBackward0>)\n",
      "tensor([0.4096, 0.3871, 0.3910, 0.3880, 0.4417], grad_fn=<SqueezeBackward3>) tensor([-1.3165, -7.2578, -9.5524, -5.0692, -1.6823], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0173, grad_fn=<NegBackward0>)\n",
      "tensor([0.4098, 0.3872, 0.3911, 0.3881, 0.4420], grad_fn=<SqueezeBackward3>) tensor([-1.3171, -7.2574, -9.5535, -5.0690, -1.6810], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0182, grad_fn=<NegBackward0>)\n",
      "tensor([0.4100, 0.3873, 0.3912, 0.3883, 0.4424], grad_fn=<SqueezeBackward3>) tensor([-1.3176, -7.2570, -9.5546, -5.0689, -1.6797], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0191, grad_fn=<NegBackward0>)\n",
      "tensor([0.4102, 0.3874, 0.3913, 0.3884, 0.4427], grad_fn=<SqueezeBackward3>) tensor([-1.3182, -7.2566, -9.5558, -5.0687, -1.6783], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0200, grad_fn=<NegBackward0>)\n",
      "tensor([0.4104, 0.3875, 0.3914, 0.3885, 0.4430], grad_fn=<SqueezeBackward3>) tensor([-1.3187, -7.2562, -9.5569, -5.0686, -1.6770], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0208, grad_fn=<NegBackward0>)\n",
      "tensor([0.4106, 0.3876, 0.3916, 0.3886, 0.4433], grad_fn=<SqueezeBackward3>) tensor([-1.3193, -7.2558, -9.5580, -5.0684, -1.6756], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0217, grad_fn=<NegBackward0>)\n",
      "tensor([0.4108, 0.3877, 0.3917, 0.3887, 0.4437], grad_fn=<SqueezeBackward3>) tensor([-1.3198, -7.2554, -9.5591, -5.0682, -1.6743], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0226, grad_fn=<NegBackward0>)\n",
      "tensor([0.4110, 0.3879, 0.3918, 0.3888, 0.4440], grad_fn=<SqueezeBackward3>) tensor([-1.3203, -7.2550, -9.5603, -5.0681, -1.6729], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0235, grad_fn=<NegBackward0>)\n",
      "tensor([0.4112, 0.3880, 0.3919, 0.3889, 0.4443], grad_fn=<SqueezeBackward3>) tensor([-1.3209, -7.2547, -9.5614, -5.0679, -1.6715], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0243, grad_fn=<NegBackward0>)\n",
      "tensor([0.4114, 0.3881, 0.3920, 0.3891, 0.4447], grad_fn=<SqueezeBackward3>) tensor([-1.3214, -7.2543, -9.5625, -5.0678, -1.6701], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0252, grad_fn=<NegBackward0>)\n",
      "tensor([0.4116, 0.3882, 0.3921, 0.3892, 0.4450], grad_fn=<SqueezeBackward3>) tensor([-1.3220, -7.2539, -9.5637, -5.0676, -1.6687], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0261, grad_fn=<NegBackward0>)\n",
      "tensor([0.4118, 0.3883, 0.3923, 0.3893, 0.4453], grad_fn=<SqueezeBackward3>) tensor([-1.3225, -7.2535, -9.5648, -5.0675, -1.6673], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0270, grad_fn=<NegBackward0>)\n",
      "tensor([0.4120, 0.3884, 0.3924, 0.3894, 0.4457], grad_fn=<SqueezeBackward3>) tensor([-1.3230, -7.2531, -9.5660, -5.0673, -1.6659], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0278, grad_fn=<NegBackward0>)\n",
      "tensor([0.4122, 0.3885, 0.3925, 0.3895, 0.4460], grad_fn=<SqueezeBackward3>) tensor([-1.3236, -7.2527, -9.5671, -5.0671, -1.6645], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0287, grad_fn=<NegBackward0>)\n",
      "tensor([0.4124, 0.3886, 0.3926, 0.3896, 0.4463], grad_fn=<SqueezeBackward3>) tensor([-1.3241, -7.2523, -9.5682, -5.0670, -1.6630], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0296, grad_fn=<NegBackward0>)\n",
      "tensor([0.4126, 0.3887, 0.3927, 0.3898, 0.4467], grad_fn=<SqueezeBackward3>) tensor([-1.3246, -7.2519, -9.5694, -5.0668, -1.6616], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0305, grad_fn=<NegBackward0>)\n",
      "tensor([0.4128, 0.3888, 0.3928, 0.3899, 0.4470], grad_fn=<SqueezeBackward3>) tensor([-1.3252, -7.2516, -9.5705, -5.0667, -1.6601], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0314, grad_fn=<NegBackward0>)\n",
      "tensor([0.4130, 0.3890, 0.3930, 0.3900, 0.4473], grad_fn=<SqueezeBackward3>) tensor([-1.3257, -7.2512, -9.5717, -5.0665, -1.6586], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0322, grad_fn=<NegBackward0>)\n",
      "tensor([0.4132, 0.3891, 0.3931, 0.3901, 0.4477], grad_fn=<SqueezeBackward3>) tensor([-1.3262, -7.2508, -9.5728, -5.0664, -1.6572], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0331, grad_fn=<NegBackward0>)\n",
      "tensor([0.4134, 0.3892, 0.3932, 0.3902, 0.4480], grad_fn=<SqueezeBackward3>) tensor([-1.3268, -7.2504, -9.5739, -5.0662, -1.6557], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0340, grad_fn=<NegBackward0>)\n",
      "tensor([0.4136, 0.3893, 0.3933, 0.3903, 0.4483], grad_fn=<SqueezeBackward3>) tensor([-1.3273, -7.2500, -9.5751, -5.0660, -1.6542], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0349, grad_fn=<NegBackward0>)\n",
      "tensor([0.4138, 0.3894, 0.3934, 0.3905, 0.4487], grad_fn=<SqueezeBackward3>) tensor([-1.3278, -7.2496, -9.5762, -5.0659, -1.6527], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0358, grad_fn=<NegBackward0>)\n",
      "tensor([0.4140, 0.3895, 0.3936, 0.3906, 0.4490], grad_fn=<SqueezeBackward3>) tensor([-1.3284, -7.2492, -9.5774, -5.0657, -1.6512], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0367, grad_fn=<NegBackward0>)\n",
      "tensor([0.4142, 0.3896, 0.3937, 0.3907, 0.4494], grad_fn=<SqueezeBackward3>) tensor([-1.3289, -7.2489, -9.5785, -5.0656, -1.6497], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0376, grad_fn=<NegBackward0>)\n",
      "tensor([0.4144, 0.3897, 0.3938, 0.3908, 0.4497], grad_fn=<SqueezeBackward3>) tensor([-1.3294, -7.2485, -9.5797, -5.0654, -1.6482], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0384, grad_fn=<NegBackward0>)\n",
      "tensor([0.4146, 0.3898, 0.3939, 0.3909, 0.4500], grad_fn=<SqueezeBackward3>) tensor([-1.3299, -7.2481, -9.5808, -5.0653, -1.6467], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0393, grad_fn=<NegBackward0>)\n",
      "tensor([0.4148, 0.3899, 0.3940, 0.3910, 0.4504], grad_fn=<SqueezeBackward3>) tensor([-1.3305, -7.2477, -9.5820, -5.0651, -1.6452], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0402, grad_fn=<NegBackward0>)\n",
      "tensor([0.4151, 0.3901, 0.3941, 0.3911, 0.4507], grad_fn=<SqueezeBackward3>) tensor([-1.3310, -7.2473, -9.5831, -5.0650, -1.6437], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0411, grad_fn=<NegBackward0>)\n",
      "tensor([0.4153, 0.3902, 0.3943, 0.3913, 0.4511], grad_fn=<SqueezeBackward3>) tensor([-1.3315, -7.2469, -9.5843, -5.0648, -1.6422], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0420, grad_fn=<NegBackward0>)\n",
      "tensor([0.4155, 0.3903, 0.3944, 0.3914, 0.4514], grad_fn=<SqueezeBackward3>) tensor([-1.3320, -7.2466, -9.5854, -5.0646, -1.6407], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0429, grad_fn=<NegBackward0>)\n",
      "tensor([0.4157, 0.3904, 0.3945, 0.3915, 0.4517], grad_fn=<SqueezeBackward3>) tensor([-1.3325, -7.2462, -9.5866, -5.0645, -1.6391], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0438, grad_fn=<NegBackward0>)\n",
      "tensor([0.4159, 0.3905, 0.3946, 0.3916, 0.4521], grad_fn=<SqueezeBackward3>) tensor([-1.3331, -7.2458, -9.5877, -5.0643, -1.6376], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0447, grad_fn=<NegBackward0>)\n",
      "tensor([0.4161, 0.3906, 0.3947, 0.3917, 0.4524], grad_fn=<SqueezeBackward3>) tensor([-1.3336, -7.2454, -9.5889, -5.0642, -1.6361], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0456, grad_fn=<NegBackward0>)\n",
      "tensor([0.4163, 0.3907, 0.3949, 0.3918, 0.4528], grad_fn=<SqueezeBackward3>) tensor([-1.3341, -7.2450, -9.5900, -5.0640, -1.6345], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0465, grad_fn=<NegBackward0>)\n",
      "tensor([0.4165, 0.3908, 0.3950, 0.3920, 0.4531], grad_fn=<SqueezeBackward3>) tensor([-1.3346, -7.2446, -9.5912, -5.0639, -1.6330], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0474, grad_fn=<NegBackward0>)\n",
      "tensor([0.4167, 0.3909, 0.3951, 0.3921, 0.4535], grad_fn=<SqueezeBackward3>) tensor([-1.3351, -7.2443, -9.5924, -5.0637, -1.6314], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0483, grad_fn=<NegBackward0>)\n",
      "tensor([0.4169, 0.3911, 0.3952, 0.3922, 0.4538], grad_fn=<SqueezeBackward3>) tensor([-1.3356, -7.2439, -9.5935, -5.0636, -1.6299], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0492, grad_fn=<NegBackward0>)\n",
      "tensor([0.4171, 0.3912, 0.3953, 0.3923, 0.4542], grad_fn=<SqueezeBackward3>) tensor([-1.3362, -7.2435, -9.5947, -5.0634, -1.6284], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0501, grad_fn=<NegBackward0>)\n",
      "tensor([0.4173, 0.3913, 0.3954, 0.3924, 0.4545], grad_fn=<SqueezeBackward3>) tensor([-1.3367, -7.2431, -9.5958, -5.0633, -1.6268], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0510, grad_fn=<NegBackward0>)\n",
      "tensor([0.4175, 0.3914, 0.3956, 0.3925, 0.4549], grad_fn=<SqueezeBackward3>) tensor([-1.3372, -7.2427, -9.5970, -5.0631, -1.6253], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0519, grad_fn=<NegBackward0>)\n",
      "tensor([0.4177, 0.3915, 0.3957, 0.3927, 0.4552], grad_fn=<SqueezeBackward3>) tensor([-1.3377, -7.2424, -9.5982, -5.0630, -1.6237], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0528, grad_fn=<NegBackward0>)\n",
      "tensor([0.4179, 0.3916, 0.3958, 0.3928, 0.4556], grad_fn=<SqueezeBackward3>) tensor([-1.3382, -7.2420, -9.5993, -5.0628, -1.6221], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0537, grad_fn=<NegBackward0>)\n",
      "tensor([0.4181, 0.3917, 0.3959, 0.3929, 0.4559], grad_fn=<SqueezeBackward3>) tensor([-1.3387, -7.2416, -9.6005, -5.0627, -1.6206], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0546, grad_fn=<NegBackward0>)\n",
      "tensor([0.4184, 0.3918, 0.3960, 0.3930, 0.4563], grad_fn=<SqueezeBackward3>) tensor([-1.3392, -7.2412, -9.6017, -5.0625, -1.6190], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0555, grad_fn=<NegBackward0>)\n",
      "tensor([0.4186, 0.3919, 0.3962, 0.3931, 0.4566], grad_fn=<SqueezeBackward3>) tensor([-1.3397, -7.2409, -9.6028, -5.0624, -1.6175], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0564, grad_fn=<NegBackward0>)\n",
      "tensor([0.4188, 0.3921, 0.3963, 0.3932, 0.4570], grad_fn=<SqueezeBackward3>) tensor([-1.3402, -7.2405, -9.6040, -5.0622, -1.6159], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0573, grad_fn=<NegBackward0>)\n",
      "tensor([0.4190, 0.3922, 0.3964, 0.3934, 0.4573], grad_fn=<SqueezeBackward3>) tensor([-1.3407, -7.2401, -9.6052, -5.0621, -1.6143], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0582, grad_fn=<NegBackward0>)\n",
      "tensor([0.4192, 0.3923, 0.3965, 0.3935, 0.4577], grad_fn=<SqueezeBackward3>) tensor([-1.3412, -7.2397, -9.6063, -5.0619, -1.6128], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0591, grad_fn=<NegBackward0>)\n",
      "tensor([0.4194, 0.3924, 0.3966, 0.3936, 0.4580], grad_fn=<SqueezeBackward3>) tensor([-1.3417, -7.2393, -9.6075, -5.0618, -1.6112], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0601, grad_fn=<NegBackward0>)\n",
      "tensor([0.4196, 0.3925, 0.3967, 0.3937, 0.4584], grad_fn=<SqueezeBackward3>) tensor([-1.3422, -7.2390, -9.6087, -5.0616, -1.6096], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0610, grad_fn=<NegBackward0>)\n",
      "tensor([0.4198, 0.3926, 0.3969, 0.3938, 0.4588], grad_fn=<SqueezeBackward3>) tensor([-1.3427, -7.2386, -9.6099, -5.0615, -1.6081], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0619, grad_fn=<NegBackward0>)\n",
      "tensor([0.4200, 0.3927, 0.3970, 0.3939, 0.4591], grad_fn=<SqueezeBackward3>) tensor([-1.3432, -7.2382, -9.6110, -5.0613, -1.6065], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0628, grad_fn=<NegBackward0>)\n",
      "tensor([0.4202, 0.3928, 0.3971, 0.3941, 0.4595], grad_fn=<SqueezeBackward3>) tensor([-1.3437, -7.2378, -9.6122, -5.0612, -1.6049], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0637, grad_fn=<NegBackward0>)\n",
      "tensor([0.4204, 0.3930, 0.3972, 0.3942, 0.4598], grad_fn=<SqueezeBackward3>) tensor([-1.3442, -7.2375, -9.6134, -5.0610, -1.6034], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0646, grad_fn=<NegBackward0>)\n",
      "tensor([0.4207, 0.3931, 0.3973, 0.3943, 0.4602], grad_fn=<SqueezeBackward3>) tensor([-1.3447, -7.2371, -9.6146, -5.0609, -1.6018], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0655, grad_fn=<NegBackward0>)\n",
      "tensor([0.4209, 0.3932, 0.3975, 0.3944, 0.4606], grad_fn=<SqueezeBackward3>) tensor([-1.3452, -7.2367, -9.6157, -5.0607, -1.6002], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0665, grad_fn=<NegBackward0>)\n",
      "tensor([0.4211, 0.3933, 0.3976, 0.3945, 0.4609], grad_fn=<SqueezeBackward3>) tensor([-1.3457, -7.2364, -9.6169, -5.0606, -1.5987], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0674, grad_fn=<NegBackward0>)\n",
      "tensor([0.4213, 0.3934, 0.3977, 0.3947, 0.4613], grad_fn=<SqueezeBackward3>) tensor([-1.3462, -7.2360, -9.6181, -5.0605, -1.5971], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0683, grad_fn=<NegBackward0>)\n",
      "tensor([0.4215, 0.3935, 0.3978, 0.3948, 0.4616], grad_fn=<SqueezeBackward3>) tensor([-1.3467, -7.2356, -9.6193, -5.0603, -1.5955], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0692, grad_fn=<NegBackward0>)\n",
      "tensor([0.4217, 0.3936, 0.3979, 0.3949, 0.4620], grad_fn=<SqueezeBackward3>) tensor([-1.3472, -7.2352, -9.6205, -5.0602, -1.5939], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0702, grad_fn=<NegBackward0>)\n",
      "tensor([0.4219, 0.3937, 0.3981, 0.3950, 0.4624], grad_fn=<SqueezeBackward3>) tensor([-1.3476, -7.2349, -9.6217, -5.0600, -1.5924], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0711, grad_fn=<NegBackward0>)\n",
      "tensor([0.4221, 0.3938, 0.3982, 0.3951, 0.4627], grad_fn=<SqueezeBackward3>) tensor([-1.3481, -7.2345, -9.6228, -5.0599, -1.5908], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0720, grad_fn=<NegBackward0>)\n",
      "tensor([0.4223, 0.3940, 0.3983, 0.3952, 0.4631], grad_fn=<SqueezeBackward3>) tensor([-1.3486, -7.2341, -9.6240, -5.0597, -1.5892], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0729, grad_fn=<NegBackward0>)\n",
      "tensor([0.4225, 0.3941, 0.3984, 0.3954, 0.4635], grad_fn=<SqueezeBackward3>) tensor([-1.3491, -7.2338, -9.6252, -5.0596, -1.5876], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0739, grad_fn=<NegBackward0>)\n",
      "tensor([0.4228, 0.3942, 0.3985, 0.3955, 0.4638], grad_fn=<SqueezeBackward3>) tensor([-1.3496, -7.2334, -9.6264, -5.0595, -1.5861], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0748, grad_fn=<NegBackward0>)\n",
      "tensor([0.4230, 0.3943, 0.3987, 0.3956, 0.4642], grad_fn=<SqueezeBackward3>) tensor([-1.3501, -7.2330, -9.6276, -5.0593, -1.5845], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0757, grad_fn=<NegBackward0>)\n",
      "tensor([0.4232, 0.3944, 0.3988, 0.3957, 0.4646], grad_fn=<SqueezeBackward3>) tensor([-1.3505, -7.2327, -9.6288, -5.0592, -1.5829], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0766, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4234, 0.3945, 0.3989, 0.3958, 0.4649], grad_fn=<SqueezeBackward3>) tensor([-1.3510, -7.2323, -9.6300, -5.0590, -1.5814], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0776, grad_fn=<NegBackward0>)\n",
      "tensor([0.4236, 0.3946, 0.3990, 0.3959, 0.4653], grad_fn=<SqueezeBackward3>) tensor([-1.3515, -7.2319, -9.6312, -5.0589, -1.5798], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0785, grad_fn=<NegBackward0>)\n",
      "tensor([0.4238, 0.3947, 0.3991, 0.3961, 0.4657], grad_fn=<SqueezeBackward3>) tensor([-1.3520, -7.2316, -9.6324, -5.0588, -1.5782], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0794, grad_fn=<NegBackward0>)\n",
      "tensor([0.4240, 0.3949, 0.3993, 0.3962, 0.4660], grad_fn=<SqueezeBackward3>) tensor([-1.3524, -7.2312, -9.6336, -5.0586, -1.5766], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0804, grad_fn=<NegBackward0>)\n",
      "tensor([0.4242, 0.3950, 0.3994, 0.3963, 0.4664], grad_fn=<SqueezeBackward3>) tensor([-1.3529, -7.2308, -9.6348, -5.0585, -1.5751], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0813, grad_fn=<NegBackward0>)\n",
      "tensor([0.4245, 0.3951, 0.3995, 0.3964, 0.4668], grad_fn=<SqueezeBackward3>) tensor([-1.3534, -7.2305, -9.6360, -5.0583, -1.5735], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0822, grad_fn=<NegBackward0>)\n",
      "tensor([0.4247, 0.3952, 0.3996, 0.3965, 0.4672], grad_fn=<SqueezeBackward3>) tensor([-1.3538, -7.2301, -9.6372, -5.0582, -1.5719], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0832, grad_fn=<NegBackward0>)\n",
      "tensor([0.4249, 0.3953, 0.3997, 0.3967, 0.4675], grad_fn=<SqueezeBackward3>) tensor([-1.3543, -7.2297, -9.6384, -5.0581, -1.5703], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0841, grad_fn=<NegBackward0>)\n",
      "tensor([0.4251, 0.3954, 0.3999, 0.3968, 0.4679], grad_fn=<SqueezeBackward3>) tensor([-1.3548, -7.2294, -9.6396, -5.0579, -1.5688], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0851, grad_fn=<NegBackward0>)\n",
      "tensor([0.4253, 0.3955, 0.4000, 0.3969, 0.4683], grad_fn=<SqueezeBackward3>) tensor([-1.3552, -7.2290, -9.6408, -5.0578, -1.5672], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0860, grad_fn=<NegBackward0>)\n",
      "tensor([0.4255, 0.3956, 0.4001, 0.3970, 0.4687], grad_fn=<SqueezeBackward3>) tensor([-1.3557, -7.2287, -9.6420, -5.0577, -1.5656], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0869, grad_fn=<NegBackward0>)\n",
      "tensor([0.4257, 0.3958, 0.4002, 0.3971, 0.4691], grad_fn=<SqueezeBackward3>) tensor([-1.3562, -7.2283, -9.6432, -5.0575, -1.5641], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0879, grad_fn=<NegBackward0>)\n",
      "tensor([0.4260, 0.3959, 0.4003, 0.3972, 0.4694], grad_fn=<SqueezeBackward3>) tensor([-1.3566, -7.2279, -9.6444, -5.0574, -1.5625], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0888, grad_fn=<NegBackward0>)\n",
      "tensor([0.4262, 0.3960, 0.4005, 0.3974, 0.4698], grad_fn=<SqueezeBackward3>) tensor([-1.3571, -7.2276, -9.6456, -5.0573, -1.5609], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0898, grad_fn=<NegBackward0>)\n",
      "tensor([0.4264, 0.3961, 0.4006, 0.3975, 0.4702], grad_fn=<SqueezeBackward3>) tensor([-1.3575, -7.2272, -9.6468, -5.0571, -1.5593], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0907, grad_fn=<NegBackward0>)\n",
      "tensor([0.4266, 0.3962, 0.4007, 0.3976, 0.4706], grad_fn=<SqueezeBackward3>) tensor([-1.3580, -7.2269, -9.6480, -5.0570, -1.5578], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0917, grad_fn=<NegBackward0>)\n",
      "tensor([0.4268, 0.3963, 0.4008, 0.3977, 0.4710], grad_fn=<SqueezeBackward3>) tensor([-1.3585, -7.2265, -9.6492, -5.0569, -1.5562], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0926, grad_fn=<NegBackward0>)\n",
      "tensor([0.4270, 0.3964, 0.4009, 0.3978, 0.4713], grad_fn=<SqueezeBackward3>) tensor([-1.3589, -7.2262, -9.6504, -5.0567, -1.5546], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0936, grad_fn=<NegBackward0>)\n",
      "tensor([0.4272, 0.3965, 0.4011, 0.3980, 0.4717], grad_fn=<SqueezeBackward3>) tensor([-1.3594, -7.2258, -9.6517, -5.0566, -1.5531], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0945, grad_fn=<NegBackward0>)\n",
      "tensor([0.4275, 0.3967, 0.4012, 0.3981, 0.4721], grad_fn=<SqueezeBackward3>) tensor([-1.3598, -7.2255, -9.6529, -5.0565, -1.5515], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0955, grad_fn=<NegBackward0>)\n",
      "tensor([0.4277, 0.3968, 0.4013, 0.3982, 0.4725], grad_fn=<SqueezeBackward3>) tensor([-1.3603, -7.2251, -9.6541, -5.0564, -1.5499], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0964, grad_fn=<NegBackward0>)\n",
      "tensor([0.4279, 0.3969, 0.4014, 0.3983, 0.4729], grad_fn=<SqueezeBackward3>) tensor([-1.3607, -7.2247, -9.6553, -5.0562, -1.5484], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0974, grad_fn=<NegBackward0>)\n",
      "tensor([0.4281, 0.3970, 0.4015, 0.3984, 0.4733], grad_fn=<SqueezeBackward3>) tensor([-1.3612, -7.2244, -9.6565, -5.0561, -1.5468], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0983, grad_fn=<NegBackward0>)\n",
      "tensor([0.4283, 0.3971, 0.4017, 0.3986, 0.4737], grad_fn=<SqueezeBackward3>) tensor([-1.3616, -7.2240, -9.6578, -5.0560, -1.5452], grad_fn=<SubBackward0>)\n",
      "tensor(-2.0993, grad_fn=<NegBackward0>)\n",
      "tensor([0.4285, 0.3972, 0.4018, 0.3987, 0.4740], grad_fn=<SqueezeBackward3>) tensor([-1.3620, -7.2237, -9.6590, -5.0558, -1.5437], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1003, grad_fn=<NegBackward0>)\n",
      "tensor([0.4288, 0.3973, 0.4019, 0.3988, 0.4744], grad_fn=<SqueezeBackward3>) tensor([-1.3625, -7.2233, -9.6602, -5.0557, -1.5421], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1012, grad_fn=<NegBackward0>)\n",
      "tensor([0.4290, 0.3975, 0.4020, 0.3989, 0.4748], grad_fn=<SqueezeBackward3>) tensor([-1.3629, -7.2230, -9.6614, -5.0556, -1.5406], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1022, grad_fn=<NegBackward0>)\n",
      "tensor([0.4292, 0.3976, 0.4021, 0.3990, 0.4752], grad_fn=<SqueezeBackward3>) tensor([-1.3634, -7.2226, -9.6627, -5.0555, -1.5390], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1031, grad_fn=<NegBackward0>)\n",
      "tensor([0.4294, 0.3977, 0.4023, 0.3991, 0.4756], grad_fn=<SqueezeBackward3>) tensor([-1.3638, -7.2223, -9.6639, -5.0554, -1.5374], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1041, grad_fn=<NegBackward0>)\n",
      "tensor([0.4296, 0.3978, 0.4024, 0.3993, 0.4760], grad_fn=<SqueezeBackward3>) tensor([-1.3642, -7.2220, -9.6651, -5.0552, -1.5359], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1051, grad_fn=<NegBackward0>)\n",
      "tensor([0.4298, 0.3979, 0.4025, 0.3994, 0.4764], grad_fn=<SqueezeBackward3>) tensor([-1.3647, -7.2216, -9.6663, -5.0551, -1.5343], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1060, grad_fn=<NegBackward0>)\n",
      "tensor([0.4301, 0.3980, 0.4026, 0.3995, 0.4768], grad_fn=<SqueezeBackward3>) tensor([-1.3651, -7.2213, -9.6676, -5.0550, -1.5328], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1070, grad_fn=<NegBackward0>)\n",
      "tensor([0.4303, 0.3981, 0.4027, 0.3996, 0.4772], grad_fn=<SqueezeBackward3>) tensor([-1.3655, -7.2209, -9.6688, -5.0549, -1.5312], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1080, grad_fn=<NegBackward0>)\n",
      "tensor([0.4305, 0.3982, 0.4029, 0.3997, 0.4776], grad_fn=<SqueezeBackward3>) tensor([-1.3660, -7.2206, -9.6700, -5.0547, -1.5296], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1089, grad_fn=<NegBackward0>)\n",
      "tensor([0.4307, 0.3984, 0.4030, 0.3999, 0.4780], grad_fn=<SqueezeBackward3>) tensor([-1.3664, -7.2202, -9.6713, -5.0546, -1.5281], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1099, grad_fn=<NegBackward0>)\n",
      "tensor([0.4309, 0.3985, 0.4031, 0.4000, 0.4784], grad_fn=<SqueezeBackward3>) tensor([-1.3668, -7.2199, -9.6725, -5.0545, -1.5265], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1109, grad_fn=<NegBackward0>)\n",
      "tensor([0.4312, 0.3986, 0.4032, 0.4001, 0.4788], grad_fn=<SqueezeBackward3>) tensor([-1.3672, -7.2196, -9.6737, -5.0544, -1.5250], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1118, grad_fn=<NegBackward0>)\n",
      "tensor([0.4314, 0.3987, 0.4033, 0.4002, 0.4792], grad_fn=<SqueezeBackward3>) tensor([-1.3676, -7.2192, -9.6750, -5.0543, -1.5234], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1128, grad_fn=<NegBackward0>)\n",
      "tensor([0.4316, 0.3988, 0.4035, 0.4003, 0.4796], grad_fn=<SqueezeBackward3>) tensor([-1.3681, -7.2189, -9.6762, -5.0542, -1.5219], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1138, grad_fn=<NegBackward0>)\n",
      "tensor([0.4318, 0.3989, 0.4036, 0.4005, 0.4800], grad_fn=<SqueezeBackward3>) tensor([-1.3685, -7.2185, -9.6775, -5.0540, -1.5203], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1148, grad_fn=<NegBackward0>)\n",
      "tensor([0.4320, 0.3990, 0.4037, 0.4006, 0.4804], grad_fn=<SqueezeBackward3>) tensor([-1.3689, -7.2182, -9.6787, -5.0539, -1.5187], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1157, grad_fn=<NegBackward0>)\n",
      "tensor([0.4323, 0.3992, 0.4038, 0.4007, 0.4808], grad_fn=<SqueezeBackward3>) tensor([-1.3693, -7.2179, -9.6800, -5.0538, -1.5172], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1167, grad_fn=<NegBackward0>)\n",
      "tensor([0.4325, 0.3993, 0.4040, 0.4008, 0.4812], grad_fn=<SqueezeBackward3>) tensor([-1.3697, -7.2175, -9.6812, -5.0537, -1.5156], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1177, grad_fn=<NegBackward0>)\n",
      "tensor([0.4327, 0.3994, 0.4041, 0.4009, 0.4816], grad_fn=<SqueezeBackward3>) tensor([-1.3701, -7.2172, -9.6825, -5.0536, -1.5141], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1187, grad_fn=<NegBackward0>)\n",
      "tensor([0.4329, 0.3995, 0.4042, 0.4011, 0.4820], grad_fn=<SqueezeBackward3>) tensor([-1.3705, -7.2169, -9.6837, -5.0535, -1.5125], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1197, grad_fn=<NegBackward0>)\n",
      "tensor([0.4331, 0.3996, 0.4043, 0.4012, 0.4824], grad_fn=<SqueezeBackward3>) tensor([-1.3710, -7.2165, -9.6850, -5.0534, -1.5110], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1207, grad_fn=<NegBackward0>)\n",
      "tensor([0.4334, 0.3997, 0.4044, 0.4013, 0.4828], grad_fn=<SqueezeBackward3>) tensor([-1.3714, -7.2162, -9.6862, -5.0532, -1.5094], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1216, grad_fn=<NegBackward0>)\n",
      "tensor([0.4336, 0.3998, 0.4046, 0.4014, 0.4832], grad_fn=<SqueezeBackward3>) tensor([-1.3718, -7.2159, -9.6875, -5.0531, -1.5079], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1226, grad_fn=<NegBackward0>)\n",
      "tensor([0.4338, 0.4000, 0.4047, 0.4015, 0.4836], grad_fn=<SqueezeBackward3>) tensor([-1.3722, -7.2155, -9.6887, -5.0530, -1.5063], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1236, grad_fn=<NegBackward0>)\n",
      "tensor([0.4340, 0.4001, 0.4048, 0.4017, 0.4841], grad_fn=<SqueezeBackward3>) tensor([-1.3726, -7.2152, -9.6900, -5.0529, -1.5048], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1246, grad_fn=<NegBackward0>)\n",
      "tensor([0.4342, 0.4002, 0.4049, 0.4018, 0.4845], grad_fn=<SqueezeBackward3>) tensor([-1.3730, -7.2149, -9.6912, -5.0528, -1.5033], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1256, grad_fn=<NegBackward0>)\n",
      "tensor([0.4345, 0.4003, 0.4050, 0.4019, 0.4849], grad_fn=<SqueezeBackward3>) tensor([-1.3734, -7.2145, -9.6925, -5.0527, -1.5017], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1266, grad_fn=<NegBackward0>)\n",
      "tensor([0.4347, 0.4004, 0.4052, 0.4020, 0.4853], grad_fn=<SqueezeBackward3>) tensor([-1.3737, -7.2142, -9.6937, -5.0526, -1.5002], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1276, grad_fn=<NegBackward0>)\n",
      "tensor([0.4349, 0.4005, 0.4053, 0.4021, 0.4857], grad_fn=<SqueezeBackward3>) tensor([-1.3741, -7.2139, -9.6950, -5.0525, -1.4986], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1286, grad_fn=<NegBackward0>)\n",
      "tensor([0.4351, 0.4006, 0.4054, 0.4023, 0.4861], grad_fn=<SqueezeBackward3>) tensor([-1.3745, -7.2136, -9.6963, -5.0524, -1.4971], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1296, grad_fn=<NegBackward0>)\n",
      "tensor([0.4354, 0.4008, 0.4055, 0.4024, 0.4866], grad_fn=<SqueezeBackward3>) tensor([-1.3749, -7.2132, -9.6975, -5.0523, -1.4955], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1306, grad_fn=<NegBackward0>)\n",
      "tensor([0.4356, 0.4009, 0.4057, 0.4025, 0.4870], grad_fn=<SqueezeBackward3>) tensor([-1.3753, -7.2129, -9.6988, -5.0522, -1.4940], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1316, grad_fn=<NegBackward0>)\n",
      "tensor([0.4358, 0.4010, 0.4058, 0.4026, 0.4874], grad_fn=<SqueezeBackward3>) tensor([-1.3757, -7.2126, -9.7001, -5.0521, -1.4925], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1326, grad_fn=<NegBackward0>)\n",
      "tensor([0.4360, 0.4011, 0.4059, 0.4027, 0.4878], grad_fn=<SqueezeBackward3>) tensor([-1.3761, -7.2123, -9.7013, -5.0520, -1.4909], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1336, grad_fn=<NegBackward0>)\n",
      "tensor([0.4363, 0.4012, 0.4060, 0.4029, 0.4882], grad_fn=<SqueezeBackward3>) tensor([-1.3764, -7.2120, -9.7026, -5.0519, -1.4894], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1346, grad_fn=<NegBackward0>)\n",
      "tensor([0.4365, 0.4013, 0.4061, 0.4030, 0.4887], grad_fn=<SqueezeBackward3>) tensor([-1.3768, -7.2116, -9.7039, -5.0518, -1.4878], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1356, grad_fn=<NegBackward0>)\n",
      "tensor([0.4367, 0.4014, 0.4063, 0.4031, 0.4891], grad_fn=<SqueezeBackward3>) tensor([-1.3772, -7.2113, -9.7051, -5.0517, -1.4863], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1366, grad_fn=<NegBackward0>)\n",
      "tensor([0.4369, 0.4016, 0.4064, 0.4032, 0.4895], grad_fn=<SqueezeBackward3>) tensor([-1.3776, -7.2110, -9.7064, -5.0516, -1.4848], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1376, grad_fn=<NegBackward0>)\n",
      "tensor([0.4372, 0.4017, 0.4065, 0.4033, 0.4899], grad_fn=<SqueezeBackward3>) tensor([-1.3779, -7.2107, -9.7077, -5.0515, -1.4832], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1386, grad_fn=<NegBackward0>)\n",
      "tensor([0.4374, 0.4018, 0.4066, 0.4035, 0.4904], grad_fn=<SqueezeBackward3>) tensor([-1.3783, -7.2104, -9.7090, -5.0514, -1.4817], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1396, grad_fn=<NegBackward0>)\n",
      "tensor([0.4376, 0.4019, 0.4068, 0.4036, 0.4908], grad_fn=<SqueezeBackward3>) tensor([-1.3787, -7.2101, -9.7102, -5.0513, -1.4802], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1406, grad_fn=<NegBackward0>)\n",
      "tensor([0.4378, 0.4020, 0.4069, 0.4037, 0.4912], grad_fn=<SqueezeBackward3>) tensor([-1.3790, -7.2097, -9.7115, -5.0512, -1.4786], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1416, grad_fn=<NegBackward0>)\n",
      "tensor([0.4381, 0.4021, 0.4070, 0.4038, 0.4916], grad_fn=<SqueezeBackward3>) tensor([-1.3794, -7.2094, -9.7128, -5.0511, -1.4771], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1427, grad_fn=<NegBackward0>)\n",
      "tensor([0.4383, 0.4022, 0.4071, 0.4039, 0.4921], grad_fn=<SqueezeBackward3>) tensor([-1.3798, -7.2091, -9.7141, -5.0510, -1.4756], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1437, grad_fn=<NegBackward0>)\n",
      "tensor([0.4385, 0.4024, 0.4073, 0.4041, 0.4925], grad_fn=<SqueezeBackward3>) tensor([-1.3801, -7.2088, -9.7154, -5.0509, -1.4740], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1447, grad_fn=<NegBackward0>)\n",
      "tensor([0.4387, 0.4025, 0.4074, 0.4042, 0.4929], grad_fn=<SqueezeBackward3>) tensor([-1.3805, -7.2085, -9.7167, -5.0508, -1.4725], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1457, grad_fn=<NegBackward0>)\n",
      "tensor([0.4390, 0.4026, 0.4075, 0.4043, 0.4934], grad_fn=<SqueezeBackward3>) tensor([-1.3808, -7.2082, -9.7180, -5.0507, -1.4710], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1467, grad_fn=<NegBackward0>)\n",
      "tensor([0.4392, 0.4027, 0.4076, 0.4044, 0.4938], grad_fn=<SqueezeBackward3>) tensor([-1.3812, -7.2079, -9.7192, -5.0506, -1.4694], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1478, grad_fn=<NegBackward0>)\n",
      "tensor([0.4394, 0.4028, 0.4077, 0.4046, 0.4943], grad_fn=<SqueezeBackward3>) tensor([-1.3815, -7.2076, -9.7205, -5.0505, -1.4679], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1488, grad_fn=<NegBackward0>)\n",
      "tensor([0.4396, 0.4029, 0.4079, 0.4047, 0.4947], grad_fn=<SqueezeBackward3>) tensor([-1.3819, -7.2073, -9.7218, -5.0504, -1.4664], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1498, grad_fn=<NegBackward0>)\n",
      "tensor([0.4399, 0.4030, 0.4080, 0.4048, 0.4951], grad_fn=<SqueezeBackward3>) tensor([-1.3822, -7.2070, -9.7231, -5.0504, -1.4649], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1508, grad_fn=<NegBackward0>)\n",
      "tensor([0.4401, 0.4032, 0.4081, 0.4049, 0.4956], grad_fn=<SqueezeBackward3>) tensor([-1.3826, -7.2067, -9.7244, -5.0503, -1.4633], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1519, grad_fn=<NegBackward0>)\n",
      "tensor([0.4403, 0.4033, 0.4082, 0.4050, 0.4960], grad_fn=<SqueezeBackward3>) tensor([-1.3829, -7.2064, -9.7257, -5.0502, -1.4618], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1529, grad_fn=<NegBackward0>)\n",
      "tensor([0.4406, 0.4034, 0.4084, 0.4052, 0.4965], grad_fn=<SqueezeBackward3>) tensor([-1.3833, -7.2061, -9.7270, -5.0501, -1.4603], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1539, grad_fn=<NegBackward0>)\n",
      "tensor([0.4408, 0.4035, 0.4085, 0.4053, 0.4969], grad_fn=<SqueezeBackward3>) tensor([-1.3836, -7.2058, -9.7283, -5.0500, -1.4588], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1550, grad_fn=<NegBackward0>)\n",
      "tensor([0.4410, 0.4036, 0.4086, 0.4054, 0.4973], grad_fn=<SqueezeBackward3>) tensor([-1.3839, -7.2055, -9.7296, -5.0499, -1.4572], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1560, grad_fn=<NegBackward0>)\n",
      "tensor([0.4412, 0.4037, 0.4087, 0.4055, 0.4978], grad_fn=<SqueezeBackward3>) tensor([-1.3843, -7.2052, -9.7309, -5.0498, -1.4557], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1570, grad_fn=<NegBackward0>)\n",
      "tensor([0.4415, 0.4039, 0.4089, 0.4056, 0.4982], grad_fn=<SqueezeBackward3>) tensor([-1.3846, -7.2049, -9.7322, -5.0498, -1.4542], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1581, grad_fn=<NegBackward0>)\n",
      "tensor([0.4417, 0.4040, 0.4090, 0.4058, 0.4987], grad_fn=<SqueezeBackward3>) tensor([-1.3849, -7.2046, -9.7335, -5.0497, -1.4527], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1591, grad_fn=<NegBackward0>)\n",
      "tensor([0.4419, 0.4041, 0.4091, 0.4059, 0.4991], grad_fn=<SqueezeBackward3>) tensor([-1.3852, -7.2043, -9.7348, -5.0496, -1.4512], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1601, grad_fn=<NegBackward0>)\n",
      "tensor([0.4422, 0.4042, 0.4092, 0.4060, 0.4996], grad_fn=<SqueezeBackward3>) tensor([-1.3856, -7.2040, -9.7361, -5.0495, -1.4496], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1612, grad_fn=<NegBackward0>)\n",
      "tensor([0.4424, 0.4043, 0.4093, 0.4061, 0.5000], grad_fn=<SqueezeBackward3>) tensor([-1.3859, -7.2037, -9.7374, -5.0494, -1.4481], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1622, grad_fn=<NegBackward0>)\n",
      "tensor([0.4426, 0.4044, 0.4095, 0.4063, 0.5005], grad_fn=<SqueezeBackward3>) tensor([-1.3862, -7.2034, -9.7388, -5.0494, -1.4466], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1633, grad_fn=<NegBackward0>)\n",
      "tensor([0.4429, 0.4045, 0.4096, 0.4064, 0.5009], grad_fn=<SqueezeBackward3>) tensor([-1.3865, -7.2031, -9.7401, -5.0493, -1.4451], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1643, grad_fn=<NegBackward0>)\n",
      "tensor([0.4431, 0.4047, 0.4097, 0.4065, 0.5014], grad_fn=<SqueezeBackward3>) tensor([-1.3868, -7.2028, -9.7414, -5.0492, -1.4436], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1654, grad_fn=<NegBackward0>)\n",
      "tensor([0.4433, 0.4048, 0.4098, 0.4066, 0.5019], grad_fn=<SqueezeBackward3>) tensor([-1.3871, -7.2025, -9.7427, -5.0491, -1.4420], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1664, grad_fn=<NegBackward0>)\n",
      "tensor([0.4435, 0.4049, 0.4100, 0.4067, 0.5023], grad_fn=<SqueezeBackward3>) tensor([-1.3874, -7.2022, -9.7440, -5.0491, -1.4405], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1675, grad_fn=<NegBackward0>)\n",
      "tensor([0.4438, 0.4050, 0.4101, 0.4069, 0.5028], grad_fn=<SqueezeBackward3>) tensor([-1.3877, -7.2020, -9.7453, -5.0490, -1.4390], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1685, grad_fn=<NegBackward0>)\n",
      "tensor([0.4440, 0.4051, 0.4102, 0.4070, 0.5032], grad_fn=<SqueezeBackward3>) tensor([-1.3880, -7.2017, -9.7467, -5.0489, -1.4375], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1696, grad_fn=<NegBackward0>)\n",
      "tensor([0.4442, 0.4052, 0.4103, 0.4071, 0.5037], grad_fn=<SqueezeBackward3>) tensor([-1.3883, -7.2014, -9.7480, -5.0488, -1.4360], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1706, grad_fn=<NegBackward0>)\n",
      "tensor([0.4445, 0.4054, 0.4105, 0.4072, 0.5042], grad_fn=<SqueezeBackward3>) tensor([-1.3886, -7.2011, -9.7493, -5.0488, -1.4345], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1717, grad_fn=<NegBackward0>)\n",
      "tensor([0.4447, 0.4055, 0.4106, 0.4074, 0.5046], grad_fn=<SqueezeBackward3>) tensor([-1.3889, -7.2008, -9.7506, -5.0487, -1.4330], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1728, grad_fn=<NegBackward0>)\n",
      "tensor([0.4449, 0.4056, 0.4107, 0.4075, 0.5051], grad_fn=<SqueezeBackward3>) tensor([-1.3892, -7.2006, -9.7520, -5.0486, -1.4315], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1738, grad_fn=<NegBackward0>)\n",
      "tensor([0.4452, 0.4057, 0.4108, 0.4076, 0.5056], grad_fn=<SqueezeBackward3>) tensor([-1.3895, -7.2003, -9.7533, -5.0486, -1.4299], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1749, grad_fn=<NegBackward0>)\n",
      "tensor([0.4454, 0.4058, 0.4110, 0.4077, 0.5060], grad_fn=<SqueezeBackward3>) tensor([-1.3898, -7.2000, -9.7546, -5.0485, -1.4284], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1760, grad_fn=<NegBackward0>)\n",
      "tensor([0.4456, 0.4059, 0.4111, 0.4078, 0.5065], grad_fn=<SqueezeBackward3>) tensor([-1.3901, -7.1997, -9.7560, -5.0484, -1.4269], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1770, grad_fn=<NegBackward0>)\n",
      "tensor([0.4459, 0.4061, 0.4112, 0.4080, 0.5070], grad_fn=<SqueezeBackward3>) tensor([-1.3903, -7.1994, -9.7573, -5.0484, -1.4254], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1781, grad_fn=<NegBackward0>)\n",
      "tensor([0.4461, 0.4062, 0.4113, 0.4081, 0.5074], grad_fn=<SqueezeBackward3>) tensor([-1.3906, -7.1992, -9.7586, -5.0483, -1.4239], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1792, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4463, 0.4063, 0.4115, 0.4082, 0.5079], grad_fn=<SqueezeBackward3>) tensor([-1.3909, -7.1989, -9.7600, -5.0482, -1.4224], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1802, grad_fn=<NegBackward0>)\n",
      "tensor([0.4466, 0.4064, 0.4116, 0.4083, 0.5084], grad_fn=<SqueezeBackward3>) tensor([-1.3912, -7.1986, -9.7613, -5.0482, -1.4209], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1813, grad_fn=<NegBackward0>)\n",
      "tensor([0.4468, 0.4065, 0.4117, 0.4085, 0.5089], grad_fn=<SqueezeBackward3>) tensor([-1.3914, -7.1984, -9.7626, -5.0481, -1.4194], grad_fn=<SubBackward0>)\n",
      "tensor(-2.1824, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_39092\\465945479.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  reward = u @ softmax(phi @ X_hat + b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x247f5c0ef70>]"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_new = torch.tensor([0.5, 0.2])\n",
    "u = torch.tensor([0.3, 0])\n",
    "phi, b = l1norm_phi(Xs, W, u)\n",
    "x_prime = softmax_gragent(x_new, phi, b, u).detach()\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.scatter(X_x, X_y, c=Y)\n",
    "plt.plot(x_new[0], x_new[1], marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"red\")\n",
    "plt.plot(x_prime[0], x_prime[1], marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ca80b",
   "metadata": {},
   "source": [
    "# Gameplan\n",
    "\n",
    "- Determine true phi and true b ahead of time, as well as true w and u\n",
    "- Points randomly sampled from 0-1 both dims\n",
    "- Every timestep use new point and old points to determine new phi\n",
    "- No actual learning since we know W, maybe collect a loss for funsies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "47a33508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xW(X_hat, Rwd, phi, b):\n",
    "    one_m = torch.ones(m)\n",
    "    xW = torch.rand(d)\n",
    "    xW.requires_grad_(True)\n",
    "    action = (phi @ X_hat + b) / (one_m @ (phi @ X_hat + b))\n",
    "    optimizer = torch.optim.Adam([xW], lr=0.0001)\n",
    "    for _ in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        action = (phi @ X_hat + b) / (one_m @ (phi @ X_hat + b))\n",
    "        loss = (Rwd - xW @ action) ** 2 + (1 - torch.norm(xW)) ** 2\n",
    "        print(loss.item(), ((Rwd - xW @ action) ** 2).item(), ((1 - torch.norm(xW)) ** 2).item(), action.tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return xW \n",
    "\n",
    "def reward(X, Xhat, W, phi, b):\n",
    "    one_m = torch.ones(m)\n",
    "    action = (phi @ Xhat + b) / (one_m @ (phi @ Xhat + b))\n",
    "    return (X @ W) @ action\n",
    "\n",
    "def opt_phi(Xhats, xWs, u): # no additional penalty for finding x?\n",
    "    one_m = torch.ones(m)\n",
    "    phi = torch.rand((m, d))\n",
    "    phi.requires_grad_(True)\n",
    "    b = torch.zeros(m)\n",
    "    # b.requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([phi], lr=0.0001)\n",
    "    relu = nn.ReLU()\n",
    "    # optimizer = torch.optim.SGD([phi], lr=0.01, momentum=0.9)\n",
    "    for _ in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        norm_phi = relu(phi) / torch.max(phi) # + torch.rand((m, d)) * 0.05\n",
    "        actions = (norm_phi @ Xhats.T + b.reshape(-1, 1)) / (one_m @ (norm_phi @ Xhats.T + b.reshape(-1, 1))) # (md @ dn + m) / (m @ (md @ dn + m)) --> mn\n",
    "        loss = - torch.mean(xWs @ actions) # + (1 - torch.norm(phi)) ** 2 # mean(nm @ mn) --> 1\n",
    "        # print(torch.mean(xWs @ actions).item(), loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return (relu(phi) / torch.max(phi)).detach(), b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a768166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012271380983293056 0.007384782657027245 0.004886598326265812 [0.0, 1.0]\n",
      "0.012258635833859444 0.0073676030151546 0.0048910328187048435 [0.0, 1.0]\n",
      "0.012245921418070793 0.007350453175604343 0.004895468708127737 [0.0, 1.0]\n",
      "0.01223321445286274 0.007333323825150728 0.004899890627712011 [0.0, 1.0]\n",
      "0.01222054474055767 0.00731621403247118 0.004904330708086491 [0.0, 1.0]\n",
      "0.012207907624542713 0.007299134507775307 0.0049087731167674065 [0.0, 1.0]\n",
      "0.01219528540968895 0.0072820852510631084 0.0049132006242871284 [0.0, 1.0]\n",
      "0.012182696722447872 0.007265066262334585 0.004917630460113287 [0.0, 1.0]\n",
      "0.012170139700174332 0.00724807707592845 0.004922062158584595 [0.0, 1.0]\n",
      "0.01215759664773941 0.007231117691844702 0.004926478955894709 [0.0, 1.0]\n",
      "0.012145096436142921 0.00721419882029295 0.004930897615849972 [0.0, 1.0]\n",
      "0.012132611125707626 0.007197309751063585 0.004935301840305328 [0.0, 1.0]\n",
      "0.012120168656110764 0.007180460728704929 0.004939707927405834 [0.0, 1.0]\n",
      "0.01210775040090084 0.007163651287555695 0.004944099113345146 [0.0, 1.0]\n",
      "0.01209537498652935 0.007146882358938456 0.004948492161929607 [0.0, 1.0]\n",
      "0.012083007022738457 0.00713015254586935 0.004952854011207819 [0.0, 1.0]\n",
      "0.012070680037140846 0.007113462779670954 0.0049572172574698925 [0.0, 1.0]\n",
      "0.012058395892381668 0.007096812594681978 0.0049615828320384026 [0.0, 1.0]\n",
      "0.01204611174762249 0.007080212235450745 0.004965899977833033 [0.0, 1.0]\n",
      "0.01203386951237917 0.007063650991767645 0.004970218520611525 [0.0, 1.0]\n",
      "0.01202166173607111 0.007047139108181 0.00497452262789011 [0.0, 1.0]\n",
      "0.01200948841869831 0.0070306770503520966 0.004978811368346214 [0.0, 1.0]\n",
      "0.011997349560260773 0.007014263886958361 0.004983085673302412 [0.0, 1.0]\n",
      "0.01198524422943592 0.006997899617999792 0.004987344611436129 [0.0, 1.0]\n",
      "0.011973155662417412 0.006981584709137678 0.004991571418941021 [0.0, 1.0]\n",
      "0.011961101554334164 0.006965318229049444 0.0049957833252847195 [0.0, 1.0]\n",
      "0.011949097737669945 0.006949101109057665 0.004999997094273567 [0.0, 1.0]\n",
      "0.011937104165554047 0.006932942196726799 0.005004162434488535 [0.0, 1.0]\n",
      "0.011925160884857178 0.0069168321788311005 0.005008329171687365 [0.0, 1.0]\n",
      "0.011913226917386055 0.006900779902935028 0.005012447014451027 [0.0, 1.0]\n",
      "0.011901343241333961 0.006884776521474123 0.005016566254198551 [0.0, 1.0]\n",
      "0.011889484710991383 0.006868830882012844 0.0050206538289785385 [0.0, 1.0]\n",
      "0.011877652257680893 0.006852943450212479 0.005024708807468414 [0.0, 1.0]\n",
      "0.011865852400660515 0.006837103981524706 0.005028748884797096 [0.0, 1.0]\n",
      "0.011854095384478569 0.006821322254836559 0.005032773595303297 [0.0, 1.0]\n",
      "0.011842364445328712 0.0068055978044867516 0.005036766175180674 [0.0, 1.0]\n",
      "0.011830674484372139 0.00678993109613657 0.005040743388235569 [0.0, 1.0]\n",
      "0.011819008737802505 0.00677432119846344 0.005044688005000353 [0.0, 1.0]\n",
      "0.011807369068264961 0.006758769042789936 0.0050486004911363125 [0.0, 1.0]\n",
      "0.011795754544436932 0.006743273697793484 0.005052480846643448 [0.0, 1.0]\n",
      "0.011784190312027931 0.006727844942361116 0.005056345369666815 [0.0, 1.0]\n",
      "0.011772651225328445 0.006712472997605801 0.0050601777620613575 [0.0, 1.0]\n",
      "0.011761152185499668 0.006697157863527536 0.005063994321972132 [0.0, 1.0]\n",
      "0.01174967736005783 0.006681899074465036 0.005067778285592794 [0.0, 1.0]\n",
      "0.011738218367099762 0.006666705943644047 0.0050715128891170025 [0.0, 1.0]\n",
      "0.011726818978786469 0.006651569623500109 0.00507524935528636 [0.0, 1.0]\n",
      "0.011715434491634369 0.006636498495936394 0.005078935530036688 [0.0, 1.0]\n",
      "0.011704090051352978 0.006621483713388443 0.005082606337964535 [0.0, 1.0]\n",
      "0.011692768894135952 0.006606524344533682 0.00508624454960227 [0.0, 1.0]\n",
      "0.011681480333209038 0.006591630633920431 0.005089849699288607 [0.0, 1.0]\n",
      "0.011670215055346489 0.00657679233700037 0.005093422252684832 [0.0, 1.0]\n",
      "0.01165899820625782 0.006562018766999245 0.0050969794392585754 [0.0, 1.0]\n",
      "0.011647796258330345 0.0065473103895783424 0.00510048633441329 [0.0, 1.0]\n",
      "0.011636633425951004 0.006532656494528055 0.005103977397084236 [0.0, 1.0]\n",
      "0.011625486426055431 0.006518067792057991 0.00510741863399744 [0.0, 1.0]\n",
      "0.011614395305514336 0.006503533571958542 0.0051108612678945065 [0.0, 1.0]\n",
      "0.011603317223489285 0.006489063613116741 0.005114253610372543 [0.0, 1.0]\n",
      "0.011592261493206024 0.0064746481366455555 0.005117613356560469 [0.0, 1.0]\n",
      "0.01158125326037407 0.006460296455770731 0.005120956804603338 [0.0, 1.0]\n",
      "0.011570258066058159 0.006446008570492268 0.005124249961227179 [0.0, 1.0]\n",
      "0.0115593196824193 0.006431775167584419 0.005127544514834881 [0.0, 1.0]\n",
      "0.011548375710844994 0.0064176046289503574 0.0051307715475559235 [0.0, 1.0]\n",
      "0.011537481099367142 0.006403497885912657 0.005133982747793198 [0.0, 1.0]\n",
      "0.011526621878147125 0.0063894446939229965 0.005137177649885416 [0.0, 1.0]\n",
      "0.011515777558088303 0.006375454366207123 0.0051403227262198925 [0.0, 1.0]\n",
      "0.011504977941513062 0.006361526902765036 0.005143451038748026 [0.0, 1.0]\n",
      "0.011494191363453865 0.006347662303596735 0.005146529525518417 [0.0, 1.0]\n",
      "0.01148342527449131 0.0063338507898151875 0.005149574484676123 [0.0, 1.0]\n",
      "0.011472704820334911 0.006320101674646139 0.005152603145688772 [0.0, 1.0]\n",
      "0.011461995542049408 0.006306414492428303 0.0051555815152823925 [0.0, 1.0]\n",
      "0.01145133376121521 0.006292789708822966 0.005158543586730957 [0.0, 1.0]\n",
      "0.01144068967550993 0.006279217544943094 0.005161472130566835 [0.0, 1.0]\n",
      "0.01143009215593338 0.006265707314014435 0.005164384376257658 [0.0, 1.0]\n",
      "0.011419504880905151 0.0062522585503757 0.005167245864868164 [0.0, 1.0]\n",
      "0.011408936232328415 0.006238861940801144 0.005170074291527271 [0.0, 1.0]\n",
      "0.011398395523428917 0.006225526798516512 0.005172869190573692 [0.0, 1.0]\n",
      "0.011387899518013 0.0062122526578605175 0.00517564732581377 [0.0, 1.0]\n",
      "0.01137741468846798 0.00619903951883316 0.005178374703973532 [0.0, 1.0]\n",
      "0.01136696431785822 0.006185878533869982 0.005181085783988237 [0.0, 1.0]\n",
      "0.011356541886925697 0.006172778084874153 0.005183763336390257 [0.0, 1.0]\n",
      "0.011346145533025265 0.0061597381718456745 0.00518640736117959 [0.0, 1.0]\n",
      "0.011335776187479496 0.006146758794784546 0.00518901739269495 [0.0, 1.0]\n",
      "0.011325433850288391 0.006133839953690767 0.005191593896597624 [0.0, 1.0]\n",
      "0.011315109208226204 0.006120971869677305 0.005194136872887611 [0.0, 1.0]\n",
      "0.011304827407002449 0.006108163855969906 0.005196663085371256 [0.0, 1.0]\n",
      "0.011294553987681866 0.006095415912568569 0.0051991380751132965 [0.0, 1.0]\n",
      "0.011284306645393372 0.006082727573812008 0.005201579537242651 [0.0, 1.0]\n",
      "0.011274094693362713 0.006070089992135763 0.00520400470122695 [0.0, 1.0]\n",
      "0.011263906955718994 0.0060575115494430065 0.005206395406275988 [0.0, 1.0]\n",
      "0.011253727599978447 0.006044992711395025 0.005208735354244709 [0.0, 1.0]\n",
      "0.011243591085076332 0.006032533012330532 0.005211058538407087 [0.0, 1.0]\n",
      "0.011233488097786903 0.006020123139023781 0.0052133649587631226 [0.0, 1.0]\n",
      "0.011223392561078072 0.006007772404700518 0.005215620622038841 [0.0, 1.0]\n",
      "0.011213322170078754 0.005995480343699455 0.005217841826379299 [0.0, 1.0]\n",
      "0.011203275993466377 0.005983246956020594 0.005220029503107071 [0.0, 1.0]\n",
      "0.011193262413144112 0.005971062928438187 0.005222199950367212 [0.0, 1.0]\n",
      "0.011183256283402443 0.00595893757417798 0.00522431917488575 [0.0, 1.0]\n",
      "0.011173275299370289 0.005946870427578688 0.005226404871791601 [0.0, 1.0]\n",
      "0.011163342744112015 0.005934851709753275 0.0052284905686974525 [0.0, 1.0]\n",
      "0.011153416708111763 0.005922891665250063 0.0052305250428617 [0.0, 1.0]\n",
      "0.011143498122692108 0.005910989362746477 0.005232508294284344 [0.0, 1.0]\n",
      "0.011133618652820587 0.0058991448022425175 0.005234474316239357 [0.0, 1.0]\n",
      "0.011123755015432835 0.005887348670512438 0.005236406344920397 [0.0, 1.0]\n",
      "0.01111393142491579 0.005875609815120697 0.0052383216097950935 [0.0, 1.0]\n",
      "0.011104114353656769 0.005863928701728582 0.005240185651928186 [0.0, 1.0]\n",
      "0.011094319634139538 0.005852304399013519 0.0052420152351260185 [0.0, 1.0]\n",
      "0.011084556579589844 0.005840728525072336 0.00524382758885622 [0.0, 1.0]\n",
      "0.01107481587678194 0.005829209461808205 0.005245606414973736 [0.0, 1.0]\n",
      "0.011065097525715828 0.005817746743559837 0.005247350316494703 [0.0, 1.0]\n",
      "0.011055393144488335 0.00580633245408535 0.005249060224741697 [0.0, 1.0]\n",
      "0.011045710183680058 0.00579497404396534 0.005250736139714718 [0.0, 1.0]\n",
      "0.011036049574613571 0.005783672444522381 0.005252377595752478 [0.0, 1.0]\n",
      "0.011026402935385704 0.005772418342530727 0.005253984592854977 [0.0, 1.0]\n",
      "0.011016794480383396 0.005761220119893551 0.005255574360489845 [0.0, 1.0]\n",
      "0.011007208377122879 0.005750077776610851 0.00525713013485074 [0.0, 1.0]\n",
      "0.010997634381055832 0.00573898246511817 0.005258651450276375 [0.0, 1.0]\n",
      "0.010988080874085426 0.005727943032979965 0.005260138306766748 [0.0, 1.0]\n",
      "0.010978550650179386 0.0057169594801962376 0.005261591169983149 [0.0, 1.0]\n",
      "0.010969031602144241 0.005706022493541241 0.005263009574264288 [0.0, 1.0]\n",
      "0.010959550738334656 0.0056951409205794334 0.00526441028341651 [0.0, 1.0]\n",
      "0.010950082913041115 0.00568430544808507 0.005265776999294758 [0.0, 1.0]\n",
      "0.01094063464552164 0.0056735253892838955 0.005267109256237745 [0.0, 1.0]\n",
      "0.010931206867098808 0.005662800278514624 0.005268407054245472 [0.0, 1.0]\n",
      "0.010921791195869446 0.005652121268212795 0.005269670393317938 [0.0, 1.0]\n",
      "0.010912413708865643 0.005641496740281582 0.005270916968584061 [0.0, 1.0]\n",
      "0.010903029702603817 0.005630918312817812 0.005272111389786005 [0.0, 1.0]\n",
      "0.01089368388056755 0.005620394833385944 0.005273288581520319 [0.0, 1.0]\n",
      "0.010884348303079605 0.005609916523098946 0.005274431314319372 [0.0, 1.0]\n",
      "0.010875049978494644 0.005599492695182562 0.005275556817650795 [0.0, 1.0]\n",
      "0.010865753516554832 0.005589122883975506 0.005276630632579327 [0.0, 1.0]\n",
      "0.01085648499429226 0.005578798707574606 0.005277686752378941 [0.0, 1.0]\n",
      "0.010847236961126328 0.0055685280822217464 0.005278708878904581 [0.0, 1.0]\n",
      "0.010837999172508717 0.005558302626013756 0.005279696546494961 [0.0, 1.0]\n",
      "0.010828780010342598 0.005548131186515093 0.005280649289488792 [0.0, 1.0]\n",
      "0.010819572024047375 0.0055380044505000114 0.005281567573547363 [0.0, 1.0]\n",
      "0.010810399428009987 0.00552793126553297 0.005282468628138304 [0.0, 1.0]\n",
      "0.01080123707652092 0.005517902784049511 0.005283334758132696 [0.0, 1.0]\n",
      "0.010792111977934837 0.0055079273879528046 0.0052841841243207455 [0.0, 1.0]\n",
      "0.010782977566123009 0.00549799669533968 0.005284981336444616 [0.0, 1.0]\n",
      "0.010773862712085247 0.005488118622452021 0.005285744089633226 [0.0, 1.0]\n",
      "0.010764792561531067 0.005478285253047943 0.0052865068428218365 [0.0, 1.0]\n",
      "0.01075571309775114 0.0054684956558048725 0.005287217441946268 [0.0, 1.0]\n",
      "0.010746652260422707 0.005458758678287268 0.005287893582135439 [0.0, 1.0]\n",
      "0.010737618431448936 0.005449065938591957 0.005288552492856979 [0.0, 1.0]\n",
      "0.010728603228926659 0.005439425818622112 0.005289176944643259 [0.0, 1.0]\n",
      "0.01071961224079132 0.005429829005151987 0.005289783701300621 [0.0, 1.0]\n",
      "0.010710623115301132 0.005420284811407328 0.005290338769555092 [0.0, 1.0]\n",
      "0.010701660066843033 0.005410783924162388 0.005290876142680645 [0.0, 1.0]\n",
      "0.01069270633161068 0.005401326809078455 0.005291379056870937 [0.0, 1.0]\n",
      "0.01068376936018467 0.005391921382397413 0.005291847512125969 [0.0, 1.0]\n",
      "0.010674857534468174 0.005382559262216091 0.005292298272252083 [0.0, 1.0]\n",
      "0.010665955021977425 0.005373239982873201 0.005292714573442936 [0.0, 1.0]\n",
      "0.010657069273293018 0.00536397285759449 0.005293096415698528 [0.0, 1.0]\n",
      "0.010648208670318127 0.005354748107492924 0.005293460562825203 [0.0, 1.0]\n",
      "0.010639356449246407 0.005345566663891077 0.005293790251016617 [0.0, 1.0]\n",
      "0.010630521923303604 0.005336436443030834 0.005294085014611483 [0.0, 1.0]\n",
      "0.010621693916618824 0.005327348597347736 0.005294345319271088 [0.0, 1.0]\n",
      "0.010612891986966133 0.005318303592503071 0.005294587928801775 [0.0, 1.0]\n",
      "0.010604115203022957 0.00530930096283555 0.005294813774526119 [0.0, 1.0]\n",
      "0.010595353320240974 0.005300349090248346 0.005295004229992628 [0.0, 1.0]\n",
      "0.010586600750684738 0.005291439592838287 0.0052951606921851635 [0.0, 1.0]\n",
      "0.01057785376906395 0.005282572470605373 0.0052952817641198635 [0.0, 1.0]\n",
      "0.01056913286447525 0.005273747257888317 0.0052953860722482204 [0.0, 1.0]\n",
      "0.010560419410467148 0.0052649639546871185 0.005295455455780029 [0.0, 1.0]\n",
      "0.010551721788942814 0.0052562314085662365 0.005295490380376577 [0.0, 1.0]\n",
      "0.01054304838180542 0.005247540306299925 0.005295507609844208 [0.0, 1.0]\n",
      "0.010534381493926048 0.005238891113549471 0.005295490380376577 [0.0, 1.0]\n",
      "0.010525738820433617 0.005230283830314875 0.005295455455780029 [0.0, 1.0]\n",
      "0.010517103597521782 0.0052217175252735615 0.0052953860722482204 [0.0, 1.0]\n",
      "0.01050847489386797 0.005213193129748106 0.0052952817641198635 [0.0, 1.0]\n",
      "0.010499870404601097 0.005204709712415934 0.0052951606921851635 [0.0, 1.0]\n",
      "0.01049128919839859 0.005196267738938332 0.005295021925121546 [0.0, 1.0]\n",
      "0.010482706129550934 0.0051878755912184715 0.00529483100399375 [0.0, 1.0]\n",
      "0.010474147275090218 0.0051795244216918945 0.005294622853398323 [0.0, 1.0]\n",
      "0.010465610772371292 0.005171214230358601 0.005294397007673979 [0.0, 1.0]\n",
      "0.01045706495642662 0.00516294501721859 0.005294119473546743 [0.0, 1.0]\n",
      "0.010448558256030083 0.005154716782271862 0.005293841939419508 [0.0, 1.0]\n",
      "0.010440042242407799 0.005146529525518417 0.005293512716889381 [0.0, 1.0]\n",
      "0.010431565344333649 0.0051383827812969685 0.0052931830286979675 [0.0, 1.0]\n",
      "0.010423077270388603 0.005130276549607515 0.005292801186442375 [0.0, 1.0]\n",
      "0.01041463017463684 0.005122210830450058 0.00529241980984807 [0.0, 1.0]\n",
      "0.010406171903014183 0.005114185623824596 0.005291986279189587 [0.0, 1.0]\n",
      "0.010397735983133316 0.005106200464069843 0.0052915350534021854 [0.0, 1.0]\n",
      "0.010389314964413643 0.005098247434943914 0.005291067063808441 [0.0, 1.0]\n",
      "0.01038091629743576 0.005090334452688694 0.005290581379085779 [0.0, 1.0]\n",
      "0.010372505523264408 0.005082461982965469 0.005290043540298939 [0.0, 1.0]\n",
      "0.010364118032157421 0.005074629094451666 0.005289488937705755 [0.0, 1.0]\n",
      "0.010355752892792225 0.005066836252808571 0.005288916639983654 [0.0, 1.0]\n",
      "0.010347393341362476 0.005059083458036184 0.005288309883326292 [0.0, 1.0]\n",
      "0.010339057072997093 0.005051370710134506 0.0052876858972013 [0.0, 1.0]\n",
      "0.01033073291182518 0.005043689161539078 0.00528704421594739 [0.0, 1.0]\n",
      "0.010322397574782372 0.0050360471941530704 0.005286350846290588 [0.0, 1.0]\n",
      "0.010314102284610271 0.005028444807976484 0.005285657476633787 [0.0, 1.0]\n",
      "0.01030581071972847 0.005020881537348032 0.005284929648041725 [0.0, 1.0]\n",
      "0.01029752567410469 0.005013358313590288 0.005284166894853115 [0.0, 1.0]\n",
      "0.010289252735674381 0.005005865823477507 0.005283386912196875 [0.0, 1.0]\n",
      "0.010281002148985863 0.004998412914574146 0.005282589700073004 [0.0, 1.0]\n",
      "0.010272756218910217 0.0049909986555576324 0.005281758029013872 [0.0, 1.0]\n",
      "0.010264525189995766 0.004983615595847368 0.00528090912848711 [0.0, 1.0]\n",
      "0.010256314650177956 0.004976271651685238 0.005280042998492718 [0.0, 1.0]\n",
      "0.010248107835650444 0.004968966357409954 0.005279141943901777 [0.0, 1.0]\n",
      "0.010239915922284126 0.00496169226244092 0.005278224125504494 [0.0, 1.0]\n",
      "0.010231727734208107 0.004954456351697445 0.005277271382510662 [0.0, 1.0]\n",
      "0.010223560966551304 0.004947259556502104 0.0052763014100492 [0.0, 1.0]\n",
      "0.010215407237410545 0.004940093494951725 0.005275314208120108 [0.0, 1.0]\n",
      "0.01020725816488266 0.0049329656176269054 0.0052742925472557545 [0.0, 1.0]\n",
      "0.010199104435741901 0.004925868008285761 0.0052732364274561405 [0.0, 1.0]\n",
      "0.010190989822149277 0.004918809048831463 0.0052721803076565266 [0.0, 1.0]\n",
      "0.010182870551943779 0.00491178035736084 0.005271089728921652 [0.0, 1.0]\n",
      "0.01017478946596384 0.004904789850115776 0.005269999615848064 [0.0, 1.0]\n",
      "0.010166686959564686 0.004897829610854387 0.0052688573487102985 [0.0, 1.0]\n",
      "0.01015862263739109 0.004890907555818558 0.005267715081572533 [0.0, 1.0]\n",
      "0.010150553658604622 0.004884015303105116 0.005266538355499506 [0.0, 1.0]\n",
      "0.010142488405108452 0.004877161234617233 0.005265327170491219 [0.0, 1.0]\n",
      "0.01013445295393467 0.004870336968451738 0.0052641164511442184 [0.0, 1.0]\n",
      "0.010126404464244843 0.004863550886511803 0.00526285357773304 [0.0, 1.0]\n",
      "0.010118385776877403 0.004856794141232967 0.005261591169983149 [0.0, 1.0]\n",
      "0.01011037826538086 0.00485006719827652 0.005260311532765627 [0.0, 1.0]\n",
      "0.010102375410497189 0.004843377973884344 0.0052589974366128445 [0.0, 1.0]\n",
      "0.010094384662806988 0.004836718551814556 0.005257666110992432 [0.0, 1.0]\n",
      "0.01008639670908451 0.00483009684830904 0.005256300326436758 [0.0, 1.0]\n",
      "0.010078439489006996 0.004823504015803337 0.005254935007542372 [0.0, 1.0]\n",
      "0.010070476680994034 0.004816940985620022 0.005253535229712725 [0.0, 1.0]\n",
      "0.010062525048851967 0.00481040682643652 0.005252118222415447 [0.0, 1.0]\n",
      "0.010054593905806541 0.004803910385817289 0.005250683985650539 [0.0, 1.0]\n",
      "0.010046659037470818 0.0047974432818591595 0.005249215755611658 [0.0, 1.0]\n",
      "0.010038752108812332 0.004791005048900843 0.005247747525572777 [0.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010030841454863548 0.004784595686942339 0.005246245302259922 [0.0, 1.0]\n",
      "0.010022949427366257 0.004778224043548107 0.00524472538381815 [0.0, 1.0]\n",
      "0.01001507043838501 0.004771880805492401 0.005243189167231321 [0.0, 1.0]\n",
      "0.01000718493014574 0.0047655669040977955 0.005241618026047945 [0.0, 1.0]\n",
      "0.009999329224228859 0.004759281873703003 0.005240047350525856 [0.0, 1.0]\n",
      "0.00999146793037653 0.004753025248646736 0.0052384426817297935 [0.0, 1.0]\n",
      "0.00998361874371767 0.00474679796025157 0.005236820783466101 [0.0, 1.0]\n",
      "0.009975780732929707 0.004740599077194929 0.0052351816557347775 [0.0, 1.0]\n",
      "0.009967954829335213 0.004734428599476814 0.005233525764197111 [0.0, 1.0]\n",
      "0.009960148483514786 0.004728295374661684 0.005231852643191814 [0.0, 1.0]\n",
      "0.009952335618436337 0.004722190089523792 0.005230145528912544 [0.0, 1.0]\n",
      "0.009944535791873932 0.004716114141047001 0.005228421650826931 [0.0, 1.0]\n",
      "0.009936763904988766 0.004710066132247448 0.005226697772741318 [0.0, 1.0]\n",
      "0.009928986430168152 0.004704046528786421 0.005224939901381731 [0.0, 1.0]\n",
      "0.009921202436089516 0.0046980553306639194 0.0052231475710868835 [0.0, 1.0]\n",
      "0.00991346500813961 0.004692092537879944 0.005221372935920954 [0.0, 1.0]\n",
      "0.00990571454167366 0.0046861497685313225 0.0052195643074810505 [0.0, 1.0]\n",
      "0.009897973388433456 0.004680235404521227 0.005217738449573517 [0.0, 1.0]\n",
      "0.009890245273709297 0.00467434898018837 0.00521589582785964 [0.0, 1.0]\n",
      "0.009882526472210884 0.004668490495532751 0.00521403644233942 [0.0, 1.0]\n",
      "0.009874820709228516 0.004662660416215658 0.0052121602930128574 [0.0, 1.0]\n",
      "0.009867126122117043 0.004656858742237091 0.005210266914218664 [0.0, 1.0]\n",
      "0.009859424084424973 0.004651084542274475 0.0052083395421504974 [0.0, 1.0]\n",
      "0.009851733222603798 0.004645338281989098 0.005206395406275988 [0.0, 1.0]\n",
      "0.00984406378120184 0.004639612045139074 0.005204451736062765 [0.0, 1.0]\n",
      "0.009836404584348202 0.0046339137479662895 0.005202490836381912 [0.0, 1.0]\n",
      "0.00982875656336546 0.004628242924809456 0.005200513638556004 [0.0, 1.0]\n",
      "0.009821102023124695 0.004622600041329861 0.005198501981794834 [0.0, 1.0]\n",
      "0.009813468903303146 0.00461697718128562 0.005196491256356239 [0.0, 1.0]\n",
      "0.009805828332901001 0.00461138179525733 0.005194446071982384 [0.0, 1.0]\n",
      "0.009798215702176094 0.004605813883244991 0.0051924013532698154 [0.0, 1.0]\n",
      "0.009790604934096336 0.00460026552900672 0.005190339870750904 [0.0, 1.0]\n",
      "0.009783007204532623 0.004594745114445686 0.005188262090086937 [0.0, 1.0]\n",
      "0.009775418788194656 0.004589251708239317 0.005186167079955339 [0.0, 1.0]\n",
      "0.009767833165824413 0.0045837778598070145 0.005184055306017399 [0.0, 1.0]\n",
      "0.009760258719325066 0.0045783319510519505 0.005181926768273115 [0.0, 1.0]\n",
      "0.009752677753567696 0.00457291305065155 0.005179764702916145 [0.0, 1.0]\n",
      "0.00974513404071331 0.004567513708025217 0.005177619867026806 [0.0, 1.0]\n",
      "0.009737582877278328 0.004562141373753548 0.00517544150352478 [0.0, 1.0]\n",
      "0.009730052202939987 0.004556788597255945 0.005173263605684042 [0.0, 1.0]\n",
      "0.00972251407802105 0.004551462829113007 0.00517105171456933 [0.0, 1.0]\n",
      "0.00971497967839241 0.004546156618744135 0.005168823059648275 [0.0, 1.0]\n",
      "0.009707472287118435 0.004540877416729927 0.005166594870388508 [0.0, 1.0]\n",
      "0.009699966758489609 0.004535617306828499 0.005164349917322397 [0.0, 1.0]\n",
      "0.009692473337054253 0.0045303842052817345 0.005162088666111231 [0.0, 1.0]\n",
      "0.009684980846941471 0.00452517019584775 0.005159810651093721 [0.0, 1.0]\n",
      "0.009677499532699585 0.004519983194768429 0.005157515872269869 [0.0, 1.0]\n",
      "0.009670020081102848 0.0045148152858018875 0.0051552047953009605 [0.0, 1.0]\n",
      "0.009662551805377007 0.00450967438519001 0.005152876954525709 [0.0, 1.0]\n",
      "0.009655102156102657 0.004504552576690912 0.005150549579411745 [0.0, 1.0]\n",
      "0.009647655300796032 0.004499449394643307 0.005148205906152725 [0.0, 1.0]\n",
      "0.009640218690037727 0.004494373220950365 0.005145845469087362 [0.0, 1.0]\n",
      "0.009632783941924572 0.004489315673708916 0.005143468268215656 [0.0, 1.0]\n",
      "0.00962535198777914 0.004484277218580246 0.0051410747691988945 [0.0, 1.0]\n",
      "0.009617947041988373 0.004479265306144953 0.00513868173584342 [0.0, 1.0]\n",
      "0.009610528126358986 0.004474272485822439 0.005136255174875259 [0.0, 1.0]\n",
      "0.009603126905858517 0.004469297826290131 0.005133829079568386 [0.0, 1.0]\n",
      "0.009595729410648346 0.004464342258870602 0.005131386686116457 [0.0, 1.0]\n",
      "0.00958835706114769 0.004459413234144449 0.005128944292664528 [0.0, 1.0]\n",
      "0.009580971673130989 0.004454502835869789 0.0051264688372612 [0.0, 1.0]\n",
      "0.00957360491156578 0.004449611064046621 0.005123993847519159 [0.0, 1.0]\n",
      "0.009566257707774639 0.004444737918674946 0.005121519789099693 [0.0, 1.0]\n",
      "0.009558895602822304 0.004439883399754763 0.005119011737406254 [0.0, 1.0]\n",
      "0.009551559574902058 0.004435054957866669 0.005116504617035389 [0.0, 1.0]\n",
      "0.009544209577143192 0.004430245608091354 0.005113963969051838 [0.0, 1.0]\n",
      "0.009536894969642162 0.004425454419106245 0.005111440550535917 [0.0, 1.0]\n",
      "0.009529565460979939 0.004420681390911341 0.005108884070068598 [0.0, 1.0]\n",
      "0.009522255510091782 0.004415927454829216 0.005106328520923853 [0.0, 1.0]\n",
      "0.009514947421848774 0.004411191213876009 0.005103756207972765 [0.0, 1.0]\n",
      "0.009507657960057259 0.004406473599374294 0.005101184360682964 [0.0, 1.0]\n",
      "0.009500354528427124 0.004401774611324072 0.005098579451441765 [0.0, 1.0]\n",
      "0.009493068791925907 0.0043970937840640545 0.005095975007861853 [0.0, 1.0]\n",
      "0.009485802613198757 0.004392431117594242 0.005093371495604515 [0.0, 1.0]\n",
      "0.009478520601987839 0.004387786611914635 0.005090734455734491 [0.0, 1.0]\n",
      "0.009471258148550987 0.00438316073268652 0.005088097881525755 [0.0, 1.0]\n",
      "0.009464014321565628 0.004378552548587322 0.005085462238639593 [0.0, 1.0]\n",
      "0.009456773288547993 0.004373962990939617 0.0050828102976083755 [0.0, 1.0]\n",
      "0.009449534118175507 0.004369391594082117 0.005080142058432102 [0.0, 1.0]\n",
      "0.009442295879125595 0.004364837892353535 0.005077457521110773 [0.0, 1.0]\n",
      "0.009435076266527176 0.004360302817076445 0.005074773449450731 [0.0, 1.0]\n",
      "0.009427858516573906 0.004355785436928272 0.005072073545306921 [0.0, 1.0]\n",
      "0.009420660324394703 0.004351286217570305 0.005069374106824398 [0.0, 1.0]\n",
      "0.009413446299731731 0.0043468051590025425 0.005066641140729189 [0.0, 1.0]\n",
      "0.009406251832842827 0.004342342261224985 0.005063909571617842 [0.0, 1.0]\n",
      "0.009399067610502243 0.0043378896079957485 0.0050611780025064945 [0.0, 1.0]\n",
      "0.009391885250806808 0.00433345464989543 0.005058430600911379 [0.0, 1.0]\n",
      "0.009384704753756523 0.004329037386924028 0.005055667366832495 [0.0, 1.0]\n",
      "0.009377559646964073 0.004324638284742832 0.005052921362221241 [0.0, 1.0]\n",
      "0.009370382875204086 0.004320257343351841 0.005050125531852245 [0.0, 1.0]\n",
      "0.009363241493701935 0.0043158940970897675 0.00504734693095088 [0.0, 1.0]\n",
      "0.009356092661619186 0.004311540629714727 0.005044552497565746 [0.0, 1.0]\n",
      "0.00934896431863308 0.004307205323129892 0.005041758995503187 [0.0, 1.0]\n",
      "0.009341836906969547 0.004302887711673975 0.005038949195295572 [0.0, 1.0]\n",
      "0.009334703907370567 0.004298580344766378 0.005036123096942902 [0.0, 1.0]\n",
      "0.009327588602900505 0.0042942906729876995 0.0050332979299128056 [0.0, 1.0]\n",
      "0.009320491924881935 0.004290018696337938 0.005030473694205284 [0.0, 1.0]\n",
      "0.009313397109508514 0.004285764414817095 0.005027633160352707 [0.0, 1.0]\n",
      "0.009306296706199646 0.004281519912183285 0.005024776328355074 [0.0, 1.0]\n",
      "0.009299213998019695 0.004277293104678392 0.005021920893341303 [0.0, 1.0]\n",
      "0.009292133152484894 0.004273083992302418 0.005019048694521189 [0.0, 1.0]\n",
      "0.00928504578769207 0.004268885124474764 0.0050161611288785934 [0.0, 1.0]\n",
      "0.009277977049350739 0.00426470348611474 0.0050132740288972855 [0.0, 1.0]\n",
      "0.009270919486880302 0.00426053162664175 0.005010387860238552 [0.0, 1.0]\n",
      "0.009263879619538784 0.004256377462297678 0.005007502157241106 [0.0, 1.0]\n",
      "0.009256824851036072 0.004252240993082523 0.005004583857953548 [0.0, 1.0]\n",
      "0.00924978032708168 0.004248114302754402 0.0050016664899885654 [0.0, 1.0]\n",
      "0.009242755360901356 0.004244005307555199 0.004998750053346157 [0.0, 1.0]\n",
      "0.009235722944140434 0.004239905625581741 0.004995817318558693 [0.0, 1.0]\n",
      "0.009228709153831005 0.004235823638737202 0.004992885515093803 [0.0, 1.0]\n",
      "0.009221706539392471 0.0042317514307796955 0.0049899546429514885 [0.0, 1.0]\n",
      "0.009214703924953938 0.00422769645228982 0.004987007472664118 [0.0, 1.0]\n",
      "0.00920769665390253 0.004223651718348265 0.004984044935554266 [0.0, 1.0]\n",
      "0.009200707077980042 0.004219623748213053 0.0049810828641057014 [0.0, 1.0]\n",
      "0.009193727746605873 0.004215606022626162 0.0049781217239797115 [0.0, 1.0]\n",
      "0.009186750277876854 0.0042116050608456135 0.004975144751369953 [0.0, 1.0]\n",
      "0.009179783053696156 0.004207614343613386 0.004972168710082769 [0.0, 1.0]\n",
      "0.009172809310257435 0.004203632939606905 0.00496917637065053 [0.0, 1.0]\n",
      "0.009165854193270206 0.004199668765068054 0.004966185428202152 [0.0, 1.0]\n",
      "0.009158909320831299 0.004195714369416237 0.004963195417076349 [0.0, 1.0]\n",
      "0.009151949547231197 0.00419177720323205 0.004960172343999147 [0.0, 1.0]\n",
      "0.00914501678198576 0.0041878498159348965 0.004957166966050863 [0.0, 1.0]\n",
      "0.009138094261288643 0.004183931741863489 0.004954162985086441 [0.0, 1.0]\n",
      "0.009131156839430332 0.004180030897259712 0.00495112594217062 [0.0, 1.0]\n",
      "0.009124246425926685 0.004176139831542969 0.004948106594383717 [0.0, 1.0]\n",
      "0.009117329493165016 0.0041722580790519714 0.004945071414113045 [0.0, 1.0]\n",
      "0.009110430255532265 0.004168393090367317 0.0049420371651649475 [0.0, 1.0]\n",
      "0.009103525429964066 0.004164538346230984 0.004938987549394369 [0.0, 1.0]\n",
      "0.00909664761275053 0.004160692449659109 0.004935955163091421 [0.0, 1.0]\n",
      "0.009089754894375801 0.004156864248216152 0.004932890180498362 [0.0, 1.0]\n",
      "0.009082871489226818 0.004153045359998941 0.004929826129227877 [0.0, 1.0]\n",
      "0.009076016023755074 0.004149235785007477 0.004926780238747597 [0.0, 1.0]\n",
      "0.009069137275218964 0.004145435523241758 0.004923701286315918 [0.0, 1.0]\n",
      "0.009062276221811771 0.00414165249094367 0.004920623730868101 [0.0, 1.0]\n",
      "0.0090554254129529 0.004137878771871328 0.004917546641081572 [0.0, 1.0]\n",
      "0.00904858484864235 0.004134114366024733 0.004914470948278904 [0.0, 1.0]\n",
      "0.009041739627718925 0.00413035973906517 0.004911379422992468 [0.0, 1.0]\n",
      "0.009034911170601845 0.004126621875911951 0.004908288829028606 [0.0, 1.0]\n",
      "0.00902809202671051 0.004122893325984478 0.0049051991663873196 [0.0, 1.0]\n",
      "0.009021267294883728 0.004119174089282751 0.004902093671262264 [0.0, 1.0]\n",
      "0.009014453738927841 0.00411546416580677 0.004898989573121071 [0.0, 1.0]\n",
      "0.009007632732391357 0.004111763555556536 0.004895869176834822 [0.0, 1.0]\n",
      "0.009000839665532112 0.004108072258532047 0.0048927669413387775 [0.0, 1.0]\n",
      "0.008994029834866524 0.004104398190975189 0.004889632109552622 [0.0, 1.0]\n",
      "0.008987247943878174 0.00410073297098279 0.004886514972895384 [0.0, 1.0]\n",
      "0.00898047536611557 0.004097077064216137 0.0048833987675607204 [0.0, 1.0]\n",
      "0.008973697200417519 0.00409343047067523 0.004880267195403576 [0.0, 1.0]\n",
      "0.00896691344678402 0.004089793190360069 0.004877119790762663 [0.0, 1.0]\n",
      "0.00896015577018261 0.004086165223270655 0.004873990081250668 [0.0, 1.0]\n",
      "0.008953407406806946 0.004082546569406986 0.004870861303061247 [0.0, 1.0]\n",
      "0.008946636691689491 0.004078936763107777 0.004867700394243002 [0.0, 1.0]\n",
      "0.008939910680055618 0.0040753367356956005 0.0048645734786987305 [0.0, 1.0]\n",
      "0.008933167904615402 0.00407175300642848 0.004861414432525635 [0.0, 1.0]\n",
      "0.008926435373723507 0.004068178590387106 0.004858256783336401 [0.0, 1.0]\n",
      "0.00891969632357359 0.004064613487571478 0.0048550828360021114 [0.0, 1.0]\n",
      "0.008912984281778336 0.004061057232320309 0.004851927049458027 [0.0, 1.0]\n",
      "0.00890626572072506 0.004057510290294886 0.004848755896091461 [0.0, 1.0]\n",
      "0.008899558335542679 0.004053972661495209 0.004845585208386183 [0.0, 1.0]\n",
      "0.008892860263586044 0.004050443880259991 0.004842415917664766 [0.0, 1.0]\n",
      "0.008886171504855156 0.004046924412250519 0.0048392475582659245 [0.0, 1.0]\n",
      "0.00887947715818882 0.004043413791805506 0.0048360638320446014 [0.0, 1.0]\n",
      "0.008872777223587036 0.004039912484586239 0.00483286427333951 [0.0, 1.0]\n",
      "0.008866120129823685 0.004036420490592718 0.004829699173569679 [0.0, 1.0]\n",
      "0.008859438821673393 0.004032937344163656 0.004826501477509737 [0.0, 1.0]\n",
      "0.00885278545320034 0.004029463045299053 0.00482332194224 [0.0, 1.0]\n",
      "0.008846117183566093 0.004025990609079599 0.004820126574486494 [0.0, 1.0]\n",
      "0.008839460089802742 0.004022527020424604 0.00481693260371685 [0.0, 1.0]\n",
      "0.008832812309265137 0.004019072744995356 0.004813739564269781 [0.0, 1.0]\n",
      "0.008826174773275852 0.004015627317130566 0.0048105474561452866 [0.0, 1.0]\n",
      "0.008819530718028545 0.0040121907368302345 0.004807339981198311 [0.0, 1.0]\n",
      "0.00881289690732956 0.00400876346975565 0.00480413343757391 [0.0, 1.0]\n",
      "0.008806273341178894 0.0040053450502455235 0.004800928290933371 [0.0, 1.0]\n",
      "0.008799659088253975 0.004001935478299856 0.004797724075615406 [0.0, 1.0]\n",
      "0.008793039247393608 0.003998535219579935 0.004794504027813673 [0.0, 1.0]\n",
      "0.00878644548356533 0.003995143808424473 0.004791302140802145 [0.0, 1.0]\n",
      "0.008779838681221008 0.0039917537942528725 0.0047880844213068485 [0.0, 1.0]\n",
      "0.0087732570245862 0.003988372627645731 0.00478488439694047 [0.0, 1.0]\n",
      "0.008766669780015945 0.003985000774264336 0.00478166900575161 [0.0, 1.0]\n",
      "0.008760092779994011 0.003981637768447399 0.0047784545458853245 [0.0, 1.0]\n",
      "0.00875350832939148 0.0039782836101949215 0.004775225184857845 [0.0, 1.0]\n",
      "0.008746935054659843 0.003974938299506903 0.0047719962894916534 [0.0, 1.0]\n",
      "0.008740371093153954 0.003971601836383343 0.0047687687911093235 [0.0, 1.0]\n",
      "0.008733808994293213 0.003968266770243645 0.004765542224049568 [0.0, 1.0]\n",
      "0.008727258071303368 0.003964941017329693 0.004762317053973675 [0.0, 1.0]\n",
      "0.008720716461539268 0.0039616236463189125 0.004759092815220356 [0.0, 1.0]\n",
      "0.008714169263839722 0.003958315588533878 0.004755853209644556 [0.0, 1.0]\n",
      "0.00870764721184969 0.003955016378313303 0.0047526308335363865 [0.0, 1.0]\n",
      "0.008701112121343613 0.003951718099415302 0.004749393556267023 [0.0, 1.0]\n",
      "0.008694586344063282 0.003948429133743048 0.004746157210320234 [0.0, 1.0]\n",
      "0.008688070811331272 0.003945149015635252 0.00474292179569602 [0.0, 1.0]\n",
      "0.00868154875934124 0.003941877745091915 0.004739671479910612 [0.0, 1.0]\n",
      "0.008675046265125275 0.003938607405871153 0.0047364383935928345 [0.0, 1.0]\n",
      "0.008668553084135056 0.003935346379876137 0.004733206704258919 [0.0, 1.0]\n",
      "0.00866205245256424 0.003932093735784292 0.004729959182441235 [0.0, 1.0]\n",
      "0.008655563928186893 0.003928850404918194 0.0047267135232687 [0.0, 1.0]\n",
      "0.008649076335132122 0.00392560800537467 0.004723468329757452 [0.0, 1.0]\n",
      "0.008642598986625671 0.003922374919056892 0.004720224533230066 [0.0, 1.0]\n",
      "0.008636131882667542 0.003919150214642286 0.004716981668025255 [0.0, 1.0]\n",
      "0.008629674091935158 0.003915934357792139 0.004713740199804306 [0.0, 1.0]\n",
      "0.00862320326268673 0.003912719897925854 0.004710483364760876 [0.0, 1.0]\n",
      "0.008616741746664047 0.00390951381996274 0.00470722746104002 [0.0, 1.0]\n",
      "0.008610306307673454 0.003906317055225372 0.004703989252448082 [0.0, 1.0]\n",
      "0.008603873662650585 0.003903121454641223 0.004700751975178719 [0.0, 1.0]\n",
      "0.00859741773456335 0.003899934468790889 0.0046974834986031055 [0.0, 1.0]\n",
      "0.00859098881483078 0.0038967563305050135 0.004694232251495123 [0.0, 1.0]\n",
      "0.008584577590227127 0.0038935793563723564 0.004690998233854771 [0.0, 1.0]\n",
      "0.008578144013881683 0.003890411229804158 0.004687733016908169 [0.0, 1.0]\n",
      "0.008571736514568329 0.003887251717969775 0.004684485029429197 [0.0, 1.0]\n",
      "0.00856531597673893 0.003884093603119254 0.004681222140789032 [0.0, 1.0]\n",
      "0.008558920584619045 0.003880944335833192 0.004677976481616497 [0.0, 1.0]\n",
      "0.008552518673241138 0.003877803450450301 0.004674715455621481 [0.0, 1.0]\n",
      "0.008546119555830956 0.0038746639620512724 0.004671455826610327 [0.0, 1.0]\n",
      "0.008539747446775436 0.0038715333212167025 0.00466821389272809 [0.0, 1.0]\n",
      "0.008533366955816746 0.003868411062285304 0.004664956126362085 [0.0, 1.0]\n",
      "0.008526990190148354 0.0038652904331684113 0.00466170022264123 [0.0, 1.0]\n",
      "0.008520622737705708 0.00386217818595469 0.004658444784581661 [0.0, 1.0]\n",
      "0.00851425901055336 0.0038590673357248306 0.004655191209167242 [0.0, 1.0]\n",
      "0.008507903665304184 0.0038559651002287865 0.00465193809941411 [0.0, 1.0]\n",
      "0.008501541800796986 0.0038528714794665575 0.0046486700884997845 [0.0, 1.0]\n",
      "0.00849519856274128 0.003849779022857547 0.0046454197727143764 [0.0, 1.0]\n",
      "0.008488848805427551 0.0038466951809823513 0.004642154090106487 [0.0, 1.0]\n",
      "0.00848251860588789 0.0038436127360910177 0.0046389056369662285 [0.0, 1.0]\n",
      "0.008476180955767632 0.0038405389059334993 0.004635642282664776 [0.0, 1.0]\n",
      "0.008469853550195694 0.003837473690509796 0.004632379859685898 [0.0, 1.0]\n",
      "0.008463544771075249 0.0038344096392393112 0.0046291351318359375 [0.0, 1.0]\n",
      "0.008457229472696781 0.0038313542027026415 0.004625875037163496 [0.0, 1.0]\n",
      "0.008450916036963463 0.00382829993031919 0.0046226163394749165 [0.0, 1.0]\n",
      "0.008444612845778465 0.0038252542726695538 0.004619359038770199 [0.0, 1.0]\n",
      "0.008438312448561192 0.0038222100120037794 0.0046161022037267685 [0.0, 1.0]\n",
      "0.008432021364569664 0.0038191741332411766 0.0046128467656672 [0.0, 1.0]\n",
      "0.008425739593803883 0.003816146869212389 0.004609592724591494 [0.0, 1.0]\n",
      "0.008419460617005825 0.0038131207693368196 0.004606339614838362 [0.0, 1.0]\n",
      "0.008413175120949745 0.0038101032841950655 0.004603071603924036 [0.0, 1.0]\n",
      "0.008406907320022583 0.0038070869632065296 0.004599820822477341 [0.0, 1.0]\n",
      "0.008400633931159973 0.003804079256951809 0.0045965551398694515 [0.0, 1.0]\n",
      "0.008394379168748856 0.003801072482019663 0.004593306686729193 [0.0, 1.0]\n",
      "0.008388117887079716 0.0037980745546519756 0.00459004333242774 [0.0, 1.0]\n",
      "0.008381875231862068 0.0037950777914375067 0.004586797207593918 [0.0, 1.0]\n",
      "0.008375626057386398 0.0037920894101262093 0.004583536181598902 [0.0, 1.0]\n",
      "0.008369393646717072 0.00378910219296813 0.004580291919410229 [0.0, 1.0]\n",
      "0.008363156579434872 0.003786123590543866 0.004577033221721649 [0.0, 1.0]\n",
      "0.008356938138604164 0.0037831461522728205 0.0045737917535007 [0.0, 1.0]\n",
      "0.00835071224719286 0.0037801770959049463 0.004570535384118557 [0.0, 1.0]\n",
      "0.008344489149749279 0.0037772092036902905 0.004567279946058989 [0.0, 1.0]\n",
      "0.008338291198015213 0.0037742499262094498 0.0045640417374670506 [0.0, 1.0]\n",
      "0.008332080207765102 0.0037712918128818274 0.004560788627713919 [0.0, 1.0]\n",
      "0.008325878530740738 0.0037683420814573765 0.004557536914944649 [0.0, 1.0]\n",
      "0.008319679647684097 0.003765393514186144 0.004554286133497953 [0.0, 1.0]\n",
      "0.008313490077853203 0.0037624535616487265 0.00455103674903512 [0.0, 1.0]\n",
      "0.008307302370667458 0.0037595145404338837 0.004547788295894861 [0.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008301124908030033 0.003756584133952856 0.004544540774077177 [0.0, 1.0]\n",
      "0.008294950239360332 0.0037536548916250467 0.004541295114904642 [0.0, 1.0]\n",
      "0.008288783952593803 0.003750734031200409 0.0045380499213933945 [0.0, 1.0]\n",
      "0.008282620459794998 0.0037478143349289894 0.004534806124866009 [0.0, 1.0]\n",
      "0.008276466280221939 0.0037449030205607414 0.004531563725322485 [0.0, 1.0]\n",
      "0.008270314894616604 0.003741992637515068 0.004528322257101536 [0.0, 1.0]\n",
      "0.00826414953917265 0.0037390836514532566 0.004525065887719393 [0.0, 1.0]\n",
      "0.008258026093244553 0.0037361830472946167 0.004521842580288649 [0.0, 1.0]\n",
      "0.008251888677477837 0.003733283607289195 0.004518604837357998 [0.0, 1.0]\n",
      "0.008245744742453098 0.003730392549186945 0.004515352193266153 [0.0, 1.0]\n",
      "0.008239618502557278 0.0037275024224072695 0.004512116312980652 [0.0, 1.0]\n",
      "0.008233502507209778 0.003724620910361409 0.004508881829679012 [0.0, 1.0]\n",
      "0.008227389305830002 0.003721740562468767 0.004505648743361235 [0.0, 1.0]\n",
      "0.008221277967095375 0.0037188611458986998 0.004502416588366032 [0.0, 1.0]\n",
      "0.008215175941586494 0.003715990111231804 0.004499185364693403 [0.0, 1.0]\n",
      "0.008209075778722763 0.0037131202407181263 0.004495955538004637 [0.0, 1.0]\n",
      "0.008202970027923584 0.0037102587521076202 0.0044927108101546764 [0.0, 1.0]\n",
      "0.008196881972253323 0.0037073984276503325 0.0044894833117723465 [0.0, 1.0]\n",
      "0.008190787397325039 0.003704546485096216 0.004486240912228823 [0.0, 1.0]\n",
      "0.008184711448848248 0.0037016954738646746 0.004483015742152929 [0.0, 1.0]\n",
      "0.008178637363016605 0.003698845626786351 0.004479791969060898 [0.0, 1.0]\n",
      "0.008172573521733284 0.0036960041616111994 0.004476569127291441 [0.0, 1.0]\n",
      "0.008166495710611343 0.003693163860589266 0.00447333138436079 [0.0, 1.0]\n",
      "0.008160442113876343 0.00369033170863986 0.00447011087089777 [0.0, 1.0]\n",
      "0.00815439224243164 0.0036875007208436728 0.004466891288757324 [0.0, 1.0]\n",
      "0.008148344233632088 0.0036846708972007036 0.00446367310360074 [0.0, 1.0]\n",
      "0.008142305538058281 0.0036818492226302624 0.004460456315428019 [0.0, 1.0]\n",
      "0.008136268705129623 0.0036790287122130394 0.004457240458577871 [0.0, 1.0]\n",
      "0.008130226284265518 0.003676216583698988 0.00445400970056653 [0.0, 1.0]\n",
      "0.00812420155853033 0.003673405386507511 0.0044507961720228195 [0.0, 1.0]\n",
      "0.008118178695440292 0.0036705953534692526 0.004447583574801683 [0.0, 1.0]\n",
      "0.008112150244414806 0.0036677937023341656 0.004444356542080641 [0.0, 1.0]\n",
      "0.008106139488518238 0.003664992982521653 0.004441146273165941 [0.0, 1.0]\n",
      "0.008100130595266819 0.003662193426862359 0.004437937401235104 [0.0, 1.0]\n",
      "0.00809413194656372 0.003659402020275593 0.004434729926288128 [0.0, 1.0]\n",
      "0.008088134229183197 0.003656611777842045 0.0044315229170024395 [0.0, 1.0]\n",
      "0.0080821318551898 0.0036538299173116684 0.004428301937878132 [0.0, 1.0]\n",
      "0.008076146245002747 0.0036510489881038666 0.00442509725689888 [0.0, 1.0]\n",
      "0.008070163428783417 0.003648269223049283 0.004421894438564777 [0.0, 1.0]\n",
      "0.008064189925789833 0.0036454976070672274 0.004418692551553249 [0.0, 1.0]\n",
      "0.008058218285441399 0.0036427269224077463 0.004415491595864296 [0.0, 1.0]\n",
      "0.008052249439060688 0.0036399574019014835 0.0044122920371592045 [0.0, 1.0]\n",
      "0.008046289905905724 0.0036371962632983923 0.004409093409776688 [0.0, 1.0]\n",
      "0.008040332235395908 0.0036344360560178757 0.004405896179378033 [0.0, 1.0]\n",
      "0.008034376427531242 0.0036316767800599337 0.004402699880301952 [0.0, 1.0]\n",
      "0.008028430864214897 0.003628925886005163 0.004399504978209734 [0.0, 1.0]\n",
      "0.008022487163543701 0.0036261759232729673 0.00439631100744009 [0.0, 1.0]\n",
      "0.008016545325517654 0.0036234271246939898 0.004393118433654308 [0.0, 1.0]\n",
      "0.008010612800717354 0.00362068647518754 0.004389926791191101 [0.0, 1.0]\n",
      "0.008004684001207352 0.0036179469898343086 0.004386736545711756 [0.0, 1.0]\n",
      "0.007998762652277946 0.0036152154207229614 0.004383547231554985 [0.0, 1.0]\n",
      "0.00799284502863884 0.003612485248595476 0.004380359314382076 [0.0, 1.0]\n",
      "0.007986928336322308 0.003609755774959922 0.004377172328531742 [0.0, 1.0]\n",
      "0.007981020957231522 0.003607034683227539 0.00437398673966527 [0.0, 1.0]\n",
      "0.00797511637210846 0.003604314522817731 0.004370802082121372 [0.0, 1.0]\n",
      "0.007969229482114315 0.003601595526561141 0.004367634188383818 [0.0, 1.0]\n",
      "0.007963329553604126 0.0035988774616271257 0.004364452324807644 [0.0, 1.0]\n",
      "0.007957438006997108 0.0035961675457656384 0.004361270926892757 [0.0, 1.0]\n",
      "0.007951565086841583 0.0035934585612267256 0.004358106758445501 [0.0, 1.0]\n",
      "0.007945679128170013 0.003590750740841031 0.004354928154498339 [0.0, 1.0]\n",
      "0.007939801551401615 0.0035880510695278645 0.004351750481873751 [0.0, 1.0]\n",
      "0.00793394260108471 0.003585352562367916 0.004348589573055506 [0.0, 1.0]\n",
      "0.00792806874960661 0.0035826547536998987 0.004345414228737354 [0.0, 1.0]\n",
      "0.007922220975160599 0.003579965326935053 0.004342256113886833 [0.0, 1.0]\n",
      "0.007916375063359737 0.0035772768314927816 0.004339098464697599 [0.0, 1.0]\n",
      "0.007910516113042831 0.003574589267373085 0.004335926845669746 [0.0, 1.0]\n",
      "0.00790468230843544 0.00357191008515656 0.0043327719904482365 [0.0, 1.0]\n",
      "0.007898833602666855 0.003569231601431966 0.004329602234065533 [0.0, 1.0]\n",
      "0.007893003523349762 0.00356655428186059 0.004326449707150459 [0.0, 1.0]\n",
      "0.007887167856097221 0.003563885111361742 0.004323282279074192 [0.0, 1.0]\n",
      "0.007881348952651024 0.0035612168721854687 0.004320132080465555 [0.0, 1.0]\n",
      "0.007875517010688782 0.00355854956433177 0.0043169669806957245 [0.0, 1.0]\n",
      "0.007869702763855457 0.0035558834206312895 0.004313819110393524 [0.0, 1.0]\n",
      "0.007863897830247879 0.0035532251931726933 0.0043106721714138985 [0.0, 1.0]\n",
      "0.007858093827962875 0.0035505681298673153 0.004307526163756847 [0.0, 1.0]\n",
      "0.00785229355096817 0.003547911997884512 0.004304381553083658 [0.0, 1.0]\n",
      "0.007846502587199211 0.0035452640149742365 0.004301238339394331 [0.0, 1.0]\n",
      "0.007840712554752827 0.0035426169633865356 0.004298095591366291 [0.0, 1.0]\n",
      "0.007834925316274166 0.0035399708431214094 0.0042949547059834 [0.0, 1.0]\n",
      "0.007829147391021252 0.003537332871928811 0.004291814751923084 [0.0, 1.0]\n",
      "0.007823371328413486 0.0035346958320587873 0.004288675729185343 [0.0, 1.0]\n",
      "0.007817598059773445 0.003532059956341982 0.004285538103431463 [0.0, 1.0]\n",
      "0.007811826188117266 0.0035294247791171074 0.004282401409000158 [0.0, 1.0]\n",
      "0.007806063629686832 0.0035267979837954044 0.004279265645891428 [0.0, 1.0]\n",
      "0.007800319232046604 0.0035241718869656324 0.004276147112250328 [0.0, 1.0]\n",
      "0.007794560864567757 0.0035215469542890787 0.004273014143109322 [0.0, 1.0]\n",
      "0.007788811810314655 0.0035189299378544092 0.00426988210529089 [0.0, 1.0]\n",
      "0.007783064618706703 0.0035163138527423143 0.0042667509987950325 [0.0, 1.0]\n",
      "0.007777336053550243 0.0035136989317834377 0.004263637121766806 [0.0, 1.0]\n",
      "0.007771608419716358 0.0035110849421471357 0.004260523710399866 [0.0, 1.0]\n",
      "0.007765875197947025 0.003508478868752718 0.004257396329194307 [0.0, 1.0]\n",
      "0.007760143838822842 0.003505873726680875 0.0042542703449726105 [0.0, 1.0]\n",
      "0.007754430174827576 0.00350326974876225 0.004251160658895969 [0.0, 1.0]\n",
      "0.007748719304800034 0.0035006667021661997 0.00424805236980319 [0.0, 1.0]\n",
      "0.0077430009841918945 0.0034980715718120337 0.004244929179549217 [0.0, 1.0]\n",
      "0.007737300358712673 0.0034954773727804422 0.004241823218762875 [0.0, 1.0]\n",
      "0.007731602527201176 0.0034928841050714254 0.004238718189299107 [0.0, 1.0]\n",
      "0.00772590609267354 0.003490292001515627 0.004235614091157913 [0.0, 1.0]\n",
      "0.007720218971371651 0.0034877078142017126 0.004232511390000582 [0.0, 1.0]\n",
      "0.007714534178376198 0.003485124558210373 0.004229409620165825 [0.0, 1.0]\n",
      "0.007708851248025894 0.003482542233541608 0.00422630924731493 [0.0, 1.0]\n",
      "0.007703177630901337 0.0034799680579453707 0.00422320980578661 [0.0, 1.0]\n",
      "0.007697506342083216 0.0034773945808410645 0.004220111761242151 [0.0, 1.0]\n",
      "0.007691836915910244 0.0034748222678899765 0.0042170146480202675 [0.0, 1.0]\n",
      "0.007686169818043709 0.003472250886261463 0.004213918931782246 [0.0, 1.0]\n",
      "0.007680511102080345 0.003469687420874834 0.004210823681205511 [0.0, 1.0]\n",
      "0.007674855180084705 0.0034671248868107796 0.004207730293273926 [0.0, 1.0]\n",
      "0.007669201120734215 0.0034645632840692997 0.004204637836664915 [0.0, 1.0]\n",
      "0.007663564290851355 0.0034620026126503944 0.00420156167820096 [0.0, 1.0]\n",
      "0.007657921407371759 0.0034594498574733734 0.004198471549898386 [0.0, 1.0]\n",
      "0.007652295753359795 0.0034568982664495707 0.004195397719740868 [0.0, 1.0]\n",
      "0.0076466575264930725 0.003454347373917699 0.00419230991974473 [0.0, 1.0]\n",
      "0.0076410360634326935 0.0034517976455390453 0.004189238417893648 [0.0, 1.0]\n",
      "0.007635408081114292 0.0034492556005716324 0.00418615248054266 [0.0, 1.0]\n",
      "0.0076297977939248085 0.0034467147197574377 0.0041830833069980145 [0.0, 1.0]\n",
      "0.007624190300703049 0.003444174537435174 0.004180015530437231 [0.0, 1.0]\n",
      "0.007618584204465151 0.0034416355192661285 0.004176948685199022 [0.0, 1.0]\n",
      "0.007612971588969231 0.0034391044173389673 0.004173867404460907 [0.0, 1.0]\n",
      "0.0076073771342635155 0.0034365742467343807 0.004170802887529135 [0.0, 1.0]\n",
      "0.007601784076541662 0.003434044774621725 0.004167739301919937 [0.0, 1.0]\n",
      "0.007596192881464958 0.0034315164666622877 0.004164676647633314 [0.0, 1.0]\n",
      "0.007590611465275288 0.0034289960749447346 0.004161615390330553 [0.0, 1.0]\n",
      "0.007585031446069479 0.0034264763817191124 0.004158555064350367 [0.0, 1.0]\n",
      "0.007579454220831394 0.0034239578526467085 0.004155496135354042 [0.0, 1.0]\n",
      "0.007573878392577171 0.003421440254896879 0.004152438137680292 [0.0, 1.0]\n",
      "0.007568320259451866 0.003418923355638981 0.004149396903812885 [0.0, 1.0]\n",
      "0.007562755607068539 0.0034164146054536104 0.004146341234445572 [0.0, 1.0]\n",
      "0.007557208649814129 0.003413906553760171 0.004143301863223314 [0.0, 1.0]\n",
      "0.0075516477227211 0.003411399433389306 0.0041402485221624374 [0.0, 1.0]\n",
      "0.007546104956418276 0.0034088934771716595 0.004137211479246616 [0.0, 1.0]\n",
      "0.007540571037679911 0.0034063952043652534 0.004134175833314657 [0.0, 1.0]\n",
      "0.0075350236147642136 0.003403897862881422 0.0041311257518827915 [0.0, 1.0]\n",
      "0.007529493421316147 0.0034014014527201653 0.004128091968595982 [0.0, 1.0]\n",
      "0.0075239501893520355 0.003398905973881483 0.0041250442154705524 [0.0, 1.0]\n",
      "0.007518431171774864 0.003396418411284685 0.004122012760490179 [0.0, 1.0]\n",
      "0.007512914482504129 0.003393931780010462 0.004118982702493668 [0.0, 1.0]\n",
      "0.007507399655878544 0.0033914458472281694 0.004115953575819731 [0.0, 1.0]\n",
      "0.00750188622623682 0.0033889610785990953 0.0041129253804683685 [0.0, 1.0]\n",
      "0.0074963755905628204 0.003386477008461952 0.004109898582100868 [0.0, 1.0]\n",
      "0.007490873336791992 0.0033840008545666933 0.0041068727150559425 [0.0, 1.0]\n",
      "0.007485373876988888 0.003381525631994009 0.004103848244994879 [0.0, 1.0]\n",
      "0.007479876279830933 0.0033790513407438993 0.00410082470625639 [0.0, 1.0]\n",
      "0.0074743954464793205 0.0033765779808163643 0.004097817465662956 [0.0, 1.0]\n",
      "0.007468908093869686 0.00337411230430007 0.004094795789569616 [0.0, 1.0]\n",
      "0.007463423535227776 0.0033716477919369936 0.004091775976121426 [0.0, 1.0]\n",
      "0.007457955740392208 0.0033691839780658484 0.004088771995157003 [0.0, 1.0]\n",
      "0.007452474907040596 0.0033667210955172777 0.004085754044353962 [0.0, 1.0]\n",
      "0.007447011768817902 0.0033642591442912817 0.004082752391695976 [0.0, 1.0]\n",
      "0.0074415565468370914 0.0033618048764765263 0.004079751670360565 [0.0, 1.0]\n",
      "0.007436088286340237 0.0033593515399843454 0.004076736979186535 [0.0, 1.0]\n",
      "0.007430638186633587 0.003356899367645383 0.00407373858615756 [0.0, 1.0]\n",
      "0.007425189018249512 0.0033544478937983513 0.00407074112445116 [0.0, 1.0]\n",
      "0.007419749163091183 0.0033520041033625603 0.0040677450597286224 [0.0, 1.0]\n",
      "0.007414311170578003 0.003349561244249344 0.004064749926328659 [0.0, 1.0]\n",
      "0.00740887550637126 0.003347119316458702 0.004061756189912558 [0.0, 1.0]\n",
      "0.007403441704809666 0.003344678319990635 0.004058763384819031 [0.0, 1.0]\n",
      "0.007398009765893221 0.0033422382548451424 0.0040557715110480785 [0.0, 1.0]\n",
      "0.007392587140202522 0.0033398058731108904 0.004052781034260988 [0.0, 1.0]\n",
      "0.007387181278318167 0.003337374422699213 0.004049806855618954 [0.0, 1.0]\n",
      "0.007381762377917767 0.0033349439036101103 0.004046818241477013 [0.0, 1.0]\n",
      "0.007376345340162516 0.003332514315843582 0.0040438310243189335 [0.0, 1.0]\n",
      "0.007370945066213608 0.003330085426568985 0.004040859639644623 [0.0, 1.0]\n",
      "0.007365554105490446 0.003327664453536272 0.004037889651954174 [0.0, 1.0]\n",
      "0.00736015010625124 0.00332524417899549 0.004034905694425106 [0.0, 1.0]\n",
      "0.007354763336479664 0.0033228250686079264 0.004031938035041094 [0.0, 1.0]\n",
      "0.007349363062530756 0.0033204066567122936 0.004028956405818462 [0.0, 1.0]\n",
      "0.007343986537307501 0.0033179959282279015 0.004025990609079599 [0.0, 1.0]\n",
      "0.007338612340390682 0.003315586131066084 0.004023026209324598 [0.0, 1.0]\n",
      "0.007333240006119013 0.003313177265226841 0.004020062740892172 [0.0, 1.0]\n",
      "0.007327869534492493 0.0033107693307101727 0.00401710020378232 [0.0, 1.0]\n",
      "0.007322501391172409 0.0033083620946854353 0.00401413906365633 [0.0, 1.0]\n",
      "0.007317141629755497 0.003305962774902582 0.004011178854852915 [0.0, 1.0]\n",
      "0.0073117841966450214 0.00330356415361166 0.0040082200430333614 [0.0, 1.0]\n",
      "0.007306428626179695 0.0033011664636433125 0.004005262162536383 [0.0, 1.0]\n",
      "0.007301074918359518 0.0032987697049975395 0.0040023052133619785 [0.0, 1.0]\n",
      "0.007295737974345684 0.0032963736448436975 0.00399936456233263 [0.0, 1.0]\n",
      "0.007290394976735115 0.00329398550093174 0.003996409475803375 [0.0, 1.0]\n",
      "0.007285054307430983 0.003291598055511713 0.0039934562519192696 [0.0, 1.0]\n",
      "0.007279730401933193 0.003289211541414261 0.003990518860518932 [0.0, 1.0]\n",
      "0.007274392992258072 0.0032868259586393833 0.003987567033618689 [0.0, 1.0]\n",
      "0.007269073277711868 0.0032844413071870804 0.003984631970524788 [0.0, 1.0]\n",
      "0.007263761945068836 0.0032820641063153744 0.003981697838753462 [0.0, 1.0]\n",
      "0.0072584375739097595 0.0032796880695968866 0.0039787497371435165 [0.0, 1.0]\n",
      "0.007253129966557026 0.003277312498539686 0.00397581746801734 [0.0, 1.0]\n",
      "0.007247824687510729 0.003274938091635704 0.003972886595875025 [0.0, 1.0]\n",
      "0.007242536172270775 0.003272564383223653 0.003969972021877766 [0.0, 1.0]\n",
      "0.007237241603434086 0.003270198591053486 0.0039670430123806 [0.0, 1.0]\n",
      "0.007231933996081352 0.00326783349737525 0.003964100498706102 [0.0, 1.0]\n",
      "0.0072266580536961555 0.0032654693350195885 0.003961188718676567 [0.0, 1.0]\n",
      "0.007221369072794914 0.003263105871155858 0.0039582629688084126 [0.0, 1.0]\n",
      "0.007216081954538822 0.0032607433386147022 0.00395533861592412 [0.0, 1.0]\n",
      "0.007210803683847189 0.003258388489484787 0.003952415194362402 [0.0, 1.0]\n",
      "0.007205542176961899 0.0032560343388468027 0.00394950807094574 [0.0, 1.0]\n",
      "0.007200268097221851 0.0032536813523620367 0.003946586512029171 [0.0, 1.0]\n",
      "0.007195010781288147 0.0032513290643692017 0.003943681716918945 [0.0, 1.0]\n",
      "0.0071897548623383045 0.0032489774748682976 0.003940777387470007 [0.0, 1.0]\n",
      "0.007184501271694899 0.003246626816689968 0.0039378744550049305 [0.0, 1.0]\n",
      "0.007179241627454758 0.0032442838419228792 0.003934957552701235 [0.0, 1.0]\n",
      "0.0071739982813596725 0.003241941798478365 0.003932056482881308 [0.0, 1.0]\n",
      "0.007168757263571024 0.0032396004535257816 0.003929156810045242 [0.0, 1.0]\n",
      "0.007163518108427525 0.003237260039895773 0.003926258068531752 [0.0, 1.0]\n",
      "0.007158280815929174 0.003234920557588339 0.003923360258340836 [0.0, 1.0]\n",
      "0.007153052370995283 0.0032325885258615017 0.003920463845133781 [0.0, 1.0]\n",
      "0.007147825788706541 0.003230257425457239 0.003917568363249302 [0.0, 1.0]\n",
      "0.007142615504562855 0.0032279270235449076 0.003914688713848591 [0.0, 1.0]\n",
      "0.007137392647564411 0.0032255975529551506 0.0039117950946092606 [0.0, 1.0]\n",
      "0.007132186554372311 0.0032232690136879683 0.003908917773514986 [0.0, 1.0]\n",
      "0.007126974873244762 0.0032209481578320265 0.0039060264825820923 [0.0, 1.0]\n",
      "0.007121779024600983 0.0032186280004680157 0.0039031512569636106 [0.0, 1.0]\n",
      "0.007116570603102446 0.003216308541595936 0.0039002620615065098 [0.0, 1.0]\n",
      "0.007111378945410252 0.0032139900140464306 0.003897388931363821 [0.0, 1.0]\n",
      "0.007106189150363207 0.0032116724178195 0.003894516732543707 [0.0, 1.0]\n",
      "0.00710099283605814 0.0032093622721731663 0.003891630796715617 [0.0, 1.0]\n",
      "0.007095798850059509 0.003207053057849407 0.003888745792210102 [0.0, 1.0]\n",
      "0.007090636529028416 0.0032047447748482227 0.003885891754180193 [0.0, 1.0]\n",
      "0.007085446268320084 0.0032024371903389692 0.0038830090779811144 [0.0, 1.0]\n",
      "0.0070802876725792885 0.0032001305371522903 0.003880157135426998 [0.0, 1.0]\n",
      "0.007075116038322449 0.0031978245824575424 0.0038772912230342627 [0.0, 1.0]\n",
      "0.0070699527859687805 0.003195526311174035 0.003874426707625389 [0.0, 1.0]\n",
      "0.0070647913962602615 0.0031932287383824587 0.0038715628907084465 [0.0, 1.0]\n",
      "0.007059632334858179 0.003190932096913457 0.003868700237944722 [0.0, 1.0]\n",
      "0.00705449003726244 0.003188636153936386 0.00386585365049541 [0.0, 1.0]\n",
      "0.007049349136650562 0.00318634114228189 0.0038630079943686724 [0.0, 1.0]\n",
      "0.00704420218244195 0.0031840535812079906 0.003860148601233959 [0.0, 1.0]\n",
      "0.0070390719920396805 0.003181766951456666 0.0038573050405830145 [0.0, 1.0]\n",
      "0.007033929228782654 0.003179481253027916 0.003854447742924094 [0.0, 1.0]\n",
      "0.007028802298009396 0.0031771960202604532 0.0038516062777489424 [0.0, 1.0]\n",
      "0.007023677695542574 0.0031749119516462088 0.003848765743896365 [0.0, 1.0]\n",
      "0.0070185549557209015 0.0031726285815238953 0.0038459263741970062 [0.0, 1.0]\n",
      "0.007013441063463688 0.0031703526619821787 0.0038430881686508656 [0.0, 1.0]\n",
      "0.007008328568190336 0.0031680776737630367 0.0038402508944272995 [0.0, 1.0]\n",
      "0.007003217935562134 0.0031658033840358257 0.003837414551526308 [0.0, 1.0]\n",
      "0.006998109631240368 0.0031635300256311893 0.003834579372778535 [0.0, 1.0]\n",
      "0.006993003189563751 0.0031612575985491276 0.00383174535818398 [0.0, 1.0]\n",
      "0.006987912580370903 0.0031589858699589968 0.00382892694324255 [0.0, 1.0]\n",
      "0.006982816383242607 0.003156721591949463 0.0038260947912931442 [0.0, 1.0]\n",
      "0.006977736949920654 0.00315445801243186 0.0038232787046581507 [0.0, 1.0]\n",
      "0.006972644478082657 0.0031521955970674753 0.003820448648184538 [0.0, 1.0]\n",
      "0.006967567838728428 0.003149933647364378 0.0038176344241946936 [0.0, 1.0]\n",
      "0.006962493993341923 0.0031476726289838552 0.0038148213643580675 [0.0, 1.0]\n",
      "0.006957413628697395 0.003145419294014573 0.003811994567513466 [0.0, 1.0]\n",
      "0.006952350027859211 0.0031431664247065783 0.0038091836031526327 [0.0, 1.0]\n",
      "0.006947273388504982 0.0031409147195518017 0.0038063586689531803 [0.0, 1.0]\n",
      "0.006942213512957096 0.0031386634800583124 0.00380354980006814 [0.0, 1.0]\n",
      "0.006937169469892979 0.0031364131718873978 0.0038007565308362246 [0.0, 1.0]\n",
      "0.006932113319635391 0.0031341637950390577 0.003797949757426977 [0.0, 1.0]\n",
      "0.006927065551280975 0.0031319218687713146 0.0037951436825096607 [0.0, 1.0]\n",
      "0.006922019645571709 0.0031296806409955025 0.003792339004576206 [0.0, 1.0]\n",
      "0.006916975602507591 0.003127440344542265 0.0037895350251346827 [0.0, 1.0]\n",
      "0.006911933422088623 0.0031252007465809584 0.003786732442677021 [0.0, 1.0]\n",
      "0.006906907074153423 0.0031229618471115828 0.0037839452270418406 [0.0, 1.0]\n",
      "0.00690188305452466 0.0031207238789647818 0.0037811591755598783 [0.0, 1.0]\n",
      "0.006896852981299162 0.0031184933613985777 0.0037783596199005842 [0.0, 1.0]\n",
      "0.006891824770718813 0.0031162637751549482 0.0037755609955638647 [0.0, 1.0]\n",
      "0.00688681285828352 0.0031140348874032497 0.00377277797088027 [0.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006881802808493376 0.003111806698143482 0.0037699961103498936 [0.0, 1.0]\n",
      "0.006876794621348381 0.0031095794402062893 0.0037672151811420918 [0.0, 1.0]\n",
      "0.0068717882968485355 0.0031073528807610273 0.003764435416087508 [0.0, 1.0]\n",
      "0.006866775453090668 0.0031051337718963623 0.003761641914024949 [0.0, 1.0]\n",
      "0.0068617938086390495 0.0031029153615236282 0.003758878679946065 [0.0, 1.0]\n",
      "0.006856800056993961 0.0031006978824734688 0.003756101941689849 [0.0, 1.0]\n",
      "0.006851807236671448 0.003098481334745884 0.0037533261347562075 [0.0, 1.0]\n",
      "0.006846816744655371 0.0030962652526795864 0.0037505514919757843 [0.0, 1.0]\n",
      "0.0068418425507843494 0.0030940501019358635 0.003747792448848486 [0.0, 1.0]\n",
      "0.006836862303316593 0.0030918424017727375 0.003745019668713212 [0.0, 1.0]\n",
      "0.006831883452832699 0.003089635632932186 0.0037422480527311563 [0.0, 1.0]\n",
      "0.006826906464993954 0.003087429329752922 0.0037394773680716753 [0.0, 1.0]\n",
      "0.006821946240961552 0.0030852241907268763 0.003736722283065319 [0.0, 1.0]\n",
      "0.006816987879574299 0.0030830195173621178 0.003733968362212181 [0.0, 1.0]\n",
      "0.006812037900090218 0.003080822294577956 0.0037312153726816177 [0.0, 1.0]\n",
      "0.006807074882090092 0.0030786260031163692 0.003728448646143079 [0.0, 1.0]\n",
      "0.006802128162235022 0.0030764304101467133 0.0037256977520883083 [0.0, 1.0]\n",
      "0.006797183305025101 0.003074235748499632 0.0037229477893561125 [0.0, 1.0]\n",
      "0.006792240310460329 0.003072041552513838 0.0037201987579464912 [0.0, 1.0]\n",
      "0.006787299178540707 0.003069848520681262 0.0037174508906900883 [0.0, 1.0]\n",
      "0.0067823599092662334 0.0030676559545099735 0.00371470395475626 [0.0, 1.0]\n",
      "0.006777429021894932 0.003065470838919282 0.00371195818297565 [0.0, 1.0]\n",
      "0.006772514432668686 0.003063286654651165 0.003709227778017521 [0.0, 1.0]\n",
      "0.006767586804926395 0.003061103168874979 0.00370648386888206 [0.0, 1.0]\n",
      "0.0067626615054905415 0.003058920381590724 0.0037037411238998175 [0.0, 1.0]\n",
      "0.006757752038538456 0.0030567385256290436 0.003701013745740056 [0.0, 1.0]\n",
      "0.006752844899892807 0.003054557368159294 0.0036982872989028692 [0.0, 1.0]\n",
      "0.006747931241989136 0.003052383428439498 0.003695547580718994 [0.0, 1.0]\n",
      "0.006743033416569233 0.0030502104200422764 0.0036928232293576 [0.0, 1.0]\n",
      "0.006738138385117054 0.0030480381101369858 0.0036901000421494246 [0.0, 1.0]\n",
      "0.00673324428498745 0.00304586673155427 0.00368737755343318 [0.0, 1.0]\n",
      "0.006728338077664375 0.0030436960514634848 0.003684641793370247 [0.0, 1.0]\n",
      "0.006723462138324976 0.0030415260698646307 0.0036819360684603453 [0.0, 1.0]\n",
      "0.006718565709888935 0.0030393635388463736 0.003679202403873205 [0.0, 1.0]\n",
      "0.00671368557959795 0.0030372017063200474 0.0036764841061085463 [0.0, 1.0]\n",
      "0.006708821747452021 0.003035040572285652 0.0036737811751663685 [0.0, 1.0]\n",
      "0.0067039309069514275 0.0030328803695738316 0.003671050537377596 [0.0, 1.0]\n",
      "0.006699070334434509 0.003030720865353942 0.003668349701911211 [0.0, 1.0]\n",
      "0.00669421162456274 0.0030285620596259832 0.0036656497977674007 [0.0, 1.0]\n",
      "0.006689347326755524 0.0030264107044786215 0.003662936622276902 [0.0, 1.0]\n",
      "0.006684484425932169 0.0030242600478231907 0.0036602243781089783 [0.0, 1.0]\n",
      "0.006679637357592583 0.003022110089659691 0.0036575275007635355 [0.0, 1.0]\n",
      "0.006674792617559433 0.003019960829988122 0.0036548315547406673 [0.0, 1.0]\n",
      "0.006669934839010239 0.0030178125016391277 0.003652122337371111 [0.0, 1.0]\n",
      "0.0066650933586061 0.0030156648717820644 0.0036494284868240356 [0.0, 1.0]\n",
      "0.006660260260105133 0.003013524692505598 0.003646735567599535 [0.0, 1.0]\n",
      "0.006655429024249315 0.0030113852117210627 0.0036440438125282526 [0.0, 1.0]\n",
      "0.006650599185377359 0.003009246429428458 0.003641352755948901 [0.0, 1.0]\n",
      "0.006645771209150553 0.0030071083456277847 0.003638662863522768 [0.0, 1.0]\n",
      "0.006640945561230183 0.003004971193149686 0.003635974135249853 [0.0, 1.0]\n",
      "0.006636120844632387 0.003002834739163518 0.003633286105468869 [0.0, 1.0]\n",
      "0.006631312891840935 0.003000698983669281 0.00363061367534101 [0.0, 1.0]\n",
      "0.006626497954130173 0.0029985704459249973 0.003627927741035819 [0.0, 1.0]\n",
      "0.006621699780225754 0.0029964428395032883 0.003625257173553109 [0.0, 1.0]\n",
      "0.006616889499127865 0.00299431593157351 0.003622573334723711 [0.0, 1.0]\n",
      "0.00661209411919117 0.002992189722135663 0.0036199046298861504 [0.0, 1.0]\n",
      "0.006607286632061005 0.002990064211189747 0.0036172226537019014 [0.0, 1.0]\n",
      "0.006602495908737183 0.0029879396315664053 0.0036145560443401337 [0.0, 1.0]\n",
      "0.0065977126359939575 0.002985822269693017 0.003611890599131584 [0.0, 1.0]\n",
      "0.006592917256057262 0.0029837056063115597 0.003609211416915059 [0.0, 1.0]\n",
      "0.006588137708604336 0.0029815896414220333 0.003606547834351659 [0.0, 1.0]\n",
      "0.006583360023796558 0.0029794746078550816 0.003603885183110833 [0.0, 1.0]\n",
      "0.006578583270311356 0.002977360039949417 0.003601223463192582 [0.0, 1.0]\n",
      "0.006573823280632496 0.0029752464033663273 0.0035985771100968122 [0.0, 1.0]\n",
      "0.006569050718098879 0.0029731334652751684 0.0035959172528237104 [0.0, 1.0]\n",
      "0.006564286537468433 0.0029710279777646065 0.003593258559703827 [0.0, 1.0]\n",
      "0.00655952375382185 0.002968922955915332 0.003590600797906518 [0.0, 1.0]\n",
      "0.006554777268320322 0.002966818865388632 0.00358795840293169 [0.0, 1.0]\n",
      "0.00655001774430275 0.0029647154733538628 0.0035853025037795305 [0.0, 1.0]\n",
      "0.006545274518430233 0.0029626127798110247 0.003582661971449852 [0.0, 1.0]\n",
      "0.006540519185364246 0.0029605107847601175 0.003580008167773485 [0.0, 1.0]\n",
      "0.006535785738378763 0.0029584162402898073 0.003577369498088956 [0.0, 1.0]\n",
      "0.006531054154038429 0.0029563221614807844 0.003574731992557645 [0.0, 1.0]\n",
      "0.006526323966681957 0.002954229013994336 0.0035720951855182648 [0.0, 1.0]\n",
      "0.006521581672132015 0.002952136564999819 0.00356944533996284 [0.0, 1.0]\n",
      "0.006516855210065842 0.0029500448144972324 0.003566810628399253 [0.0, 1.0]\n",
      "0.006512130610644817 0.002947953762486577 0.0035641768481582403 [0.0, 1.0]\n",
      "0.0065074218437075615 0.0029458636417984962 0.003561558434739709 [0.0, 1.0]\n",
      "0.006502707023173571 0.002943780506029725 0.0035589265171438456 [0.0, 1.0]\n",
      "0.006497994065284729 0.0029416983015835285 0.0035562957637012005 [0.0, 1.0]\n",
      "0.006493282504379749 0.002939616795629263 0.00355366594158113 [0.0, 1.0]\n",
      "0.006488587241619825 0.0029375359881669283 0.003551051253452897 [0.0, 1.0]\n",
      "0.006483878940343857 0.0029354558791965246 0.003548423293977976 [0.0, 1.0]\n",
      "0.006479186937212944 0.002933376468718052 0.003545810468494892 [0.0, 1.0]\n",
      "0.0064744967967271805 0.002931297989562154 0.0035431988071650267 [0.0, 1.0]\n",
      "0.006469800136983395 0.0029292264953255653 0.003540573874488473 [0.0, 1.0]\n",
      "0.006465119309723377 0.002927155699580908 0.003537963842973113 [0.0, 1.0]\n",
      "0.006460440810769796 0.002925085835158825 0.0035353549756109715 [0.0, 1.0]\n",
      "0.0064557637088000774 0.002923016669228673 0.0035327470395714045 [0.0, 1.0]\n",
      "0.006451088469475508 0.002920948201790452 0.0035301402676850557 [0.0, 1.0]\n",
      "0.0064464146271348 0.002918880432844162 0.003527534194290638 [0.0, 1.0]\n",
      "0.006441734731197357 0.0029168196488171816 0.0035249150823801756 [0.0, 1.0]\n",
      "0.006437084637582302 0.002914759796112776 0.00352232507430017 [0.0, 1.0]\n",
      "0.006432422902435064 0.0029127008747309446 0.0035197220277041197 [0.0, 1.0]\n",
      "0.006427762098610401 0.0029106424190104008 0.003517119912430644 [0.0, 1.0]\n",
      "0.006423117592930794 0.002908584661781788 0.0035145326983183622 [0.0, 1.0]\n",
      "0.006418460048735142 0.0029065278358757496 0.003511932445690036 [0.0, 1.0]\n",
      "0.0064138188026845455 0.0029044714756309986 0.003509347327053547 [0.0, 1.0]\n",
      "0.006409171503037214 0.002902422333136201 0.0035067491699010134 [0.0, 1.0]\n",
      "0.006404525600373745 0.002900374121963978 0.003504151711240411 [0.0, 1.0]\n",
      "0.006399895995855331 0.002898326376453042 0.0035015693865716457 [0.0, 1.0]\n",
      "0.00639526778832078 0.002896279562264681 0.003498988226056099 [0.0, 1.0]\n",
      "0.006390641443431377 0.0028942334465682507 0.003496407764032483 [0.0, 1.0]\n",
      "0.006386016495525837 0.0028921877965331078 0.0034938284661620855 [0.0, 1.0]\n",
      "0.006381392944604158 0.0028901430778205395 0.003491249866783619 [0.0, 1.0]\n",
      "0.006376777775585651 0.0028881055768579245 0.0034886724315583706 [0.0, 1.0]\n",
      "0.006372164469212294 0.0028860687743872404 0.003486095694825053 [0.0, 1.0]\n",
      "0.006367552559822798 0.0028840324375778437 0.003483520122244954 [0.0, 1.0]\n",
      "0.006362942513078451 0.0028819970320910215 0.0034809454809874296 [0.0, 1.0]\n",
      "0.006358348298817873 0.0028799623250961304 0.0034783859737217426 [0.0, 1.0]\n",
      "0.0063537415117025375 0.00287792831659317 0.0034758131951093674 [0.0, 1.0]\n",
      "0.006349150091409683 0.002875895006582141 0.003473255317658186 [0.0, 1.0]\n",
      "0.006344553083181381 0.002873868914321065 0.00347068440169096 [0.0, 1.0]\n",
      "0.006339971907436848 0.0028718432877212763 0.0034681286197155714 [0.0, 1.0]\n",
      "0.006335392594337463 0.0028698185924440622 0.0034655737690627575 [0.0, 1.0]\n",
      "0.0063308002427220345 0.002867794595658779 0.0034630056470632553 [0.0, 1.0]\n",
      "0.006326223723590374 0.002865771297365427 0.0034604526590555906 [0.0, 1.0]\n",
      "0.006321649067103863 0.002863748697564006 0.0034579006023705006 [0.0, 1.0]\n",
      "0.006317076273262501 0.0028617267962545156 0.003455349477007985 [0.0, 1.0]\n",
      "0.006312510930001736 0.002859711879864335 0.0034527990501374006 [0.0, 1.0]\n",
      "0.006307961884886026 0.002857697894796729 0.0034502639900892973 [0.0, 1.0]\n",
      "0.006303385831415653 0.0028556843753904104 0.003447701456025243 [0.0, 1.0]\n",
      "0.006298840045928955 0.0028536717873066664 0.0034451682586222887 [0.0, 1.0]\n",
      "0.006294281221926212 0.0028516596648842096 0.0034426217898726463 [0.0, 1.0]\n",
      "0.006289738696068525 0.0028496484737843275 0.003440090222284198 [0.0, 1.0]\n",
      "0.006285183597356081 0.0028476379811763763 0.0034375456161797047 [0.0, 1.0]\n",
      "0.00628065038472414 0.002845634473487735 0.0034350159112364054 [0.0, 1.0]\n",
      "0.006276104599237442 0.002843631664291024 0.0034324731677770615 [0.0, 1.0]\n",
      "0.006271574646234512 0.0028416295535862446 0.0034299453254789114 [0.0, 1.0]\n",
      "0.006267032586038113 0.002839628141373396 0.0034274046774953604 [0.0, 1.0]\n",
      "0.006262506358325481 0.0028376274276524782 0.0034248786978423595 [0.0, 1.0]\n",
      "0.006257995031774044 0.002835627645254135 0.0034223676193505526 [0.0, 1.0]\n",
      "0.006253471598029137 0.0028336283285170794 0.003419843502342701 [0.0, 1.0]\n",
      "0.006248942576348782 0.002831635996699333 0.0034173063468188047 [0.0, 1.0]\n",
      "0.006244428921490908 0.0028296445962041616 0.003414784325286746 [0.0, 1.0]\n",
      "0.006239916663616896 0.0028276536613702774 0.0034122630022466183 [0.0, 1.0]\n",
      "0.006235406268388033 0.0028256636578589678 0.003409742610529065 [0.0, 1.0]\n",
      "0.006230911239981651 0.002823674352839589 0.003407237119972706 [0.0, 1.0]\n",
      "0.006226418074220419 0.0028216855134814978 0.003404732560738921 [0.0, 1.0]\n",
      "0.0062219128012657166 0.002819697605445981 0.003402214962989092 [0.0, 1.0]\n",
      "0.006217414978891611 0.002817716682329774 0.003399698296561837 [0.0, 1.0]\n",
      "0.006212932989001274 0.002815736224874854 0.0033971965312957764 [0.0, 1.0]\n",
      "0.00620843842625618 0.002813756698742509 0.003394681727513671 [0.0, 1.0]\n",
      "0.006203959695994854 0.0028117778711020947 0.0033921818248927593 [0.0, 1.0]\n",
      "0.00619948236271739 0.0028097995091229677 0.0033896828535944223 [0.0, 1.0]\n",
      "0.006195006892085075 0.0028078220784664154 0.0033871845807880163 [0.0, 1.0]\n",
      "0.006190532818436623 0.002805845346301794 0.0033846874721348286 [0.0, 1.0]\n",
      "0.006186060607433319 0.0028038693126291037 0.0033821912948042154 [0.0, 1.0]\n",
      "0.0061815958470106125 0.0028019000310450792 0.0033796958159655333 [0.0, 1.0]\n",
      "0.006177132949233055 0.0027999316807836294 0.0033772015012800694 [0.0, 1.0]\n",
      "0.006172671914100647 0.0027979640290141106 0.0033747078850865364 [0.0, 1.0]\n",
      "0.006168212741613388 0.0027959970757365227 0.0033722154330462217 [0.0, 1.0]\n",
      "0.006163768004626036 0.002794030588120222 0.0033697374165058136 [0.0, 1.0]\n",
      "0.006159312091767788 0.002792065031826496 0.003367246827110648 [0.0, 1.0]\n",
      "0.006154870614409447 0.002790100174024701 0.0033647706732153893 [0.0, 1.0]\n",
      "0.006150424014776945 0.002788142068311572 0.003362281946465373 [0.0, 1.0]\n",
      "0.006145992316305637 0.0027861848939210176 0.0033598076552152634 [0.0, 1.0]\n",
      "0.006141548976302147 0.0027842281851917505 0.0033573205582797527 [0.0, 1.0]\n",
      "0.00613712053745985 0.002782272407785058 0.003354848362505436 [0.0, 1.0]\n",
      "0.006132693961262703 0.002780317096039653 0.00335237686522305 [0.0, 1.0]\n",
      "0.006128269247710705 0.0027783627156168222 0.003349906299263239 [0.0, 1.0]\n",
      "0.006123845465481281 0.002776408800855279 0.0033474366646260023 [0.0, 1.0]\n",
      "0.006119430065155029 0.0027744618710130453 0.0033449679613113403 [0.0, 1.0]\n",
      "0.006115015596151352 0.0027725156396627426 0.003342500189319253 [0.0, 1.0]\n",
      "0.006110603455454111 0.002770570106804371 0.0033400333486497402 [0.0, 1.0]\n",
      "0.006106206215918064 0.00276862527243793 0.0033375811763107777 [0.0, 1.0]\n",
      "0.0061017973348498344 0.0027666811365634203 0.0033351159654557705 [0.0, 1.0]\n",
      "0.006097389385104179 0.0027647376991808414 0.0033326519187539816 [0.0, 1.0]\n",
      "0.006092997267842293 0.0027627949602901936 0.0033302023075520992 [0.0, 1.0]\n",
      "0.006088593043386936 0.0027608529198914766 0.0033277401234954596 [0.0, 1.0]\n",
      "0.006084210239350796 0.0027589178644120693 0.0033252923749387264 [0.0, 1.0]\n",
      "0.006079828832298517 0.0027569832745939493 0.003322845557704568 [0.0, 1.0]\n",
      "0.006075435318052769 0.002755049616098404 0.0033203859347850084 [0.0, 1.0]\n",
      "0.006071057170629501 0.002753116423264146 0.003317940980195999 [0.0, 1.0]\n",
      "0.006066680885851383 0.0027511839289218187 0.0033154969569295645 [0.0, 1.0]\n",
      "0.00606230553239584 0.0027492521330714226 0.0033130536321550608 [0.0, 1.0]\n",
      "0.006057932507246733 0.0027473210357129574 0.0033106114715337753 [0.0, 1.0]\n",
      "0.006053566932678223 0.002745396923273802 0.003308170009404421 [0.0, 1.0]\n",
      "0.006049203220754862 0.002743473509326577 0.0033057297114282846 [0.0, 1.0]\n",
      "0.006044840905815363 0.0027415507938712835 0.0033032901119440794 [0.0, 1.0]\n",
      "0.006040479987859726 0.002739628544077277 0.0033008514437824488 [0.0, 1.0]\n",
      "0.00603613443672657 0.002737706992775202 0.0032984272111207247 [0.0, 1.0]\n",
      "0.006031776778399944 0.002735786372795701 0.0032959904056042433 [0.0, 1.0]\n",
      "0.006027434021234512 0.0027338662184774876 0.0032935680355876684 [0.0, 1.0]\n",
      "0.006023080088198185 0.002731946762651205 0.0032911330927163363 [0.0, 1.0]\n",
      "0.006018746644258499 0.002730034291744232 0.0032887125853449106 [0.0, 1.0]\n",
      "0.006014401558786631 0.0027281222864985466 0.003286279272288084 [0.0, 1.0]\n",
      "0.006010071840137243 0.0027262112125754356 0.0032838606275618076 [0.0, 1.0]\n",
      "0.006005743518471718 0.002724300604313612 0.003281442681327462 [0.0, 1.0]\n",
      "0.006001416593790054 0.002722390927374363 0.0032790256664156914 [0.0, 1.0]\n",
      "0.00599709153175354 0.002720481716096401 0.003276609815657139 [0.0, 1.0]\n",
      "0.00599278137087822 0.0027185732033103704 0.003274208167567849 [0.0, 1.0]\n",
      "0.005988451652228832 0.0027166714426130056 0.0032717802096158266 [0.0, 1.0]\n",
      "0.005984137766063213 0.0027147706132382154 0.0032693669199943542 [0.0, 1.0]\n",
      "0.005979824811220169 0.0027128702495247126 0.003266954328864813 [0.0, 1.0]\n",
      "0.005975513719022274 0.0027109705843031406 0.0032645429018884897 [0.0, 1.0]\n",
      "0.005971217527985573 0.0027090716175734997 0.0032621456775814295 [0.0, 1.0]\n",
      "0.005966923199594021 0.0027071733493357897 0.0032597496174275875 [0.0, 1.0]\n",
      "0.0059626162983477116 0.0027052757795900106 0.003257340518757701 [0.0, 1.0]\n",
      "0.0059583247639238834 0.002703378675505519 0.0032549460884183645 [0.0, 1.0]\n",
      "0.005954027641564608 0.002701488556340337 0.003252539085224271 [0.0, 1.0]\n",
      "0.005949745420366526 0.0026995991356670856 0.00325014628469944 [0.0, 1.0]\n",
      "0.005945464596152306 0.0026977104134857655 0.003247754415497184 [0.0, 1.0]\n",
      "0.005941172130405903 0.0026958221569657326 0.0032453499734401703 [0.0, 1.0]\n",
      "0.005936908535659313 0.0026939348317682743 0.0032429734710603952 [0.0, 1.0]\n",
      "0.005932632368057966 0.0026920479722321033 0.003240584395825863 [0.0, 1.0]\n",
      "0.0059283580631017685 0.0026901618111878633 0.003238196251913905 [0.0, 1.0]\n",
      "0.005924085155129433 0.0026882763486355543 0.0032358088064938784 [0.0, 1.0]\n",
      "0.005919819697737694 0.0026863976381719112 0.003233422292396426 [0.0, 1.0]\n",
      "0.005915556102991104 0.002684519626200199 0.0032310367096215487 [0.0, 1.0]\n",
      "0.005911307409405708 0.002682642312720418 0.003228665329515934 [0.0, 1.0]\n",
      "0.005907047539949417 0.002680765697732568 0.0032262816093862057 [0.0, 1.0]\n",
      "0.0059027886018157005 0.0026788897812366486 0.0032238985877484083 [0.0, 1.0]\n",
      "0.005898544564843178 0.0026770143304020166 0.0032215300016105175 [0.0, 1.0]\n",
      "0.00589430145919323 0.0026751395780593157 0.0032191621139645576 [0.0, 1.0]\n",
      "0.0058900536969304085 0.0026732718106359243 0.003216781886294484 [0.0, 1.0]\n",
      "0.005885820370167494 0.0026714045088738203 0.0032144158612936735 [0.0, 1.0]\n",
      "0.005881574936211109 0.0026695379056036472 0.0032120372634381056 [0.0, 1.0]\n",
      "0.005877358838915825 0.002667672000825405 0.003209686605259776 [0.0, 1.0]\n",
      "0.005873116664588451 0.002665806794539094 0.0032073096372187138 [0.0, 1.0]\n",
      "0.005868889391422272 0.00266394205391407 0.003204947104677558 [0.0, 1.0]\n",
      "0.005864663515239954 0.0026620780117809772 0.0032025855034589767 [0.0, 1.0]\n",
      "0.005860453005880117 0.0026602146681398153 0.003200238337740302 [0.0, 1.0]\n",
      "0.005856236442923546 0.002658358309417963 0.0031978783663362265 [0.0, 1.0]\n",
      "0.005852022208273411 0.002656502416357398 0.003195519559085369 [0.0, 1.0]\n",
      "0.005847808439284563 0.002654647221788764 0.003193161217495799 [0.0, 1.0]\n",
      "0.0058436100371181965 0.0026527924928814173 0.003190817544236779 [0.0, 1.0]\n",
      "0.005839399993419647 0.002650938695296645 0.0031884610652923584 [0.0, 1.0]\n",
      "0.0058351908810436726 0.0026490853633731604 0.003186105517670512 [0.0, 1.0]\n",
      "0.005830997135490179 0.0026472327299416065 0.0031837644055485725 [0.0, 1.0]\n",
      "0.0058268047869205475 0.0026453807950019836 0.003181423991918564 [0.0, 1.0]\n",
      "0.005822606850415468 0.0026435356121510267 0.0031790712382644415 [0.0, 1.0]\n",
      "0.0058184233494102955 0.002641690894961357 0.0031767324544489384 [0.0, 1.0]\n",
      "0.005814242176711559 0.002639847109094262 0.0031743948347866535 [0.0, 1.0]\n",
      "0.00581006146967411 0.0026380037888884544 0.0031720579136162996 [0.0, 1.0]\n",
      "0.005805882625281811 0.0026361611671745777 0.0031697216909378767 [0.0, 1.0]\n",
      "0.00580170564353466 0.002634319243952632 0.003167386632412672 [0.0, 1.0]\n",
      "0.005797530058771372 0.0026324777863919735 0.0031650522723793983 [0.0, 1.0]\n",
      "0.005793369375169277 0.0026306372601538897 0.003162732347846031 [0.0, 1.0]\n",
      "0.0057892026379704475 0.002628803253173828 0.003160399617627263 [0.0, 1.0]\n",
      "0.005785037763416767 0.0026269699446856976 0.0031580678187310696 [0.0, 1.0]\n",
      "0.005780874285846949 0.002625137334689498 0.0031557369511574507 [0.0, 1.0]\n",
      "0.005776725709438324 0.0026233051903545856 0.0031534205190837383 [0.0, 1.0]\n",
      "0.005772565025836229 0.0026214737445116043 0.003151091281324625 [0.0, 1.0]\n",
      "0.0057684192433953285 0.002619642997160554 0.0031487762462347746 [0.0, 1.0]\n",
      "0.00576427485793829 0.0026178129483014345 0.0031464621424674988 [0.0, 1.0]\n",
      "0.0057601323351264 0.002615983597934246 0.0031441489700227976 [0.0, 1.0]\n",
      "0.005755984224379063 0.00261416076682508 0.003141823224723339 [0.0, 1.0]\n",
      "0.0057518500834703445 0.0026123386342078447 0.0031395116820931435 [0.0, 1.0]\n",
      "0.005747718270868063 0.0026105172000825405 0.0031372010707855225 [0.0, 1.0]\n",
      "0.005743587855249643 0.0026086964644491673 0.003134891390800476 [0.0, 1.0]\n",
      "0.005739458836615086 0.0026068761944770813 0.0031325824093073606 [0.0, 1.0]\n",
      "0.005735344253480434 0.0026050566229969263 0.003130287630483508 [0.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0057312180288136005 0.0026032377500087023 0.0031279802788048983 [0.0, 1.0]\n",
      "0.005727093201130629 0.0026014193426817656 0.003125673858448863 [0.0, 1.0]\n",
      "0.0057229893282055855 0.0025996079202741385 0.0031233816407620907 [0.0, 1.0]\n",
      "0.00571887381374836 0.0025977969635277987 0.003121076850220561 [0.0, 1.0]\n",
      "0.005714759696274996 0.00259598670527339 0.003118772991001606 [0.0, 1.0]\n",
      "0.005710660479962826 0.0025941769126802683 0.003116483334451914 [0.0, 1.0]\n",
      "0.0057065486907958984 0.0025923678185790777 0.0031141811050474644 [0.0, 1.0]\n",
      "0.0057024527341127396 0.002590559422969818 0.003111893078312278 [0.0, 1.0]\n",
      "0.005698357708752155 0.0025887517258524895 0.003109605982899666 [0.0, 1.0]\n",
      "0.005694264080375433 0.002586944494396448 0.003107319585978985 [0.0, 1.0]\n",
      "0.0056901779025793076 0.0025851440150290728 0.0031050341203808784 [0.0, 1.0]\n",
      "0.0056860800832509995 0.0025833442341536283 0.0031027360819280148 [0.0, 1.0]\n",
      "0.005681997165083885 0.0025815449189394712 0.003100452246144414 [0.0, 1.0]\n",
      "0.005677915643900633 0.002579746302217245 0.0030981693416833878 [0.0, 1.0]\n",
      "0.0056738355197012424 0.00257794838398695 0.0030958871357142925 [0.0, 1.0]\n",
      "0.005669756792485714 0.0025761511642485857 0.003093605861067772 [0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4764, 0.9420], requires_grad=True), tensor(0.9927))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xW = get_xW(x_prime, rew, phi, b)\n",
    "xW, rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e1f8eb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9927, 1.1150]),\n",
       " tensor([0., 1.]),\n",
       " tensor(1.1150),\n",
       " tensor(0.9420, grad_fn=<DotBackward0>),\n",
       " tensor(0.0026, grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = phi @ x_prime / (one_m @ (phi @ x_prime))\n",
    "x_new @ W, a, (x_new @ W) @ a, xW @ a, (rew - xW @ a) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "12a90fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_x = [np.random.uniform(low=0, high=1) for _ in range(10_000)]\n",
    "X_y = [np.random.uniform(low=0, high=1) for _ in range(10_000)]\n",
    "X = torch.tensor(np.array([X_x, X_y]).T, dtype=torch.float32)\n",
    "W = torch.rand((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "15c43527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x15e10d722b0>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = (X @ W)[:,0] > (X @ W)[:,1]\n",
    "plt.scatter(X_x, X_y, c=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cd32d7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8385, 0.5451],\n",
       "        [0.7961, 0.9919]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66bd8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = opt_phi(X, X @ W, torch.zeros(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ffa9f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2 # number of actions\n",
    "d = 2 # number of dimensions\n",
    "dataset = []\n",
    "adv_dataset = []\n",
    "labels = []\n",
    "true_phi = torch.rand((m, d))\n",
    "true_b = torch.zeros(m)\n",
    "# u = torch.rand(m)\n",
    "# W = torch.rand((d, m))\n",
    "one_m = torch.ones(m)\n",
    "phi = torch.rand((m, d))\n",
    "b = torch.zeros(m)\n",
    "\n",
    "x_new = torch.rand(d)\n",
    "x_prime = l1norm_gragent(x_new, phi, b, u).detach()\n",
    "dataset.append(x_new)\n",
    "adv_dataset.append(x_prime)\n",
    "rew = reward(x_new, x_prime, W, phi, b).detach()\n",
    "xW = get_xW(x_prime, rew, phi, b).detach()\n",
    "labels.append(xW)\n",
    "\n",
    "Xhats = torch.stack(adv_dataset)\n",
    "xWs = torch.stack(labels)\n",
    "Xs = torch.stack(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ff81938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xWs:  [[0.776593804359436, 0.7279911041259766], [1.3853859901428223, 1.3658171892166138], [0.765604555606842, 0.8181165456771851], [1.142574667930603, 1.0694520473480225], [0.6457672119140625, 0.8045029044151306], [1.357229471206665, 1.3206204175949097]]\n",
      "Phi:  [[0.0, 0.0], [0.7253062129020691, 1.0]]\n",
      "Xhat: [[0.47946423292160034, 0.4704628884792328], [0.7209693789482117, 0.9807912111282349], [0.271691232919693, 0.675505518913269], [0.7086511254310608, 0.6887703537940979], [0.00012470535875763744, 0.8110173940658569], [0.7412154674530029, 0.924098551273346]]\n",
      "Clf:  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n",
      "Rew:  [0.7279911041259766, 1.3658171892166138, 0.8181165456771851, 1.0694520473480225, 0.8045029044151306, 1.3206204175949097]\n"
     ]
    }
   ],
   "source": [
    "xW_true = Xhats @ W\n",
    "print('xWs: ', xW_true.tolist())\n",
    "relu = nn.ReLU()\n",
    "phi, b = opt_phi(Xhats, xW_true, None)\n",
    "# nphi = relu(phi) / torch.max(phi)\n",
    "print('Phi: ', phi.tolist())\n",
    "print('Xhat:', Xhats.tolist())\n",
    "clf = phi @ Xhats.T / (torch.ones(m) @ (phi @ Xhats.T))\n",
    "print('Clf: ', clf.tolist()) # action\n",
    "print('Rew: ', (xW_true @ clf)[:,0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8a76dab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6797, 0.0000])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi @ Xhats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bd2c426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = torch.rand(d)\n",
    "Xhats = torch.stack(adv_dataset)\n",
    "xWs = torch.stack(labels)\n",
    "Xs = torch.stack(dataset)\n",
    "phi, b = opt_phi(Xhats, xWs, u)\n",
    "phi = phi.detach()\n",
    "\n",
    "x_prime = l1norm_gragent(x_new, phi, b, u).detach()\n",
    "dataset.append(x_new)\n",
    "adv_dataset.append(x_prime)\n",
    "rew = reward(x_new, x_prime, W, phi, b).detach()\n",
    "xW = get_xW(x_prime, rew, phi, b).detach()\n",
    "labels.append(xW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1e8ac404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15e10d0fe20>]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actions = torch.max(phi @ Xs.T + b.reshape(-1, 1) / (one_m @ (phi @ Xs.T + b.reshape(-1, 1))), axis=0)\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.scatter(Xs[:, 0], Xs[:, 1], c=actions.indices.tolist())\n",
    "plt.plot(x_new[0], x_new[1], marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"red\")\n",
    "plt.plot(x_prime[0], x_prime[1], marker=\"o\", markersize=10, markeredgecolor=\"orange\", markerfacecolor=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf4b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "id": "eb212332",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = torch.rand(d)\n",
    "Xs = torch.stack(dataset)\n",
    "phi, b = l1norm_phi(Xs, W, u)\n",
    "x_prime = l1norm_gragent(x_new, phi, b, u).detach()\n",
    "dataset.append(x_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "c35c216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2062, 0.1774],\n",
       "         [0.8851, 0.7943]], requires_grad=True),\n",
       " tensor([[0.5000, 0.5000]]),\n",
       " tensor([[0.1859],\n",
       "         [0.8141]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi, Xs, phi @ Xs.T / (one_m @ (phi @ Xs.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "6040fde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x247f74c48e0>]"
      ]
     },
     "execution_count": 970,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actions = torch.max(phi @ Xs.T / (one_m @ (phi @ Xs.T)), axis=0)\n",
    "# print(actions.indices.tolist())\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.scatter(Xs[:, 0], Xs[:, 1], c=actions.indices.tolist())\n",
    "plt.plot(x_new[0], x_new[1], marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"red\")\n",
    "plt.plot(x_prime[0], x_prime[1], marker=\"o\", markersize=10, markeredgecolor=\"orange\", markerfacecolor=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb783728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
